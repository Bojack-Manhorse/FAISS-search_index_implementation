{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2024-06-12 21:41:11.215035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 21:41:11.989096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import json \n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Datasets/Products.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0             7156\n",
       " id                     7156\n",
       " product_name           7156\n",
       " category               7156\n",
       " product_description    7156\n",
       " price                  7156\n",
       " location               7156\n",
       " dtype: int64,\n",
       " Unnamed: 0             7156\n",
       " id                     7156\n",
       " product_name           7156\n",
       " category               7156\n",
       " product_description    7156\n",
       " price                  7156\n",
       " location               7156\n",
       " dtype: int64,\n",
       " array(['£'], dtype=object),\n",
       " array(['.00', '.99', '.78', '.01', '.97', '.25', '.50', '.20', '.90',\n",
       "        '.80', '.60', '.23', '.05', '.75', '.56', '.40', '.44', '.95',\n",
       "        '.66', '.35', '.85', '.30', '.45', '.16', '.69', '.49', '.55',\n",
       "        '.09', '.11'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count(), dataset.dropna().count(), dataset[\"price\"].map(lambda x: x[0]).unique(), dataset[\"price\"].map(lambda x: x[-3:]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7156 entries, 0 to 7155\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Unnamed: 0           7156 non-null   int64 \n",
      " 1   id                   7156 non-null   object\n",
      " 2   product_name         7156 non-null   object\n",
      " 3   category             7156 non-null   object\n",
      " 4   product_description  7156 non-null   object\n",
      " 5   price                7156 non-null   object\n",
      " 6   location             7156 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 391.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the dataset seems to already be clean\n",
    "\n",
    "Now we convert the price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset\n",
    "\n",
    "def remove_pound_sign(string_to_replace) -> str:\n",
    "    return \n",
    "\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x : x.replace('£', '')) #Removes the pound signs\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x: x.replace(',', '')) #Removes commas\n",
    "\n",
    "dataset_cleaned['price'] = pd.to_numeric(dataset_cleaned['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we extract the root category from each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['category'] = dataset_cleaned['category'].map(lambda x: x.split(' /' )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home & Garden', 'Baby & Kids Stuff', 'DIY Tools & Materials',\n",
       "       'Music, Films, Books & Games', 'Phones, Mobile Phones & Telecoms',\n",
       "       'Clothes, Footwear & Accessories', 'Other Goods',\n",
       "       'Health & Beauty', 'Sports, Leisure & Travel', 'Appliances',\n",
       "       'Computers & Software', 'Office Furniture & Equipment',\n",
       "       'Video Games & Consoles'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an encoder to go to and from the categories and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_categories = list(dataset_cleaned['category'].unique())\n",
    "\n",
    "encoder = {x: list_of_categories.index(x) for x in list_of_categories}\n",
    "\n",
    "decoder = {list_of_categories.index(x):x for x in list_of_categories}\n",
    "\n",
    "encoder, decoder\n",
    "\n",
    "#Let's save these to pickle files:\n",
    "\n",
    "with open(\"encoder_pickle\", 'wb') as encody:\n",
    "    pickle.dump(encoder, encody)\n",
    "\n",
    "with open(\"decoder_pickle\", 'wb') as decody:\n",
    "    pickle.dump(decoder, decody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we merge this with the original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['root_category'] = dataset_cleaned['category']\n",
    "dataset_cleaned['root_category_index'] = dataset_cleaned['category'].map(lambda x:encoder[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we merge this with the images table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0               int64\n",
       " id                      object\n",
       " product_name            object\n",
       " category                object\n",
       " product_description     object\n",
       " price                  float64\n",
       " location                object\n",
       " root_category           object\n",
       " root_category_index      int64\n",
       " merge_column            object\n",
       " dtype: object,\n",
       " Unnamed: 0       int64\n",
       " id              object\n",
       " product_id      object\n",
       " merge_column    object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dataset = pd.read_csv('Datasets/Images.csv')\n",
    "\n",
    "dataset_cleaned[\"merge_column\"] = dataset_cleaned['id']\n",
    "images_dataset['merge_column'] = images_dataset['product_id']\n",
    "\n",
    "dataset_cleaned.dtypes, images_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = images_dataset.merge(dataset_cleaned, on='merge_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     object\n",
       "root_category          object\n",
       "root_category_index     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df[[\"id_x\", \"product_id\", \"root_category\", \"root_category_index\"]]\n",
    "merged_df = merged_df.rename(columns={'id_x':'id'})\n",
    "merged_df = merged_df.drop(columns=['product_id'])\n",
    "\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Datasets/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Image Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(final_size, im):\n",
    "    size = im.size\n",
    "    ratio = float(final_size) / max(size)\n",
    "    new_image_size = tuple([int(x*ratio) for x in size])\n",
    "    im = im.resize(new_image_size)\n",
    "    new_im = Image.new(\"RGB\", (final_size, final_size))\n",
    "    new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n",
    "    return new_im\n",
    "\n",
    "def clean_images(path_to_extract = \"Datasets/images/\", path_to_save = \"Datasets/cleaned_images/\", image_size = 64):\n",
    "    dirs = os.listdir(path_to_extract)\n",
    "    final_size = image_size\n",
    "    for n, item in enumerate(dirs, 1):\n",
    "        #print(n, item)\n",
    "        im = Image.open(path_to_extract + item)\n",
    "        #print(im.width, im.height)\n",
    "        new_im = resize_image(final_size, im)\n",
    "        #print(new_im.width, new_im.height)\n",
    "        new_im.save(path_to_save + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_images(path_to_save=\"Datasets/cleaned_images_64/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting into a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df_of_keys:pd.DataFrame, folder_of_images:str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the labels to be the column 'root_cotegory_index' of df_of_keys\n",
    "        self.labels = df_of_keys['root_category_index']\n",
    "\n",
    "        # Assings image_paths to the file name from the column 'id' of df_of_keys and maps it to it's path relative to the project root folder\n",
    "        self.image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "\n",
    "        # The Resent50 documentation said these transforms must be applied to the images before the model processes them\n",
    "        self.image_transformer = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(degrees=45),\n",
    "            transforms.PILToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Opens the image at index with using PIL.Image\n",
    "        with Image.open(self.image_paths[index]) as img:\n",
    "\n",
    "            # Sets the feature to be a tensor obtained by applying PILToTensor to the relevant image\n",
    "            X = self.image_transformer(img)\n",
    "        \n",
    "        # y is the label from self.labels\n",
    "        y = self.labels[index]\n",
    "\n",
    "        #print(X.shape, y.shape)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " (),\n",
       " 12604)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = ImageDataset(merged_df, \"Datasets/cleaned_images_64/\")\n",
    "\n",
    "# A quick sanity check of the dataset:\n",
    "my_dataset[1][0], my_dataset[1][1].shape, len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    my_dataset, [10000, len(my_dataset) - 10000],\n",
    ")\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [8000, 2000]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = transforms.ToPILImage()\n",
    "\n",
    "thing(train_dataset[5][0].squeeze(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ibs/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, device(type='cuda'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets the device for PyTorch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Loads the resnet50 model\n",
    "#resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', model = 'nvidia_resnet50', pretrained = True )\n",
    "resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "# Don't know what this does\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "# Again don't know what this does\n",
    "resnet50.eval().to(device)\n",
    "\n",
    "torch.cuda.is_available(), device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the model\n",
    "\n",
    "Let's print out the layers of resnet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "bn1.weight True\n",
      "bn1.bias True\n",
      "layer1.0.conv1.weight True\n",
      "layer1.0.bn1.weight True\n",
      "layer1.0.bn1.bias True\n",
      "layer1.0.conv2.weight True\n",
      "layer1.0.bn2.weight True\n",
      "layer1.0.bn2.bias True\n",
      "layer1.0.conv3.weight True\n",
      "layer1.0.bn3.weight True\n",
      "layer1.0.bn3.bias True\n",
      "layer1.0.downsample.0.weight True\n",
      "layer1.0.downsample.1.weight True\n",
      "layer1.0.downsample.1.bias True\n",
      "layer1.1.conv1.weight True\n",
      "layer1.1.bn1.weight True\n",
      "layer1.1.bn1.bias True\n",
      "layer1.1.conv2.weight True\n",
      "layer1.1.bn2.weight True\n",
      "layer1.1.bn2.bias True\n",
      "layer1.1.conv3.weight True\n",
      "layer1.1.bn3.weight True\n",
      "layer1.1.bn3.bias True\n",
      "layer1.2.conv1.weight True\n",
      "layer1.2.bn1.weight True\n",
      "layer1.2.bn1.bias True\n",
      "layer1.2.conv2.weight True\n",
      "layer1.2.bn2.weight True\n",
      "layer1.2.bn2.bias True\n",
      "layer1.2.conv3.weight True\n",
      "layer1.2.bn3.weight True\n",
      "layer1.2.bn3.bias True\n",
      "layer2.0.conv1.weight True\n",
      "layer2.0.bn1.weight True\n",
      "layer2.0.bn1.bias True\n",
      "layer2.0.conv2.weight True\n",
      "layer2.0.bn2.weight True\n",
      "layer2.0.bn2.bias True\n",
      "layer2.0.conv3.weight True\n",
      "layer2.0.bn3.weight True\n",
      "layer2.0.bn3.bias True\n",
      "layer2.0.downsample.0.weight True\n",
      "layer2.0.downsample.1.weight True\n",
      "layer2.0.downsample.1.bias True\n",
      "layer2.1.conv1.weight True\n",
      "layer2.1.bn1.weight True\n",
      "layer2.1.bn1.bias True\n",
      "layer2.1.conv2.weight True\n",
      "layer2.1.bn2.weight True\n",
      "layer2.1.bn2.bias True\n",
      "layer2.1.conv3.weight True\n",
      "layer2.1.bn3.weight True\n",
      "layer2.1.bn3.bias True\n",
      "layer2.2.conv1.weight True\n",
      "layer2.2.bn1.weight True\n",
      "layer2.2.bn1.bias True\n",
      "layer2.2.conv2.weight True\n",
      "layer2.2.bn2.weight True\n",
      "layer2.2.bn2.bias True\n",
      "layer2.2.conv3.weight True\n",
      "layer2.2.bn3.weight True\n",
      "layer2.2.bn3.bias True\n",
      "layer2.3.conv1.weight True\n",
      "layer2.3.bn1.weight True\n",
      "layer2.3.bn1.bias True\n",
      "layer2.3.conv2.weight True\n",
      "layer2.3.bn2.weight True\n",
      "layer2.3.bn2.bias True\n",
      "layer2.3.conv3.weight True\n",
      "layer2.3.bn3.weight True\n",
      "layer2.3.bn3.bias True\n",
      "layer3.0.conv1.weight True\n",
      "layer3.0.bn1.weight True\n",
      "layer3.0.bn1.bias True\n",
      "layer3.0.conv2.weight True\n",
      "layer3.0.bn2.weight True\n",
      "layer3.0.bn2.bias True\n",
      "layer3.0.conv3.weight True\n",
      "layer3.0.bn3.weight True\n",
      "layer3.0.bn3.bias True\n",
      "layer3.0.downsample.0.weight True\n",
      "layer3.0.downsample.1.weight True\n",
      "layer3.0.downsample.1.bias True\n",
      "layer3.1.conv1.weight True\n",
      "layer3.1.bn1.weight True\n",
      "layer3.1.bn1.bias True\n",
      "layer3.1.conv2.weight True\n",
      "layer3.1.bn2.weight True\n",
      "layer3.1.bn2.bias True\n",
      "layer3.1.conv3.weight True\n",
      "layer3.1.bn3.weight True\n",
      "layer3.1.bn3.bias True\n",
      "layer3.2.conv1.weight True\n",
      "layer3.2.bn1.weight True\n",
      "layer3.2.bn1.bias True\n",
      "layer3.2.conv2.weight True\n",
      "layer3.2.bn2.weight True\n",
      "layer3.2.bn2.bias True\n",
      "layer3.2.conv3.weight True\n",
      "layer3.2.bn3.weight True\n",
      "layer3.2.bn3.bias True\n",
      "layer3.3.conv1.weight True\n",
      "layer3.3.bn1.weight True\n",
      "layer3.3.bn1.bias True\n",
      "layer3.3.conv2.weight True\n",
      "layer3.3.bn2.weight True\n",
      "layer3.3.bn2.bias True\n",
      "layer3.3.conv3.weight True\n",
      "layer3.3.bn3.weight True\n",
      "layer3.3.bn3.bias True\n",
      "layer3.4.conv1.weight True\n",
      "layer3.4.bn1.weight True\n",
      "layer3.4.bn1.bias True\n",
      "layer3.4.conv2.weight True\n",
      "layer3.4.bn2.weight True\n",
      "layer3.4.bn2.bias True\n",
      "layer3.4.conv3.weight True\n",
      "layer3.4.bn3.weight True\n",
      "layer3.4.bn3.bias True\n",
      "layer3.5.conv1.weight True\n",
      "layer3.5.bn1.weight True\n",
      "layer3.5.bn1.bias True\n",
      "layer3.5.conv2.weight True\n",
      "layer3.5.bn2.weight True\n",
      "layer3.5.bn2.bias True\n",
      "layer3.5.conv3.weight True\n",
      "layer3.5.bn3.weight True\n",
      "layer3.5.bn3.bias True\n",
      "layer4.0.conv1.weight True\n",
      "layer4.0.bn1.weight True\n",
      "layer4.0.bn1.bias True\n",
      "layer4.0.conv2.weight True\n",
      "layer4.0.bn2.weight True\n",
      "layer4.0.bn2.bias True\n",
      "layer4.0.conv3.weight True\n",
      "layer4.0.bn3.weight True\n",
      "layer4.0.bn3.bias True\n",
      "layer4.0.downsample.0.weight True\n",
      "layer4.0.downsample.1.weight True\n",
      "layer4.0.downsample.1.bias True\n",
      "layer4.1.conv1.weight True\n",
      "layer4.1.bn1.weight True\n",
      "layer4.1.bn1.bias True\n",
      "layer4.1.conv2.weight True\n",
      "layer4.1.bn2.weight True\n",
      "layer4.1.bn2.bias True\n",
      "layer4.1.conv3.weight True\n",
      "layer4.1.bn3.weight True\n",
      "layer4.1.bn3.bias True\n",
      "layer4.2.conv1.weight True\n",
      "layer4.2.bn1.weight True\n",
      "layer4.2.bn1.bias True\n",
      "layer4.2.conv2.weight True\n",
      "layer4.2.bn2.weight True\n",
      "layer4.2.bn2.bias True\n",
      "layer4.2.conv3.weight True\n",
      "layer4.2.bn3.weight True\n",
      "layer4.2.bn3.bias True\n",
      "fc.weight True\n",
      "fc.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, module in resnet50.named_parameters():\n",
    "    if any(sub_string in name for sub_string in ['resnet50.layer4.2', 'resnet50.fc' ]):\n",
    "        print(name, module.requires_grad)\n",
    "    else:\n",
    "        print(name, module.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final layer, `fc` takes in 2048 features and outputs 1000 features. The number of labels we need to classify, i.e. the number of distinct values in `merged_df['root_categories']` is 13.\n",
    "\n",
    "- So we need to modify the `fc` layer so that it has 13 outputs. We do this by changing `fc` to `torch.nn.Linear(2048, 13)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_labels:int) -> None:\n",
    "        super().__init__()\n",
    "        self.resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze all the layers, by setting 'requires_grad = False'\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "            pass\n",
    "        \n",
    "        # Unfreeze the final layer, by setting 'requires_grad = False' for the fc layer\n",
    "        \n",
    "        for name, module in self.named_parameters():\n",
    "            if any(sub_string in name for sub_string in ['resnet50.layer4', 'resnet50.fc' ]):\n",
    "                module.requires_grad = True\n",
    "                print(name, module.requires_grad)\n",
    "        \n",
    "\n",
    "        self.final = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.float()\n",
    "        X = self.resnet50(X)\n",
    "        X = self.final(X)\n",
    "        X = F.softmax(X, dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50.layer4.0.conv1.weight True\n",
      "resnet50.layer4.0.bn1.weight True\n",
      "resnet50.layer4.0.bn1.bias True\n",
      "resnet50.layer4.0.conv2.weight True\n",
      "resnet50.layer4.0.bn2.weight True\n",
      "resnet50.layer4.0.bn2.bias True\n",
      "resnet50.layer4.0.conv3.weight True\n",
      "resnet50.layer4.0.bn3.weight True\n",
      "resnet50.layer4.0.bn3.bias True\n",
      "resnet50.layer4.0.downsample.0.weight True\n",
      "resnet50.layer4.0.downsample.1.weight True\n",
      "resnet50.layer4.0.downsample.1.bias True\n",
      "resnet50.layer4.1.conv1.weight True\n",
      "resnet50.layer4.1.bn1.weight True\n",
      "resnet50.layer4.1.bn1.bias True\n",
      "resnet50.layer4.1.conv2.weight True\n",
      "resnet50.layer4.1.bn2.weight True\n",
      "resnet50.layer4.1.bn2.bias True\n",
      "resnet50.layer4.1.conv3.weight True\n",
      "resnet50.layer4.1.bn3.weight True\n",
      "resnet50.layer4.1.bn3.bias True\n",
      "resnet50.layer4.2.conv1.weight True\n",
      "resnet50.layer4.2.bn1.weight True\n",
      "resnet50.layer4.2.bn1.bias True\n",
      "resnet50.layer4.2.conv2.weight True\n",
      "resnet50.layer4.2.bn2.weight True\n",
      "resnet50.layer4.2.bn2.bias True\n",
      "resnet50.layer4.2.conv3.weight True\n",
      "resnet50.layer4.2.bn3.weight True\n",
      "resnet50.layer4.2.bn3.bias True\n",
      "resnet50.fc.weight True\n",
      "resnet50.fc.bias True\n",
      "Here resnet50.conv1.weight False\n",
      "Here resnet50.bn1.weight False\n",
      "Here resnet50.bn1.bias False\n",
      "Here resnet50.layer1.0.conv1.weight False\n",
      "Here resnet50.layer1.0.bn1.weight False\n",
      "Here resnet50.layer1.0.bn1.bias False\n",
      "Here resnet50.layer1.0.conv2.weight False\n",
      "Here resnet50.layer1.0.bn2.weight False\n",
      "Here resnet50.layer1.0.bn2.bias False\n",
      "Here resnet50.layer1.0.conv3.weight False\n",
      "Here resnet50.layer1.0.bn3.weight False\n",
      "Here resnet50.layer1.0.bn3.bias False\n",
      "Here resnet50.layer1.0.downsample.0.weight False\n",
      "Here resnet50.layer1.0.downsample.1.weight False\n",
      "Here resnet50.layer1.0.downsample.1.bias False\n",
      "Here resnet50.layer1.1.conv1.weight False\n",
      "Here resnet50.layer1.1.bn1.weight False\n",
      "Here resnet50.layer1.1.bn1.bias False\n",
      "Here resnet50.layer1.1.conv2.weight False\n",
      "Here resnet50.layer1.1.bn2.weight False\n",
      "Here resnet50.layer1.1.bn2.bias False\n",
      "Here resnet50.layer1.1.conv3.weight False\n",
      "Here resnet50.layer1.1.bn3.weight False\n",
      "Here resnet50.layer1.1.bn3.bias False\n",
      "Here resnet50.layer1.2.conv1.weight False\n",
      "Here resnet50.layer1.2.bn1.weight False\n",
      "Here resnet50.layer1.2.bn1.bias False\n",
      "Here resnet50.layer1.2.conv2.weight False\n",
      "Here resnet50.layer1.2.bn2.weight False\n",
      "Here resnet50.layer1.2.bn2.bias False\n",
      "Here resnet50.layer1.2.conv3.weight False\n",
      "Here resnet50.layer1.2.bn3.weight False\n",
      "Here resnet50.layer1.2.bn3.bias False\n",
      "Here resnet50.layer2.0.conv1.weight False\n",
      "Here resnet50.layer2.0.bn1.weight False\n",
      "Here resnet50.layer2.0.bn1.bias False\n",
      "Here resnet50.layer2.0.conv2.weight False\n",
      "Here resnet50.layer2.0.bn2.weight False\n",
      "Here resnet50.layer2.0.bn2.bias False\n",
      "Here resnet50.layer2.0.conv3.weight False\n",
      "Here resnet50.layer2.0.bn3.weight False\n",
      "Here resnet50.layer2.0.bn3.bias False\n",
      "Here resnet50.layer2.0.downsample.0.weight False\n",
      "Here resnet50.layer2.0.downsample.1.weight False\n",
      "Here resnet50.layer2.0.downsample.1.bias False\n",
      "Here resnet50.layer2.1.conv1.weight False\n",
      "Here resnet50.layer2.1.bn1.weight False\n",
      "Here resnet50.layer2.1.bn1.bias False\n",
      "Here resnet50.layer2.1.conv2.weight False\n",
      "Here resnet50.layer2.1.bn2.weight False\n",
      "Here resnet50.layer2.1.bn2.bias False\n",
      "Here resnet50.layer2.1.conv3.weight False\n",
      "Here resnet50.layer2.1.bn3.weight False\n",
      "Here resnet50.layer2.1.bn3.bias False\n",
      "Here resnet50.layer2.2.conv1.weight False\n",
      "Here resnet50.layer2.2.bn1.weight False\n",
      "Here resnet50.layer2.2.bn1.bias False\n",
      "Here resnet50.layer2.2.conv2.weight False\n",
      "Here resnet50.layer2.2.bn2.weight False\n",
      "Here resnet50.layer2.2.bn2.bias False\n",
      "Here resnet50.layer2.2.conv3.weight False\n",
      "Here resnet50.layer2.2.bn3.weight False\n",
      "Here resnet50.layer2.2.bn3.bias False\n",
      "Here resnet50.layer2.3.conv1.weight False\n",
      "Here resnet50.layer2.3.bn1.weight False\n",
      "Here resnet50.layer2.3.bn1.bias False\n",
      "Here resnet50.layer2.3.conv2.weight False\n",
      "Here resnet50.layer2.3.bn2.weight False\n",
      "Here resnet50.layer2.3.bn2.bias False\n",
      "Here resnet50.layer2.3.conv3.weight False\n",
      "Here resnet50.layer2.3.bn3.weight False\n",
      "Here resnet50.layer2.3.bn3.bias False\n",
      "Here resnet50.layer3.0.conv1.weight False\n",
      "Here resnet50.layer3.0.bn1.weight False\n",
      "Here resnet50.layer3.0.bn1.bias False\n",
      "Here resnet50.layer3.0.conv2.weight False\n",
      "Here resnet50.layer3.0.bn2.weight False\n",
      "Here resnet50.layer3.0.bn2.bias False\n",
      "Here resnet50.layer3.0.conv3.weight False\n",
      "Here resnet50.layer3.0.bn3.weight False\n",
      "Here resnet50.layer3.0.bn3.bias False\n",
      "Here resnet50.layer3.0.downsample.0.weight False\n",
      "Here resnet50.layer3.0.downsample.1.weight False\n",
      "Here resnet50.layer3.0.downsample.1.bias False\n",
      "Here resnet50.layer3.1.conv1.weight False\n",
      "Here resnet50.layer3.1.bn1.weight False\n",
      "Here resnet50.layer3.1.bn1.bias False\n",
      "Here resnet50.layer3.1.conv2.weight False\n",
      "Here resnet50.layer3.1.bn2.weight False\n",
      "Here resnet50.layer3.1.bn2.bias False\n",
      "Here resnet50.layer3.1.conv3.weight False\n",
      "Here resnet50.layer3.1.bn3.weight False\n",
      "Here resnet50.layer3.1.bn3.bias False\n",
      "Here resnet50.layer3.2.conv1.weight False\n",
      "Here resnet50.layer3.2.bn1.weight False\n",
      "Here resnet50.layer3.2.bn1.bias False\n",
      "Here resnet50.layer3.2.conv2.weight False\n",
      "Here resnet50.layer3.2.bn2.weight False\n",
      "Here resnet50.layer3.2.bn2.bias False\n",
      "Here resnet50.layer3.2.conv3.weight False\n",
      "Here resnet50.layer3.2.bn3.weight False\n",
      "Here resnet50.layer3.2.bn3.bias False\n",
      "Here resnet50.layer3.3.conv1.weight False\n",
      "Here resnet50.layer3.3.bn1.weight False\n",
      "Here resnet50.layer3.3.bn1.bias False\n",
      "Here resnet50.layer3.3.conv2.weight False\n",
      "Here resnet50.layer3.3.bn2.weight False\n",
      "Here resnet50.layer3.3.bn2.bias False\n",
      "Here resnet50.layer3.3.conv3.weight False\n",
      "Here resnet50.layer3.3.bn3.weight False\n",
      "Here resnet50.layer3.3.bn3.bias False\n",
      "Here resnet50.layer3.4.conv1.weight False\n",
      "Here resnet50.layer3.4.bn1.weight False\n",
      "Here resnet50.layer3.4.bn1.bias False\n",
      "Here resnet50.layer3.4.conv2.weight False\n",
      "Here resnet50.layer3.4.bn2.weight False\n",
      "Here resnet50.layer3.4.bn2.bias False\n",
      "Here resnet50.layer3.4.conv3.weight False\n",
      "Here resnet50.layer3.4.bn3.weight False\n",
      "Here resnet50.layer3.4.bn3.bias False\n",
      "Here resnet50.layer3.5.conv1.weight False\n",
      "Here resnet50.layer3.5.bn1.weight False\n",
      "Here resnet50.layer3.5.bn1.bias False\n",
      "Here resnet50.layer3.5.conv2.weight False\n",
      "Here resnet50.layer3.5.bn2.weight False\n",
      "Here resnet50.layer3.5.bn2.bias False\n",
      "Here resnet50.layer3.5.conv3.weight False\n",
      "Here resnet50.layer3.5.bn3.weight False\n",
      "Here resnet50.layer3.5.bn3.bias False\n",
      "Here resnet50.layer4.0.conv1.weight True\n",
      "Here resnet50.layer4.0.bn1.weight True\n",
      "Here resnet50.layer4.0.bn1.bias True\n",
      "Here resnet50.layer4.0.conv2.weight True\n",
      "Here resnet50.layer4.0.bn2.weight True\n",
      "Here resnet50.layer4.0.bn2.bias True\n",
      "Here resnet50.layer4.0.conv3.weight True\n",
      "Here resnet50.layer4.0.bn3.weight True\n",
      "Here resnet50.layer4.0.bn3.bias True\n",
      "Here resnet50.layer4.0.downsample.0.weight True\n",
      "Here resnet50.layer4.0.downsample.1.weight True\n",
      "Here resnet50.layer4.0.downsample.1.bias True\n",
      "Here resnet50.layer4.1.conv1.weight True\n",
      "Here resnet50.layer4.1.bn1.weight True\n",
      "Here resnet50.layer4.1.bn1.bias True\n",
      "Here resnet50.layer4.1.conv2.weight True\n",
      "Here resnet50.layer4.1.bn2.weight True\n",
      "Here resnet50.layer4.1.bn2.bias True\n",
      "Here resnet50.layer4.1.conv3.weight True\n",
      "Here resnet50.layer4.1.bn3.weight True\n",
      "Here resnet50.layer4.1.bn3.bias True\n",
      "resnet50.layer4.2.conv1.weight True\n",
      "resnet50.layer4.2.bn1.weight True\n",
      "resnet50.layer4.2.bn1.bias True\n",
      "resnet50.layer4.2.conv2.weight True\n",
      "resnet50.layer4.2.bn2.weight True\n",
      "resnet50.layer4.2.bn2.bias True\n",
      "resnet50.layer4.2.conv3.weight True\n",
      "resnet50.layer4.2.bn3.weight True\n",
      "resnet50.layer4.2.bn3.bias True\n",
      "resnet50.fc.weight True\n",
      "resnet50.fc.bias True\n",
      "Here final.2.weight True\n",
      "Here final.2.bias True\n",
      "Here final.5.weight True\n",
      "Here final.5.bias True\n"
     ]
    }
   ],
   "source": [
    "model = ImageClassifier(13)\n",
    "\n",
    "#print(model(my_dataset[0][0].unsqueeze(0)))\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for name, module in model.named_parameters():\n",
    "    if any(sub_string in name for sub_string in ['resnet50.layer4.2', 'resnet50.fc' ]):\n",
    "        print(name, module.requires_grad)\n",
    "    else:\n",
    "        print('Here', name, module.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1):\n",
    "    torch.cuda.memory_summary(device=device, abbreviated=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    # Only want the parameters in fc to be updated\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=30, )\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "    \n",
    "    # Prints an validation score\n",
    "    print(f\"Initial validation accuracy score{accuracy_score_from_valiadation(model, validation_loader)}\")\n",
    "\n",
    "    #Create a dictionary to store the best model parameters\n",
    "    best_model_parameters = {'Epoch':-1, 'Accuracy':0, 'Parameters':model.state_dict()}\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = F.cross_entropy(model(features), labels)\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "        \n",
    "        # Print the validation loss\n",
    "        accuracy = accuracy_score_from_valiadation(model, validation_loader)\n",
    "        print(f\"Epoch {epoch}, validation accuracy score{accuracy}\")\n",
    "\n",
    "        # Check if the model has the best perfomrance and save the parameters to 'best_model.pt'\n",
    "        if accuracy > best_model_parameters['Accuracy']:\n",
    "            best_model_parameters['Epoch'] = epoch\n",
    "            best_model_parameters['Accuracy'] = loss.item()\n",
    "            best_model_parameters['Parameters'] = model.state_dict()\n",
    "            torch.save(model.state_dict(), 'model_evaluation/weights/best_model.pt')\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch % 50)\n",
    "            # Create an instance of the datetime class\n",
    "            dt = datetime.now()\n",
    "            date_stamp = str(dt).replace(':', '_').replace('.', '_').replace(' ', '_')\n",
    "\n",
    "            # Save the model parameters to the folder 'model_evaluation/weights', along with the time and epoch they were generated\n",
    "            torch.save(model.state_dict(), f'model_evaluation/weights/model_{date_stamp}_epoch_{epoch}_accuracy_{accuracy}.pt') # Is there a better way to do this?\n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    validation_data = next(iter(validation_loader))\n",
    "\n",
    "    features, labels = validation_data\n",
    "\n",
    "    features = features.to(device)\n",
    "    labels = labels\n",
    "\n",
    "    predictions = [torch.argmax(prediction).cpu() for prediction in model(features)]\n",
    "\n",
    "    #predictions.to('cpu')\n",
    "    return accuracy_score(predictions, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score0.0703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch index: 0, learning rate: [1], loss:2.5718727111816406\n",
      "Epoch: 0, batch index: 1, learning rate: [1], loss:2.564833402633667\n",
      "Epoch: 0, batch index: 2, learning rate: [1], loss:2.5673069953918457\n",
      "Epoch: 0, batch index: 3, learning rate: [1], loss:2.5496344566345215\n",
      "Epoch: 0, batch index: 4, learning rate: [1], loss:2.530001640319824\n",
      "Epoch: 0, batch index: 5, learning rate: [1], loss:2.5343739986419678\n",
      "Epoch: 0, batch index: 6, learning rate: [1], loss:2.5364067554473877\n",
      "Epoch: 0, batch index: 7, learning rate: [1], loss:2.525616407394409\n",
      "Epoch: 0, batch index: 8, learning rate: [1], loss:2.5332541465759277\n",
      "Epoch: 0, batch index: 9, learning rate: [1], loss:2.510932207107544\n",
      "Epoch: 0, batch index: 10, learning rate: [1], loss:2.51045560836792\n",
      "Epoch: 0, batch index: 11, learning rate: [1], loss:2.549928903579712\n",
      "Epoch: 0, batch index: 12, learning rate: [1], loss:2.5228328704833984\n",
      "Epoch: 0, batch index: 13, learning rate: [1], loss:2.517366647720337\n",
      "Epoch: 0, batch index: 14, learning rate: [1], loss:2.502446413040161\n",
      "Epoch: 0, batch index: 15, learning rate: [1], loss:2.4595985412597656\n",
      "Epoch 0, validation accuracy score0.1640625\n",
      "0\n",
      "Epoch: 1, batch index: 16, learning rate: [1], loss:2.477971315383911\n",
      "Epoch: 1, batch index: 17, learning rate: [1], loss:2.460866689682007\n",
      "Epoch: 1, batch index: 18, learning rate: [1], loss:2.4507601261138916\n",
      "Epoch: 1, batch index: 19, learning rate: [1], loss:2.497554302215576\n",
      "Epoch: 1, batch index: 20, learning rate: [1], loss:2.4708468914031982\n",
      "Epoch: 1, batch index: 21, learning rate: [1], loss:2.4494497776031494\n",
      "Epoch: 1, batch index: 22, learning rate: [1], loss:2.5052976608276367\n",
      "Epoch: 1, batch index: 23, learning rate: [1], loss:2.4972870349884033\n",
      "Epoch: 1, batch index: 24, learning rate: [1], loss:2.485097885131836\n",
      "Epoch: 1, batch index: 25, learning rate: [1], loss:2.502042770385742\n",
      "Epoch: 1, batch index: 26, learning rate: [1], loss:2.4619526863098145\n",
      "Epoch: 1, batch index: 27, learning rate: [1], loss:2.458401679992676\n",
      "Epoch: 1, batch index: 28, learning rate: [1], loss:2.4944159984588623\n",
      "Epoch: 1, batch index: 29, learning rate: [1], loss:2.4785165786743164\n",
      "Epoch: 1, batch index: 30, learning rate: [1], loss:2.4571099281311035\n",
      "Epoch: 1, batch index: 31, learning rate: [1], loss:2.465343952178955\n",
      "Epoch 1, validation accuracy score0.197265625\n",
      "Epoch: 2, batch index: 32, learning rate: [1], loss:2.4588675498962402\n",
      "Epoch: 2, batch index: 33, learning rate: [1], loss:2.428379774093628\n",
      "Epoch: 2, batch index: 34, learning rate: [1], loss:2.4290571212768555\n",
      "Epoch: 2, batch index: 35, learning rate: [1], loss:2.4694018363952637\n",
      "Epoch: 2, batch index: 36, learning rate: [1], loss:2.4450762271881104\n",
      "Epoch: 2, batch index: 37, learning rate: [1], loss:2.4638445377349854\n",
      "Epoch: 2, batch index: 38, learning rate: [1], loss:2.4382073879241943\n",
      "Epoch: 2, batch index: 39, learning rate: [1], loss:2.4441728591918945\n",
      "Epoch: 2, batch index: 40, learning rate: [1], loss:2.454362630844116\n",
      "Epoch: 2, batch index: 41, learning rate: [1], loss:2.4402687549591064\n",
      "Epoch: 2, batch index: 42, learning rate: [1], loss:2.4438602924346924\n",
      "Epoch: 2, batch index: 43, learning rate: [1], loss:2.4777369499206543\n",
      "Epoch: 2, batch index: 44, learning rate: [1], loss:2.45892333984375\n",
      "Epoch: 2, batch index: 45, learning rate: [1], loss:2.426642894744873\n",
      "Epoch: 2, batch index: 46, learning rate: [1], loss:2.4348819255828857\n",
      "Epoch: 2, batch index: 47, learning rate: [1], loss:2.435359477996826\n",
      "Epoch 2, validation accuracy score0.1953125\n",
      "Epoch: 3, batch index: 48, learning rate: [1], loss:2.4124045372009277\n",
      "Epoch: 3, batch index: 49, learning rate: [1], loss:2.4294614791870117\n",
      "Epoch: 3, batch index: 50, learning rate: [1], loss:2.395569324493408\n",
      "Epoch: 3, batch index: 51, learning rate: [1], loss:2.4154012203216553\n",
      "Epoch: 3, batch index: 52, learning rate: [1], loss:2.4189610481262207\n",
      "Epoch: 3, batch index: 53, learning rate: [1], loss:2.4487197399139404\n",
      "Epoch: 3, batch index: 54, learning rate: [1], loss:2.4339349269866943\n",
      "Epoch: 3, batch index: 55, learning rate: [1], loss:2.4202213287353516\n",
      "Epoch: 3, batch index: 56, learning rate: [1], loss:2.417203664779663\n",
      "Epoch: 3, batch index: 57, learning rate: [1], loss:2.41709041595459\n",
      "Epoch: 3, batch index: 58, learning rate: [1], loss:2.4135959148406982\n",
      "Epoch: 3, batch index: 59, learning rate: [1], loss:2.4201810359954834\n",
      "Epoch: 3, batch index: 60, learning rate: [1], loss:2.4013779163360596\n",
      "Epoch: 3, batch index: 61, learning rate: [1], loss:2.427244186401367\n",
      "Epoch: 3, batch index: 62, learning rate: [1], loss:2.376461982727051\n",
      "Epoch: 3, batch index: 63, learning rate: [1], loss:2.47027325630188\n",
      "Epoch 3, validation accuracy score0.232421875\n",
      "Epoch: 4, batch index: 64, learning rate: [1], loss:2.398380756378174\n",
      "Epoch: 4, batch index: 65, learning rate: [1], loss:2.4166183471679688\n",
      "Epoch: 4, batch index: 66, learning rate: [1], loss:2.416987180709839\n",
      "Epoch: 4, batch index: 67, learning rate: [1], loss:2.4133386611938477\n",
      "Epoch: 4, batch index: 68, learning rate: [1], loss:2.3802947998046875\n",
      "Epoch: 4, batch index: 69, learning rate: [1], loss:2.382383346557617\n",
      "Epoch: 4, batch index: 70, learning rate: [1], loss:2.389960765838623\n",
      "Epoch: 4, batch index: 71, learning rate: [1], loss:2.4170594215393066\n",
      "Epoch: 4, batch index: 72, learning rate: [1], loss:2.3897881507873535\n",
      "Epoch: 4, batch index: 73, learning rate: [1], loss:2.38102388381958\n",
      "Epoch: 4, batch index: 74, learning rate: [1], loss:2.426936149597168\n",
      "Epoch: 4, batch index: 75, learning rate: [1], loss:2.3994667530059814\n",
      "Epoch: 4, batch index: 76, learning rate: [1], loss:2.4163691997528076\n",
      "Epoch: 4, batch index: 77, learning rate: [1], loss:2.3998818397521973\n",
      "Epoch: 4, batch index: 78, learning rate: [1], loss:2.412008047103882\n",
      "Epoch: 4, batch index: 79, learning rate: [1], loss:2.421621322631836\n",
      "Epoch 4, validation accuracy score0.30078125\n",
      "Epoch: 5, batch index: 80, learning rate: [1], loss:2.3786888122558594\n",
      "Epoch: 5, batch index: 81, learning rate: [1], loss:2.395286798477173\n",
      "Epoch: 5, batch index: 82, learning rate: [1], loss:2.398167610168457\n",
      "Epoch: 5, batch index: 83, learning rate: [1], loss:2.422628402709961\n",
      "Epoch: 5, batch index: 84, learning rate: [1], loss:2.387402057647705\n",
      "Epoch: 5, batch index: 85, learning rate: [1], loss:2.4072458744049072\n",
      "Epoch: 5, batch index: 86, learning rate: [1], loss:2.364509105682373\n",
      "Epoch: 5, batch index: 87, learning rate: [1], loss:2.391636848449707\n",
      "Epoch: 5, batch index: 88, learning rate: [1], loss:2.402571678161621\n",
      "Epoch: 5, batch index: 89, learning rate: [1], loss:2.3726391792297363\n",
      "Epoch: 5, batch index: 90, learning rate: [1], loss:2.401965618133545\n",
      "Epoch: 5, batch index: 91, learning rate: [1], loss:2.3819773197174072\n",
      "Epoch: 5, batch index: 92, learning rate: [1], loss:2.374967098236084\n",
      "Epoch: 5, batch index: 93, learning rate: [1], loss:2.377135753631592\n",
      "Epoch: 5, batch index: 94, learning rate: [1], loss:2.3536689281463623\n",
      "Epoch: 5, batch index: 95, learning rate: [1], loss:2.369572401046753\n",
      "Epoch 5, validation accuracy score0.287109375\n",
      "Epoch: 6, batch index: 96, learning rate: [1], loss:2.3369193077087402\n",
      "Epoch: 6, batch index: 97, learning rate: [1], loss:2.382172107696533\n",
      "Epoch: 6, batch index: 98, learning rate: [1], loss:2.355666160583496\n",
      "Epoch: 6, batch index: 99, learning rate: [1], loss:2.395360231399536\n",
      "Epoch: 6, batch index: 100, learning rate: [1], loss:2.3487212657928467\n",
      "Epoch: 6, batch index: 101, learning rate: [1], loss:2.376030206680298\n",
      "Epoch: 6, batch index: 102, learning rate: [1], loss:2.405097007751465\n",
      "Epoch: 6, batch index: 103, learning rate: [1], loss:2.378861665725708\n",
      "Epoch: 6, batch index: 104, learning rate: [1], loss:2.358285903930664\n",
      "Epoch: 6, batch index: 105, learning rate: [1], loss:2.3767149448394775\n",
      "Epoch: 6, batch index: 106, learning rate: [1], loss:2.356339693069458\n",
      "Epoch: 6, batch index: 107, learning rate: [1], loss:2.3724210262298584\n",
      "Epoch: 6, batch index: 108, learning rate: [1], loss:2.3362913131713867\n",
      "Epoch: 6, batch index: 109, learning rate: [1], loss:2.349503993988037\n",
      "Epoch: 6, batch index: 110, learning rate: [1], loss:2.385465383529663\n",
      "Epoch: 6, batch index: 111, learning rate: [1], loss:2.393068790435791\n",
      "Epoch 6, validation accuracy score0.283203125\n",
      "Epoch: 7, batch index: 112, learning rate: [1], loss:2.372612714767456\n",
      "Epoch: 7, batch index: 113, learning rate: [1], loss:2.361107110977173\n",
      "Epoch: 7, batch index: 114, learning rate: [1], loss:2.334465503692627\n",
      "Epoch: 7, batch index: 115, learning rate: [1], loss:2.3265273571014404\n",
      "Epoch: 7, batch index: 116, learning rate: [1], loss:2.3797430992126465\n",
      "Epoch: 7, batch index: 117, learning rate: [1], loss:2.3542003631591797\n",
      "Epoch: 7, batch index: 118, learning rate: [1], loss:2.3848025798797607\n",
      "Epoch: 7, batch index: 119, learning rate: [1], loss:2.345433473587036\n",
      "Epoch: 7, batch index: 120, learning rate: [1], loss:2.3793251514434814\n",
      "Epoch: 7, batch index: 121, learning rate: [1], loss:2.396042585372925\n",
      "Epoch: 7, batch index: 122, learning rate: [1], loss:2.394217014312744\n",
      "Epoch: 7, batch index: 123, learning rate: [1], loss:2.371650457382202\n",
      "Epoch: 7, batch index: 124, learning rate: [1], loss:2.371328115463257\n",
      "Epoch: 7, batch index: 125, learning rate: [1], loss:2.3472352027893066\n",
      "Epoch: 7, batch index: 126, learning rate: [1], loss:2.404477834701538\n",
      "Epoch: 7, batch index: 127, learning rate: [1], loss:2.335439920425415\n",
      "Epoch 7, validation accuracy score0.28515625\n",
      "Epoch: 8, batch index: 128, learning rate: [1], loss:2.3388075828552246\n",
      "Epoch: 8, batch index: 129, learning rate: [1], loss:2.355250597000122\n",
      "Epoch: 8, batch index: 130, learning rate: [1], loss:2.3524491786956787\n",
      "Epoch: 8, batch index: 131, learning rate: [1], loss:2.4057178497314453\n",
      "Epoch: 8, batch index: 132, learning rate: [1], loss:2.3854801654815674\n",
      "Epoch: 8, batch index: 133, learning rate: [1], loss:2.3950908184051514\n",
      "Epoch: 8, batch index: 134, learning rate: [1], loss:2.3544657230377197\n",
      "Epoch: 8, batch index: 135, learning rate: [1], loss:2.3623969554901123\n",
      "Epoch: 8, batch index: 136, learning rate: [1], loss:2.3357651233673096\n",
      "Epoch: 8, batch index: 137, learning rate: [1], loss:2.3597733974456787\n",
      "Epoch: 8, batch index: 138, learning rate: [1], loss:2.3449785709381104\n",
      "Epoch: 8, batch index: 139, learning rate: [1], loss:2.3552913665771484\n",
      "Epoch: 8, batch index: 140, learning rate: [1], loss:2.3682661056518555\n",
      "Epoch: 8, batch index: 141, learning rate: [1], loss:2.350623369216919\n",
      "Epoch: 8, batch index: 142, learning rate: [1], loss:2.365776777267456\n",
      "Epoch: 8, batch index: 143, learning rate: [1], loss:2.384873390197754\n",
      "Epoch 8, validation accuracy score0.279296875\n",
      "Epoch: 9, batch index: 144, learning rate: [1], loss:2.3699986934661865\n",
      "Epoch: 9, batch index: 145, learning rate: [1], loss:2.342949390411377\n",
      "Epoch: 9, batch index: 146, learning rate: [1], loss:2.361215591430664\n",
      "Epoch: 9, batch index: 147, learning rate: [1], loss:2.356935977935791\n",
      "Epoch: 9, batch index: 148, learning rate: [1], loss:2.3446545600891113\n",
      "Epoch: 9, batch index: 149, learning rate: [1], loss:2.3426849842071533\n",
      "Epoch: 9, batch index: 150, learning rate: [1], loss:2.377345561981201\n",
      "Epoch: 9, batch index: 151, learning rate: [1], loss:2.360154151916504\n",
      "Epoch: 9, batch index: 152, learning rate: [1], loss:2.378594160079956\n",
      "Epoch: 9, batch index: 153, learning rate: [1], loss:2.3646554946899414\n",
      "Epoch: 9, batch index: 154, learning rate: [1], loss:2.3674700260162354\n",
      "Epoch: 9, batch index: 155, learning rate: [1], loss:2.404564619064331\n",
      "Epoch: 9, batch index: 156, learning rate: [1], loss:2.3452749252319336\n",
      "Epoch: 9, batch index: 157, learning rate: [1], loss:2.3835387229919434\n",
      "Epoch: 9, batch index: 158, learning rate: [1], loss:2.3695194721221924\n",
      "Epoch: 9, batch index: 159, learning rate: [1], loss:2.3777010440826416\n",
      "Epoch 9, validation accuracy score0.275390625\n",
      "Epoch: 10, batch index: 160, learning rate: [1], loss:2.3249990940093994\n",
      "Epoch: 10, batch index: 161, learning rate: [1], loss:2.3733279705047607\n",
      "Epoch: 10, batch index: 162, learning rate: [1], loss:2.325460910797119\n",
      "Epoch: 10, batch index: 163, learning rate: [1], loss:2.3721227645874023\n",
      "Epoch: 10, batch index: 164, learning rate: [1], loss:2.3850018978118896\n",
      "Epoch: 10, batch index: 165, learning rate: [1], loss:2.3614964485168457\n",
      "Epoch: 10, batch index: 166, learning rate: [1], loss:2.3449652194976807\n",
      "Epoch: 10, batch index: 167, learning rate: [1], loss:2.345425605773926\n",
      "Epoch: 10, batch index: 168, learning rate: [1], loss:2.3325090408325195\n",
      "Epoch: 10, batch index: 169, learning rate: [1], loss:2.3603360652923584\n",
      "Epoch: 10, batch index: 170, learning rate: [1], loss:2.3753390312194824\n",
      "Epoch: 10, batch index: 171, learning rate: [1], loss:2.3648579120635986\n",
      "Epoch: 10, batch index: 172, learning rate: [1], loss:2.362689971923828\n",
      "Epoch: 10, batch index: 173, learning rate: [1], loss:2.365943670272827\n",
      "Epoch: 10, batch index: 174, learning rate: [1], loss:2.3636460304260254\n",
      "Epoch: 10, batch index: 175, learning rate: [1], loss:2.3032851219177246\n",
      "Epoch 10, validation accuracy score0.302734375\n",
      "Epoch: 11, batch index: 176, learning rate: [1], loss:2.36431884765625\n",
      "Epoch: 11, batch index: 177, learning rate: [1], loss:2.3470773696899414\n",
      "Epoch: 11, batch index: 178, learning rate: [1], loss:2.343872547149658\n",
      "Epoch: 11, batch index: 179, learning rate: [1], loss:2.3694236278533936\n",
      "Epoch: 11, batch index: 180, learning rate: [1], loss:2.3766121864318848\n",
      "Epoch: 11, batch index: 181, learning rate: [1], loss:2.397242307662964\n",
      "Epoch: 11, batch index: 182, learning rate: [1], loss:2.3435897827148438\n",
      "Epoch: 11, batch index: 183, learning rate: [1], loss:2.3237736225128174\n",
      "Epoch: 11, batch index: 184, learning rate: [1], loss:2.3512015342712402\n",
      "Epoch: 11, batch index: 185, learning rate: [1], loss:2.3770580291748047\n",
      "Epoch: 11, batch index: 186, learning rate: [1], loss:2.3815436363220215\n",
      "Epoch: 11, batch index: 187, learning rate: [1], loss:2.368997097015381\n",
      "Epoch: 11, batch index: 188, learning rate: [1], loss:2.395409345626831\n",
      "Epoch: 11, batch index: 189, learning rate: [1], loss:2.3756840229034424\n",
      "Epoch: 11, batch index: 190, learning rate: [1], loss:2.3691797256469727\n",
      "Epoch: 11, batch index: 191, learning rate: [1], loss:2.351592779159546\n",
      "Epoch 11, validation accuracy score0.298828125\n",
      "Epoch: 12, batch index: 192, learning rate: [1], loss:2.38435435295105\n",
      "Epoch: 12, batch index: 193, learning rate: [1], loss:2.335402488708496\n",
      "Epoch: 12, batch index: 194, learning rate: [1], loss:2.3425981998443604\n",
      "Epoch: 12, batch index: 195, learning rate: [1], loss:2.336367130279541\n",
      "Epoch: 12, batch index: 196, learning rate: [1], loss:2.3121635913848877\n",
      "Epoch: 12, batch index: 197, learning rate: [1], loss:2.361783742904663\n",
      "Epoch: 12, batch index: 198, learning rate: [1], loss:2.3794846534729004\n",
      "Epoch: 12, batch index: 199, learning rate: [1], loss:2.341524124145508\n",
      "Epoch: 12, batch index: 200, learning rate: [1], loss:2.3352720737457275\n",
      "Epoch: 12, batch index: 201, learning rate: [1], loss:2.3663508892059326\n",
      "Epoch: 12, batch index: 202, learning rate: [1], loss:2.368365526199341\n",
      "Epoch: 12, batch index: 203, learning rate: [1], loss:2.382275104522705\n",
      "Epoch: 12, batch index: 204, learning rate: [1], loss:2.362299919128418\n",
      "Epoch: 12, batch index: 205, learning rate: [1], loss:2.353414535522461\n",
      "Epoch: 12, batch index: 206, learning rate: [1], loss:2.3746275901794434\n",
      "Epoch: 12, batch index: 207, learning rate: [1], loss:2.3847992420196533\n",
      "Epoch 12, validation accuracy score0.296875\n",
      "Epoch: 13, batch index: 208, learning rate: [1], loss:2.3499841690063477\n",
      "Epoch: 13, batch index: 209, learning rate: [1], loss:2.3316283226013184\n",
      "Epoch: 13, batch index: 210, learning rate: [1], loss:2.295234441757202\n",
      "Epoch: 13, batch index: 211, learning rate: [1], loss:2.3296501636505127\n",
      "Epoch: 13, batch index: 212, learning rate: [1], loss:2.3159899711608887\n",
      "Epoch: 13, batch index: 213, learning rate: [1], loss:2.34447979927063\n",
      "Epoch: 13, batch index: 214, learning rate: [1], loss:2.3079535961151123\n",
      "Epoch: 13, batch index: 215, learning rate: [1], loss:2.336779832839966\n",
      "Epoch: 13, batch index: 216, learning rate: [1], loss:2.3580853939056396\n",
      "Epoch: 13, batch index: 217, learning rate: [1], loss:2.360152006149292\n",
      "Epoch: 13, batch index: 218, learning rate: [1], loss:2.371577262878418\n",
      "Epoch: 13, batch index: 219, learning rate: [1], loss:2.3560702800750732\n",
      "Epoch: 13, batch index: 220, learning rate: [1], loss:2.3785603046417236\n",
      "Epoch: 13, batch index: 221, learning rate: [1], loss:2.348151206970215\n",
      "Epoch: 13, batch index: 222, learning rate: [1], loss:2.3731608390808105\n",
      "Epoch: 13, batch index: 223, learning rate: [1], loss:2.3321895599365234\n",
      "Epoch 13, validation accuracy score0.26953125\n",
      "Epoch: 14, batch index: 224, learning rate: [1], loss:2.349689245223999\n",
      "Epoch: 14, batch index: 225, learning rate: [1], loss:2.328622817993164\n",
      "Epoch: 14, batch index: 226, learning rate: [1], loss:2.3582074642181396\n",
      "Epoch: 14, batch index: 227, learning rate: [1], loss:2.3260385990142822\n",
      "Epoch: 14, batch index: 228, learning rate: [1], loss:2.326789617538452\n",
      "Epoch: 14, batch index: 229, learning rate: [1], loss:2.3252172470092773\n",
      "Epoch: 14, batch index: 230, learning rate: [1], loss:2.3487813472747803\n",
      "Epoch: 14, batch index: 231, learning rate: [1], loss:2.345766305923462\n",
      "Epoch: 14, batch index: 232, learning rate: [1], loss:2.30889630317688\n",
      "Epoch: 14, batch index: 233, learning rate: [1], loss:2.3686275482177734\n",
      "Epoch: 14, batch index: 234, learning rate: [1], loss:2.3227999210357666\n",
      "Epoch: 14, batch index: 235, learning rate: [1], loss:2.314891815185547\n",
      "Epoch: 14, batch index: 236, learning rate: [1], loss:2.3728740215301514\n",
      "Epoch: 14, batch index: 237, learning rate: [1], loss:2.325719118118286\n",
      "Epoch: 14, batch index: 238, learning rate: [1], loss:2.347104787826538\n",
      "Epoch: 14, batch index: 239, learning rate: [1], loss:2.323777437210083\n",
      "Epoch 14, validation accuracy score0.28125\n",
      "Epoch: 15, batch index: 240, learning rate: [1], loss:2.3130321502685547\n",
      "Epoch: 15, batch index: 241, learning rate: [1], loss:2.3410325050354004\n",
      "Epoch: 15, batch index: 242, learning rate: [1], loss:2.3007473945617676\n",
      "Epoch: 15, batch index: 243, learning rate: [1], loss:2.3559482097625732\n",
      "Epoch: 15, batch index: 244, learning rate: [1], loss:2.3777518272399902\n",
      "Epoch: 15, batch index: 245, learning rate: [1], loss:2.3520593643188477\n",
      "Epoch: 15, batch index: 246, learning rate: [1], loss:2.3317506313323975\n",
      "Epoch: 15, batch index: 247, learning rate: [1], loss:2.3642523288726807\n",
      "Epoch: 15, batch index: 248, learning rate: [1], loss:2.329514741897583\n",
      "Epoch: 15, batch index: 249, learning rate: [1], loss:2.328322410583496\n",
      "Epoch: 15, batch index: 250, learning rate: [1], loss:2.334627866744995\n",
      "Epoch: 15, batch index: 251, learning rate: [1], loss:2.300806999206543\n",
      "Epoch: 15, batch index: 252, learning rate: [1], loss:2.3393545150756836\n",
      "Epoch: 15, batch index: 253, learning rate: [1], loss:2.3126566410064697\n",
      "Epoch: 15, batch index: 254, learning rate: [1], loss:2.315896987915039\n",
      "Epoch: 15, batch index: 255, learning rate: [1], loss:2.360304594039917\n",
      "Epoch 15, validation accuracy score0.328125\n",
      "Epoch: 16, batch index: 256, learning rate: [1], loss:2.308464527130127\n",
      "Epoch: 16, batch index: 257, learning rate: [1], loss:2.2935245037078857\n",
      "Epoch: 16, batch index: 258, learning rate: [1], loss:2.298200845718384\n",
      "Epoch: 16, batch index: 259, learning rate: [1], loss:2.3564748764038086\n",
      "Epoch: 16, batch index: 260, learning rate: [1], loss:2.3384337425231934\n",
      "Epoch: 16, batch index: 261, learning rate: [1], loss:2.2729921340942383\n",
      "Epoch: 16, batch index: 262, learning rate: [1], loss:2.3286774158477783\n",
      "Epoch: 16, batch index: 263, learning rate: [1], loss:2.346759080886841\n",
      "Epoch: 16, batch index: 264, learning rate: [1], loss:2.338196277618408\n",
      "Epoch: 16, batch index: 265, learning rate: [1], loss:2.309223175048828\n",
      "Epoch: 16, batch index: 266, learning rate: [1], loss:2.334625244140625\n",
      "Epoch: 16, batch index: 267, learning rate: [1], loss:2.2795441150665283\n",
      "Epoch: 16, batch index: 268, learning rate: [1], loss:2.3103368282318115\n",
      "Epoch: 16, batch index: 269, learning rate: [1], loss:2.346747636795044\n",
      "Epoch: 16, batch index: 270, learning rate: [1], loss:2.3467984199523926\n",
      "Epoch: 16, batch index: 271, learning rate: [1], loss:2.308896541595459\n",
      "Epoch 16, validation accuracy score0.279296875\n",
      "Epoch: 17, batch index: 272, learning rate: [1], loss:2.3040361404418945\n",
      "Epoch: 17, batch index: 273, learning rate: [1], loss:2.339492082595825\n",
      "Epoch: 17, batch index: 274, learning rate: [1], loss:2.333355665206909\n",
      "Epoch: 17, batch index: 275, learning rate: [1], loss:2.3515965938568115\n",
      "Epoch: 17, batch index: 276, learning rate: [1], loss:2.2997689247131348\n",
      "Epoch: 17, batch index: 277, learning rate: [1], loss:2.303534507751465\n",
      "Epoch: 17, batch index: 278, learning rate: [1], loss:2.326687812805176\n",
      "Epoch: 17, batch index: 279, learning rate: [1], loss:2.3265035152435303\n",
      "Epoch: 17, batch index: 280, learning rate: [1], loss:2.303248882293701\n",
      "Epoch: 17, batch index: 281, learning rate: [1], loss:2.3094258308410645\n",
      "Epoch: 17, batch index: 282, learning rate: [1], loss:2.3233282566070557\n",
      "Epoch: 17, batch index: 283, learning rate: [1], loss:2.3137497901916504\n",
      "Epoch: 17, batch index: 284, learning rate: [1], loss:2.332474708557129\n",
      "Epoch: 17, batch index: 285, learning rate: [1], loss:2.350642204284668\n",
      "Epoch: 17, batch index: 286, learning rate: [1], loss:2.319843292236328\n",
      "Epoch: 17, batch index: 287, learning rate: [1], loss:2.278744697570801\n",
      "Epoch 17, validation accuracy score0.30859375\n",
      "Epoch: 18, batch index: 288, learning rate: [1], loss:2.333189010620117\n",
      "Epoch: 18, batch index: 289, learning rate: [1], loss:2.3322460651397705\n",
      "Epoch: 18, batch index: 290, learning rate: [1], loss:2.3295371532440186\n",
      "Epoch: 18, batch index: 291, learning rate: [1], loss:2.307699680328369\n",
      "Epoch: 18, batch index: 292, learning rate: [1], loss:2.281698703765869\n",
      "Epoch: 18, batch index: 293, learning rate: [1], loss:2.3081471920013428\n",
      "Epoch: 18, batch index: 294, learning rate: [1], loss:2.3116371631622314\n",
      "Epoch: 18, batch index: 295, learning rate: [1], loss:2.3224241733551025\n",
      "Epoch: 18, batch index: 296, learning rate: [1], loss:2.3354365825653076\n",
      "Epoch: 18, batch index: 297, learning rate: [1], loss:2.2887024879455566\n",
      "Epoch: 18, batch index: 298, learning rate: [1], loss:2.3477396965026855\n",
      "Epoch: 18, batch index: 299, learning rate: [1], loss:2.326827049255371\n",
      "Epoch: 18, batch index: 300, learning rate: [1], loss:2.308088541030884\n",
      "Epoch: 18, batch index: 301, learning rate: [1], loss:2.331453800201416\n",
      "Epoch: 18, batch index: 302, learning rate: [1], loss:2.304593086242676\n",
      "Epoch: 18, batch index: 303, learning rate: [1], loss:2.28672194480896\n",
      "Epoch 18, validation accuracy score0.283203125\n",
      "Epoch: 19, batch index: 304, learning rate: [1], loss:2.296527624130249\n",
      "Epoch: 19, batch index: 305, learning rate: [1], loss:2.3272128105163574\n",
      "Epoch: 19, batch index: 306, learning rate: [1], loss:2.316779851913452\n",
      "Epoch: 19, batch index: 307, learning rate: [1], loss:2.2933008670806885\n",
      "Epoch: 19, batch index: 308, learning rate: [1], loss:2.320103645324707\n",
      "Epoch: 19, batch index: 309, learning rate: [1], loss:2.3151936531066895\n",
      "Epoch: 19, batch index: 310, learning rate: [1], loss:2.308858871459961\n",
      "Epoch: 19, batch index: 311, learning rate: [1], loss:2.3294503688812256\n",
      "Epoch: 19, batch index: 312, learning rate: [1], loss:2.33707332611084\n",
      "Epoch: 19, batch index: 313, learning rate: [0.1], loss:2.3184292316436768\n",
      "Epoch: 19, batch index: 314, learning rate: [0.1], loss:2.3072948455810547\n",
      "Epoch: 19, batch index: 315, learning rate: [0.1], loss:2.313936710357666\n",
      "Epoch: 19, batch index: 316, learning rate: [0.1], loss:2.344312906265259\n",
      "Epoch: 19, batch index: 317, learning rate: [0.1], loss:2.295902967453003\n",
      "Epoch: 19, batch index: 318, learning rate: [0.1], loss:2.3054049015045166\n",
      "Epoch: 19, batch index: 319, learning rate: [0.1], loss:2.2827084064483643\n",
      "Epoch 19, validation accuracy score0.30859375\n",
      "Epoch: 20, batch index: 320, learning rate: [0.1], loss:2.3166728019714355\n",
      "Epoch: 20, batch index: 321, learning rate: [0.1], loss:2.2934067249298096\n",
      "Epoch: 20, batch index: 322, learning rate: [0.1], loss:2.3017470836639404\n",
      "Epoch: 20, batch index: 323, learning rate: [0.1], loss:2.2921578884124756\n",
      "Epoch: 20, batch index: 324, learning rate: [0.1], loss:2.3129167556762695\n",
      "Epoch: 20, batch index: 325, learning rate: [0.1], loss:2.2703874111175537\n",
      "Epoch: 20, batch index: 326, learning rate: [0.1], loss:2.30613112449646\n",
      "Epoch: 20, batch index: 327, learning rate: [0.1], loss:2.310795783996582\n",
      "Epoch: 20, batch index: 328, learning rate: [0.1], loss:2.307115077972412\n",
      "Epoch: 20, batch index: 329, learning rate: [0.1], loss:2.296783685684204\n",
      "Epoch: 20, batch index: 330, learning rate: [0.1], loss:2.3099777698516846\n",
      "Epoch: 20, batch index: 331, learning rate: [0.1], loss:2.2973923683166504\n",
      "Epoch: 20, batch index: 332, learning rate: [0.1], loss:2.341031074523926\n",
      "Epoch: 20, batch index: 333, learning rate: [0.1], loss:2.2839457988739014\n",
      "Epoch: 20, batch index: 334, learning rate: [0.1], loss:2.3102941513061523\n",
      "Epoch: 20, batch index: 335, learning rate: [0.1], loss:2.2736382484436035\n",
      "Epoch 20, validation accuracy score0.314453125\n",
      "Epoch: 21, batch index: 336, learning rate: [0.1], loss:2.3133645057678223\n",
      "Epoch: 21, batch index: 337, learning rate: [0.1], loss:2.284964084625244\n",
      "Epoch: 21, batch index: 338, learning rate: [0.1], loss:2.277031660079956\n",
      "Epoch: 21, batch index: 339, learning rate: [0.1], loss:2.2945504188537598\n",
      "Epoch: 21, batch index: 340, learning rate: [0.1], loss:2.3155314922332764\n",
      "Epoch: 21, batch index: 341, learning rate: [0.1], loss:2.303135395050049\n",
      "Epoch: 21, batch index: 342, learning rate: [0.1], loss:2.2979769706726074\n",
      "Epoch: 21, batch index: 343, learning rate: [0.1], loss:2.2615249156951904\n",
      "Epoch: 21, batch index: 344, learning rate: [0.1], loss:2.2783615589141846\n",
      "Epoch: 21, batch index: 345, learning rate: [0.1], loss:2.302371025085449\n",
      "Epoch: 21, batch index: 346, learning rate: [0.1], loss:2.2963340282440186\n",
      "Epoch: 21, batch index: 347, learning rate: [0.1], loss:2.3466591835021973\n",
      "Epoch: 21, batch index: 348, learning rate: [0.1], loss:2.303936243057251\n",
      "Epoch: 21, batch index: 349, learning rate: [0.1], loss:2.307438373565674\n",
      "Epoch: 21, batch index: 350, learning rate: [0.1], loss:2.2763452529907227\n",
      "Epoch: 21, batch index: 351, learning rate: [0.1], loss:2.2849743366241455\n",
      "Epoch 21, validation accuracy score0.310546875\n",
      "Epoch: 22, batch index: 352, learning rate: [0.1], loss:2.277212142944336\n",
      "Epoch: 22, batch index: 353, learning rate: [0.1], loss:2.259040117263794\n",
      "Epoch: 22, batch index: 354, learning rate: [0.1], loss:2.2845637798309326\n",
      "Epoch: 22, batch index: 355, learning rate: [0.1], loss:2.2847652435302734\n",
      "Epoch: 22, batch index: 356, learning rate: [0.1], loss:2.320756673812866\n",
      "Epoch: 22, batch index: 357, learning rate: [0.1], loss:2.23844575881958\n",
      "Epoch: 22, batch index: 358, learning rate: [0.1], loss:2.323107957839966\n",
      "Epoch: 22, batch index: 359, learning rate: [0.1], loss:2.2783522605895996\n",
      "Epoch: 22, batch index: 360, learning rate: [0.1], loss:2.287792682647705\n",
      "Epoch: 22, batch index: 361, learning rate: [0.1], loss:2.273773431777954\n",
      "Epoch: 22, batch index: 362, learning rate: [0.1], loss:2.268258571624756\n",
      "Epoch: 22, batch index: 363, learning rate: [0.1], loss:2.2981338500976562\n",
      "Epoch: 22, batch index: 364, learning rate: [0.1], loss:2.254215717315674\n",
      "Epoch: 22, batch index: 365, learning rate: [0.1], loss:2.290134906768799\n",
      "Epoch: 22, batch index: 366, learning rate: [0.1], loss:2.2902259826660156\n",
      "Epoch: 22, batch index: 367, learning rate: [0.1], loss:2.3270421028137207\n",
      "Epoch 22, validation accuracy score0.2890625\n",
      "Epoch: 23, batch index: 368, learning rate: [0.1], loss:2.283449172973633\n",
      "Epoch: 23, batch index: 369, learning rate: [0.1], loss:2.2847814559936523\n",
      "Epoch: 23, batch index: 370, learning rate: [0.1], loss:2.282978057861328\n",
      "Epoch: 23, batch index: 371, learning rate: [0.1], loss:2.2663674354553223\n",
      "Epoch: 23, batch index: 372, learning rate: [0.1], loss:2.264617681503296\n",
      "Epoch: 23, batch index: 373, learning rate: [0.1], loss:2.2569000720977783\n",
      "Epoch: 23, batch index: 374, learning rate: [0.1], loss:2.286390542984009\n",
      "Epoch: 23, batch index: 375, learning rate: [0.1], loss:2.2896902561187744\n",
      "Epoch: 23, batch index: 376, learning rate: [0.1], loss:2.2886621952056885\n",
      "Epoch: 23, batch index: 377, learning rate: [0.1], loss:2.274097204208374\n",
      "Epoch: 23, batch index: 378, learning rate: [0.1], loss:2.2784616947174072\n",
      "Epoch: 23, batch index: 379, learning rate: [0.1], loss:2.270216464996338\n",
      "Epoch: 23, batch index: 380, learning rate: [0.1], loss:2.276013135910034\n",
      "Epoch: 23, batch index: 381, learning rate: [0.1], loss:2.319183111190796\n",
      "Epoch: 23, batch index: 382, learning rate: [0.1], loss:2.262348175048828\n",
      "Epoch: 23, batch index: 383, learning rate: [0.1], loss:2.3364100456237793\n",
      "Epoch 23, validation accuracy score0.279296875\n",
      "Epoch: 24, batch index: 384, learning rate: [0.1], loss:2.3011226654052734\n",
      "Epoch: 24, batch index: 385, learning rate: [0.1], loss:2.2594873905181885\n",
      "Epoch: 24, batch index: 386, learning rate: [0.1], loss:2.2983546257019043\n",
      "Epoch: 24, batch index: 387, learning rate: [0.1], loss:2.298694372177124\n",
      "Epoch: 24, batch index: 388, learning rate: [0.1], loss:2.293809652328491\n",
      "Epoch: 24, batch index: 389, learning rate: [0.1], loss:2.2480225563049316\n",
      "Epoch: 24, batch index: 390, learning rate: [0.1], loss:2.29364013671875\n",
      "Epoch: 24, batch index: 391, learning rate: [0.1], loss:2.2975449562072754\n",
      "Epoch: 24, batch index: 392, learning rate: [0.1], loss:2.2805023193359375\n",
      "Epoch: 24, batch index: 393, learning rate: [0.1], loss:2.2541775703430176\n",
      "Epoch: 24, batch index: 394, learning rate: [0.1], loss:2.288762092590332\n",
      "Epoch: 24, batch index: 395, learning rate: [0.1], loss:2.293736457824707\n",
      "Epoch: 24, batch index: 396, learning rate: [0.1], loss:2.246182441711426\n",
      "Epoch: 24, batch index: 397, learning rate: [0.1], loss:2.281214714050293\n",
      "Epoch: 24, batch index: 398, learning rate: [0.1], loss:2.2949564456939697\n",
      "Epoch: 24, batch index: 399, learning rate: [0.1], loss:2.273564577102661\n",
      "Epoch 24, validation accuracy score0.376953125\n",
      "Epoch: 25, batch index: 400, learning rate: [0.1], loss:2.2641143798828125\n",
      "Epoch: 25, batch index: 401, learning rate: [0.1], loss:2.2667694091796875\n",
      "Epoch: 25, batch index: 402, learning rate: [0.1], loss:2.292949676513672\n",
      "Epoch: 25, batch index: 403, learning rate: [0.1], loss:2.2853217124938965\n",
      "Epoch: 25, batch index: 404, learning rate: [0.1], loss:2.265981674194336\n",
      "Epoch: 25, batch index: 405, learning rate: [0.1], loss:2.2776434421539307\n",
      "Epoch: 25, batch index: 406, learning rate: [0.1], loss:2.2673563957214355\n",
      "Epoch: 25, batch index: 407, learning rate: [0.1], loss:2.288968801498413\n",
      "Epoch: 25, batch index: 408, learning rate: [0.1], loss:2.2879443168640137\n",
      "Epoch: 25, batch index: 409, learning rate: [0.010000000000000002], loss:2.2692172527313232\n",
      "Epoch: 25, batch index: 410, learning rate: [0.010000000000000002], loss:2.3104817867279053\n",
      "Epoch: 25, batch index: 411, learning rate: [0.010000000000000002], loss:2.254408597946167\n",
      "Epoch: 25, batch index: 412, learning rate: [0.010000000000000002], loss:2.2217307090759277\n",
      "Epoch: 25, batch index: 413, learning rate: [0.010000000000000002], loss:2.2639710903167725\n",
      "Epoch: 25, batch index: 414, learning rate: [0.010000000000000002], loss:2.2921743392944336\n",
      "Epoch: 25, batch index: 415, learning rate: [0.010000000000000002], loss:2.2145843505859375\n",
      "Epoch 25, validation accuracy score0.31640625\n",
      "Epoch: 26, batch index: 416, learning rate: [0.010000000000000002], loss:2.2882802486419678\n",
      "Epoch: 26, batch index: 417, learning rate: [0.010000000000000002], loss:2.3170413970947266\n",
      "Epoch: 26, batch index: 418, learning rate: [0.010000000000000002], loss:2.283345937728882\n",
      "Epoch: 26, batch index: 419, learning rate: [0.010000000000000002], loss:2.2888715267181396\n",
      "Epoch: 26, batch index: 420, learning rate: [0.010000000000000002], loss:2.252626657485962\n",
      "Epoch: 26, batch index: 421, learning rate: [0.010000000000000002], loss:2.281348943710327\n",
      "Epoch: 26, batch index: 422, learning rate: [0.010000000000000002], loss:2.2588789463043213\n",
      "Epoch: 26, batch index: 423, learning rate: [0.010000000000000002], loss:2.2797703742980957\n",
      "Epoch: 26, batch index: 424, learning rate: [0.010000000000000002], loss:2.3044233322143555\n",
      "Epoch: 26, batch index: 425, learning rate: [0.010000000000000002], loss:2.2557766437530518\n",
      "Epoch: 26, batch index: 426, learning rate: [0.010000000000000002], loss:2.265573024749756\n",
      "Epoch: 26, batch index: 427, learning rate: [0.010000000000000002], loss:2.244934320449829\n",
      "Epoch: 26, batch index: 428, learning rate: [0.010000000000000002], loss:2.289071559906006\n",
      "Epoch: 26, batch index: 429, learning rate: [0.010000000000000002], loss:2.2325611114501953\n",
      "Epoch: 26, batch index: 430, learning rate: [0.010000000000000002], loss:2.2920994758605957\n",
      "Epoch: 26, batch index: 431, learning rate: [0.010000000000000002], loss:2.281904697418213\n",
      "Epoch 26, validation accuracy score0.30078125\n",
      "Epoch: 27, batch index: 432, learning rate: [0.010000000000000002], loss:2.2644236087799072\n",
      "Epoch: 27, batch index: 433, learning rate: [0.010000000000000002], loss:2.274219036102295\n",
      "Epoch: 27, batch index: 434, learning rate: [0.010000000000000002], loss:2.2920098304748535\n",
      "Epoch: 27, batch index: 435, learning rate: [0.010000000000000002], loss:2.2522473335266113\n",
      "Epoch: 27, batch index: 436, learning rate: [0.010000000000000002], loss:2.304198980331421\n",
      "Epoch: 27, batch index: 437, learning rate: [0.010000000000000002], loss:2.247840166091919\n",
      "Epoch: 27, batch index: 438, learning rate: [0.010000000000000002], loss:2.247286558151245\n",
      "Epoch: 27, batch index: 439, learning rate: [0.010000000000000002], loss:2.278212070465088\n",
      "Epoch: 27, batch index: 440, learning rate: [0.010000000000000002], loss:2.293004035949707\n",
      "Epoch: 27, batch index: 441, learning rate: [0.010000000000000002], loss:2.289287805557251\n",
      "Epoch: 27, batch index: 442, learning rate: [0.010000000000000002], loss:2.3032896518707275\n",
      "Epoch: 27, batch index: 443, learning rate: [0.010000000000000002], loss:2.268566370010376\n",
      "Epoch: 27, batch index: 444, learning rate: [0.010000000000000002], loss:2.27445650100708\n",
      "Epoch: 27, batch index: 445, learning rate: [0.010000000000000002], loss:2.273844003677368\n",
      "Epoch: 27, batch index: 446, learning rate: [0.010000000000000002], loss:2.2612497806549072\n",
      "Epoch: 27, batch index: 447, learning rate: [0.010000000000000002], loss:2.3054497241973877\n",
      "Epoch 27, validation accuracy score0.30078125\n",
      "Epoch: 28, batch index: 448, learning rate: [0.010000000000000002], loss:2.2646536827087402\n",
      "Epoch: 28, batch index: 449, learning rate: [0.010000000000000002], loss:2.275254487991333\n",
      "Epoch: 28, batch index: 450, learning rate: [0.010000000000000002], loss:2.2510862350463867\n",
      "Epoch: 28, batch index: 451, learning rate: [0.010000000000000002], loss:2.279327630996704\n",
      "Epoch: 28, batch index: 452, learning rate: [0.010000000000000002], loss:2.257378101348877\n",
      "Epoch: 28, batch index: 453, learning rate: [0.010000000000000002], loss:2.264052391052246\n",
      "Epoch: 28, batch index: 454, learning rate: [0.010000000000000002], loss:2.251098155975342\n",
      "Epoch: 28, batch index: 455, learning rate: [0.010000000000000002], loss:2.2669365406036377\n",
      "Epoch: 28, batch index: 456, learning rate: [0.010000000000000002], loss:2.239474058151245\n",
      "Epoch: 28, batch index: 457, learning rate: [0.010000000000000002], loss:2.2473580837249756\n",
      "Epoch: 28, batch index: 458, learning rate: [0.010000000000000002], loss:2.2707760334014893\n",
      "Epoch: 28, batch index: 459, learning rate: [0.010000000000000002], loss:2.2705061435699463\n",
      "Epoch: 28, batch index: 460, learning rate: [0.010000000000000002], loss:2.2947006225585938\n",
      "Epoch: 28, batch index: 461, learning rate: [0.010000000000000002], loss:2.2633821964263916\n",
      "Epoch: 28, batch index: 462, learning rate: [0.010000000000000002], loss:2.2664122581481934\n",
      "Epoch: 28, batch index: 463, learning rate: [0.010000000000000002], loss:2.2418766021728516\n",
      "Epoch 28, validation accuracy score0.28125\n",
      "Epoch: 29, batch index: 464, learning rate: [0.010000000000000002], loss:2.2540998458862305\n",
      "Epoch: 29, batch index: 465, learning rate: [0.010000000000000002], loss:2.2497479915618896\n",
      "Epoch: 29, batch index: 466, learning rate: [0.010000000000000002], loss:2.2577362060546875\n",
      "Epoch: 29, batch index: 467, learning rate: [0.010000000000000002], loss:2.260969638824463\n",
      "Epoch: 29, batch index: 468, learning rate: [0.010000000000000002], loss:2.2915070056915283\n",
      "Epoch: 29, batch index: 469, learning rate: [0.010000000000000002], loss:2.2353897094726562\n",
      "Epoch: 29, batch index: 470, learning rate: [0.010000000000000002], loss:2.271571397781372\n",
      "Epoch: 29, batch index: 471, learning rate: [0.010000000000000002], loss:2.32494854927063\n",
      "Epoch: 29, batch index: 472, learning rate: [0.010000000000000002], loss:2.2598557472229004\n",
      "Epoch: 29, batch index: 473, learning rate: [0.010000000000000002], loss:2.268563985824585\n",
      "Epoch: 29, batch index: 474, learning rate: [0.010000000000000002], loss:2.2876100540161133\n",
      "Epoch: 29, batch index: 475, learning rate: [0.010000000000000002], loss:2.2746775150299072\n",
      "Epoch: 29, batch index: 476, learning rate: [0.010000000000000002], loss:2.263404130935669\n",
      "Epoch: 29, batch index: 477, learning rate: [0.010000000000000002], loss:2.2674407958984375\n",
      "Epoch: 29, batch index: 478, learning rate: [0.010000000000000002], loss:2.269439220428467\n",
      "Epoch: 29, batch index: 479, learning rate: [0.010000000000000002], loss:2.2864737510681152\n",
      "Epoch 29, validation accuracy score0.294921875\n",
      "Epoch: 30, batch index: 480, learning rate: [0.010000000000000002], loss:2.307724952697754\n",
      "Epoch: 30, batch index: 481, learning rate: [0.010000000000000002], loss:2.2551510334014893\n",
      "Epoch: 30, batch index: 482, learning rate: [0.010000000000000002], loss:2.2295804023742676\n",
      "Epoch: 30, batch index: 483, learning rate: [0.010000000000000002], loss:2.2432162761688232\n",
      "Epoch: 30, batch index: 484, learning rate: [0.010000000000000002], loss:2.273516893386841\n",
      "Epoch: 30, batch index: 485, learning rate: [0.010000000000000002], loss:2.2821431159973145\n",
      "Epoch: 30, batch index: 486, learning rate: [0.010000000000000002], loss:2.2782375812530518\n",
      "Epoch: 30, batch index: 487, learning rate: [0.010000000000000002], loss:2.244415521621704\n",
      "Epoch: 30, batch index: 488, learning rate: [0.010000000000000002], loss:2.288726806640625\n",
      "Epoch: 30, batch index: 489, learning rate: [0.010000000000000002], loss:2.285696029663086\n",
      "Epoch: 30, batch index: 490, learning rate: [0.0010000000000000002], loss:2.3070573806762695\n",
      "Epoch: 30, batch index: 491, learning rate: [0.0010000000000000002], loss:2.2527902126312256\n",
      "Epoch: 30, batch index: 492, learning rate: [0.0010000000000000002], loss:2.2542054653167725\n",
      "Epoch: 30, batch index: 493, learning rate: [0.0010000000000000002], loss:2.259613513946533\n",
      "Epoch: 30, batch index: 494, learning rate: [0.0010000000000000002], loss:2.260171890258789\n",
      "Epoch: 30, batch index: 495, learning rate: [0.0010000000000000002], loss:2.2691264152526855\n",
      "Epoch 30, validation accuracy score0.330078125\n",
      "Epoch: 31, batch index: 496, learning rate: [0.0010000000000000002], loss:2.2827506065368652\n",
      "Epoch: 31, batch index: 497, learning rate: [0.0010000000000000002], loss:2.2861483097076416\n",
      "Epoch: 31, batch index: 498, learning rate: [0.0010000000000000002], loss:2.265549421310425\n",
      "Epoch: 31, batch index: 499, learning rate: [0.0010000000000000002], loss:2.271066188812256\n",
      "Epoch: 31, batch index: 500, learning rate: [0.0010000000000000002], loss:2.2647922039031982\n",
      "Epoch: 31, batch index: 501, learning rate: [0.0010000000000000002], loss:2.256847620010376\n",
      "Epoch: 31, batch index: 502, learning rate: [0.0010000000000000002], loss:2.3070805072784424\n",
      "Epoch: 31, batch index: 503, learning rate: [0.0010000000000000002], loss:2.263399839401245\n",
      "Epoch: 31, batch index: 504, learning rate: [0.0010000000000000002], loss:2.2656352519989014\n",
      "Epoch: 31, batch index: 505, learning rate: [0.0010000000000000002], loss:2.2837307453155518\n",
      "Epoch: 31, batch index: 506, learning rate: [0.0010000000000000002], loss:2.2213385105133057\n",
      "Epoch: 31, batch index: 507, learning rate: [0.0010000000000000002], loss:2.2848005294799805\n",
      "Epoch: 31, batch index: 508, learning rate: [0.0010000000000000002], loss:2.27282977104187\n",
      "Epoch: 31, batch index: 509, learning rate: [0.0010000000000000002], loss:2.2666826248168945\n",
      "Epoch: 31, batch index: 510, learning rate: [0.0010000000000000002], loss:2.305347204208374\n",
      "Epoch: 31, batch index: 511, learning rate: [0.0010000000000000002], loss:2.222372531890869\n",
      "Epoch 31, validation accuracy score0.314453125\n",
      "Epoch: 32, batch index: 512, learning rate: [0.0010000000000000002], loss:2.280456066131592\n",
      "Epoch: 32, batch index: 513, learning rate: [0.0010000000000000002], loss:2.2458624839782715\n",
      "Epoch: 32, batch index: 514, learning rate: [0.0010000000000000002], loss:2.2633087635040283\n",
      "Epoch: 32, batch index: 515, learning rate: [0.0010000000000000002], loss:2.3018264770507812\n",
      "Epoch: 32, batch index: 516, learning rate: [0.0010000000000000002], loss:2.249445676803589\n",
      "Epoch: 32, batch index: 517, learning rate: [0.0010000000000000002], loss:2.224940538406372\n",
      "Epoch: 32, batch index: 518, learning rate: [0.0010000000000000002], loss:2.279620409011841\n",
      "Epoch: 32, batch index: 519, learning rate: [0.0010000000000000002], loss:2.276768207550049\n",
      "Epoch: 32, batch index: 520, learning rate: [0.0010000000000000002], loss:2.253711223602295\n",
      "Epoch: 32, batch index: 521, learning rate: [0.0010000000000000002], loss:2.2812631130218506\n",
      "Epoch: 32, batch index: 522, learning rate: [0.0010000000000000002], loss:2.2761709690093994\n",
      "Epoch: 32, batch index: 523, learning rate: [0.0010000000000000002], loss:2.28051495552063\n",
      "Epoch: 32, batch index: 524, learning rate: [0.0010000000000000002], loss:2.265796184539795\n",
      "Epoch: 32, batch index: 525, learning rate: [0.0010000000000000002], loss:2.2587926387786865\n",
      "Epoch: 32, batch index: 526, learning rate: [0.0010000000000000002], loss:2.2751150131225586\n",
      "Epoch: 32, batch index: 527, learning rate: [0.0010000000000000002], loss:2.298837661743164\n",
      "Epoch 32, validation accuracy score0.27734375\n",
      "Epoch: 33, batch index: 528, learning rate: [0.0010000000000000002], loss:2.2625887393951416\n",
      "Epoch: 33, batch index: 529, learning rate: [0.0010000000000000002], loss:2.293649435043335\n",
      "Epoch: 33, batch index: 530, learning rate: [0.0010000000000000002], loss:2.2667336463928223\n",
      "Epoch: 33, batch index: 531, learning rate: [0.0010000000000000002], loss:2.2688498497009277\n",
      "Epoch: 33, batch index: 532, learning rate: [0.0010000000000000002], loss:2.2604804039001465\n",
      "Epoch: 33, batch index: 533, learning rate: [0.0010000000000000002], loss:2.2675821781158447\n",
      "Epoch: 33, batch index: 534, learning rate: [0.0010000000000000002], loss:2.2611348628997803\n",
      "Epoch: 33, batch index: 535, learning rate: [0.0010000000000000002], loss:2.2995388507843018\n",
      "Epoch: 33, batch index: 536, learning rate: [0.0010000000000000002], loss:2.242758274078369\n",
      "Epoch: 33, batch index: 537, learning rate: [0.0010000000000000002], loss:2.2739155292510986\n",
      "Epoch: 33, batch index: 538, learning rate: [0.0010000000000000002], loss:2.2430574893951416\n",
      "Epoch: 33, batch index: 539, learning rate: [0.0010000000000000002], loss:2.2752585411071777\n",
      "Epoch: 33, batch index: 540, learning rate: [0.0010000000000000002], loss:2.275991201400757\n",
      "Epoch: 33, batch index: 541, learning rate: [0.0010000000000000002], loss:2.2754039764404297\n",
      "Epoch: 33, batch index: 542, learning rate: [0.0010000000000000002], loss:2.2791268825531006\n",
      "Epoch: 33, batch index: 543, learning rate: [0.0010000000000000002], loss:2.238387107849121\n",
      "Epoch 33, validation accuracy score0.328125\n",
      "Epoch: 34, batch index: 544, learning rate: [0.0010000000000000002], loss:2.276862621307373\n",
      "Epoch: 34, batch index: 545, learning rate: [0.0010000000000000002], loss:2.2802014350891113\n",
      "Epoch: 34, batch index: 546, learning rate: [0.0010000000000000002], loss:2.291677951812744\n",
      "Epoch: 34, batch index: 547, learning rate: [0.0010000000000000002], loss:2.276597023010254\n",
      "Epoch: 34, batch index: 548, learning rate: [0.0010000000000000002], loss:2.245933771133423\n",
      "Epoch: 34, batch index: 549, learning rate: [0.0010000000000000002], loss:2.2590951919555664\n",
      "Epoch: 34, batch index: 550, learning rate: [0.0010000000000000002], loss:2.2732086181640625\n",
      "Epoch: 34, batch index: 551, learning rate: [0.0010000000000000002], loss:2.212613821029663\n",
      "Epoch: 34, batch index: 552, learning rate: [0.0010000000000000002], loss:2.315969467163086\n",
      "Epoch: 34, batch index: 553, learning rate: [0.0010000000000000002], loss:2.250152826309204\n",
      "Epoch: 34, batch index: 554, learning rate: [0.0010000000000000002], loss:2.272425651550293\n",
      "Epoch: 34, batch index: 555, learning rate: [0.0010000000000000002], loss:2.2516822814941406\n",
      "Epoch: 34, batch index: 556, learning rate: [0.0010000000000000002], loss:2.272479295730591\n",
      "Epoch: 34, batch index: 557, learning rate: [0.0010000000000000002], loss:2.2858071327209473\n",
      "Epoch: 34, batch index: 558, learning rate: [0.0010000000000000002], loss:2.2456374168395996\n",
      "Epoch: 34, batch index: 559, learning rate: [0.0010000000000000002], loss:2.281355142593384\n",
      "Epoch 34, validation accuracy score0.341796875\n",
      "Epoch: 35, batch index: 560, learning rate: [0.0010000000000000002], loss:2.2858755588531494\n",
      "Epoch: 35, batch index: 561, learning rate: [0.0010000000000000002], loss:2.2950732707977295\n",
      "Epoch: 35, batch index: 562, learning rate: [0.0010000000000000002], loss:2.2696948051452637\n",
      "Epoch: 35, batch index: 563, learning rate: [0.0010000000000000002], loss:2.251610517501831\n",
      "Epoch: 35, batch index: 564, learning rate: [0.0010000000000000002], loss:2.271129846572876\n",
      "Epoch: 35, batch index: 565, learning rate: [0.0010000000000000002], loss:2.2729930877685547\n",
      "Epoch: 35, batch index: 566, learning rate: [0.0010000000000000002], loss:2.269524335861206\n",
      "Epoch: 35, batch index: 567, learning rate: [0.0010000000000000002], loss:2.2686712741851807\n",
      "Epoch: 35, batch index: 568, learning rate: [0.0010000000000000002], loss:2.2736012935638428\n",
      "Epoch: 35, batch index: 569, learning rate: [0.0010000000000000002], loss:2.285471200942993\n",
      "Epoch: 35, batch index: 570, learning rate: [0.0010000000000000002], loss:2.2633485794067383\n",
      "Epoch: 35, batch index: 571, learning rate: [0.0010000000000000002], loss:2.2725534439086914\n",
      "Epoch: 35, batch index: 572, learning rate: [0.0010000000000000002], loss:2.25317120552063\n",
      "Epoch: 35, batch index: 573, learning rate: [0.0010000000000000002], loss:2.2593042850494385\n",
      "Epoch: 35, batch index: 574, learning rate: [0.0010000000000000002], loss:2.2565207481384277\n",
      "Epoch: 35, batch index: 575, learning rate: [0.0010000000000000002], loss:2.3013088703155518\n",
      "Epoch 35, validation accuracy score0.28515625\n",
      "Epoch: 36, batch index: 576, learning rate: [0.0010000000000000002], loss:2.2763633728027344\n",
      "Epoch: 36, batch index: 577, learning rate: [0.0010000000000000002], loss:2.2613096237182617\n",
      "Epoch: 36, batch index: 578, learning rate: [0.0010000000000000002], loss:2.295187473297119\n",
      "Epoch: 36, batch index: 579, learning rate: [0.0010000000000000002], loss:2.2567195892333984\n",
      "Epoch: 36, batch index: 580, learning rate: [0.0010000000000000002], loss:2.2695682048797607\n",
      "Epoch: 36, batch index: 581, learning rate: [0.0010000000000000002], loss:2.2650279998779297\n",
      "Epoch: 36, batch index: 582, learning rate: [0.0010000000000000002], loss:2.280463933944702\n",
      "Epoch: 36, batch index: 583, learning rate: [0.0010000000000000002], loss:2.265239715576172\n",
      "Epoch: 36, batch index: 584, learning rate: [0.0010000000000000002], loss:2.2825541496276855\n",
      "Epoch: 36, batch index: 585, learning rate: [0.0010000000000000002], loss:2.293825387954712\n",
      "Epoch: 36, batch index: 586, learning rate: [0.0010000000000000002], loss:2.266846179962158\n",
      "Epoch: 36, batch index: 587, learning rate: [0.0010000000000000002], loss:2.274886131286621\n",
      "Epoch: 36, batch index: 588, learning rate: [0.0010000000000000002], loss:2.2582640647888184\n",
      "Epoch: 36, batch index: 589, learning rate: [0.0010000000000000002], loss:2.2686614990234375\n",
      "Epoch: 36, batch index: 590, learning rate: [0.0010000000000000002], loss:2.240591287612915\n",
      "Epoch: 36, batch index: 591, learning rate: [0.0010000000000000002], loss:2.311906337738037\n",
      "Epoch 36, validation accuracy score0.306640625\n",
      "Epoch: 37, batch index: 592, learning rate: [0.0010000000000000002], loss:2.2470667362213135\n",
      "Epoch: 37, batch index: 593, learning rate: [0.0010000000000000002], loss:2.2437970638275146\n",
      "Epoch: 37, batch index: 594, learning rate: [0.0010000000000000002], loss:2.271935224533081\n",
      "Epoch: 37, batch index: 595, learning rate: [0.0010000000000000002], loss:2.3094027042388916\n",
      "Epoch: 37, batch index: 596, learning rate: [0.0010000000000000002], loss:2.258563756942749\n",
      "Epoch: 37, batch index: 597, learning rate: [0.0010000000000000002], loss:2.2728283405303955\n",
      "Epoch: 37, batch index: 598, learning rate: [0.0010000000000000002], loss:2.2720303535461426\n",
      "Epoch: 37, batch index: 599, learning rate: [0.0010000000000000002], loss:2.2929513454437256\n",
      "Epoch: 37, batch index: 600, learning rate: [0.0010000000000000002], loss:2.2574844360351562\n",
      "Epoch: 37, batch index: 601, learning rate: [0.0010000000000000002], loss:2.289158821105957\n",
      "Epoch: 37, batch index: 602, learning rate: [0.0010000000000000002], loss:2.274914026260376\n",
      "Epoch: 37, batch index: 603, learning rate: [0.00010000000000000003], loss:2.2977449893951416\n",
      "Epoch: 37, batch index: 604, learning rate: [0.00010000000000000003], loss:2.3050591945648193\n",
      "Epoch: 37, batch index: 605, learning rate: [0.00010000000000000003], loss:2.285461664199829\n",
      "Epoch: 37, batch index: 606, learning rate: [0.00010000000000000003], loss:2.2610607147216797\n",
      "Epoch: 37, batch index: 607, learning rate: [0.00010000000000000003], loss:2.2601735591888428\n",
      "Epoch 37, validation accuracy score0.310546875\n",
      "Epoch: 38, batch index: 608, learning rate: [0.00010000000000000003], loss:2.273939609527588\n",
      "Epoch: 38, batch index: 609, learning rate: [0.00010000000000000003], loss:2.297774076461792\n",
      "Epoch: 38, batch index: 610, learning rate: [0.00010000000000000003], loss:2.2887749671936035\n",
      "Epoch: 38, batch index: 611, learning rate: [0.00010000000000000003], loss:2.2523820400238037\n",
      "Epoch: 38, batch index: 612, learning rate: [0.00010000000000000003], loss:2.2377545833587646\n",
      "Epoch: 38, batch index: 613, learning rate: [0.00010000000000000003], loss:2.241563558578491\n",
      "Epoch: 38, batch index: 614, learning rate: [0.00010000000000000003], loss:2.279874086380005\n",
      "Epoch: 38, batch index: 615, learning rate: [0.00010000000000000003], loss:2.2304537296295166\n",
      "Epoch: 38, batch index: 616, learning rate: [0.00010000000000000003], loss:2.273364782333374\n",
      "Epoch: 38, batch index: 617, learning rate: [0.00010000000000000003], loss:2.2387380599975586\n",
      "Epoch: 38, batch index: 618, learning rate: [0.00010000000000000003], loss:2.2737464904785156\n",
      "Epoch: 38, batch index: 619, learning rate: [0.00010000000000000003], loss:2.2748355865478516\n",
      "Epoch: 38, batch index: 620, learning rate: [0.00010000000000000003], loss:2.2633237838745117\n",
      "Epoch: 38, batch index: 621, learning rate: [0.00010000000000000003], loss:2.2892932891845703\n",
      "Epoch: 38, batch index: 622, learning rate: [0.00010000000000000003], loss:2.2948849201202393\n",
      "Epoch: 38, batch index: 623, learning rate: [0.00010000000000000003], loss:2.2618374824523926\n",
      "Epoch 38, validation accuracy score0.314453125\n",
      "Epoch: 39, batch index: 624, learning rate: [0.00010000000000000003], loss:2.2218799591064453\n",
      "Epoch: 39, batch index: 625, learning rate: [0.00010000000000000003], loss:2.245124101638794\n",
      "Epoch: 39, batch index: 626, learning rate: [0.00010000000000000003], loss:2.2707316875457764\n",
      "Epoch: 39, batch index: 627, learning rate: [0.00010000000000000003], loss:2.2443482875823975\n",
      "Epoch: 39, batch index: 628, learning rate: [0.00010000000000000003], loss:2.2605345249176025\n",
      "Epoch: 39, batch index: 629, learning rate: [0.00010000000000000003], loss:2.283475875854492\n",
      "Epoch: 39, batch index: 630, learning rate: [0.00010000000000000003], loss:2.2619376182556152\n",
      "Epoch: 39, batch index: 631, learning rate: [0.00010000000000000003], loss:2.2419674396514893\n",
      "Epoch: 39, batch index: 632, learning rate: [0.00010000000000000003], loss:2.2751126289367676\n",
      "Epoch: 39, batch index: 633, learning rate: [0.00010000000000000003], loss:2.255280017852783\n",
      "Epoch: 39, batch index: 634, learning rate: [0.00010000000000000003], loss:2.2810566425323486\n",
      "Epoch: 39, batch index: 635, learning rate: [0.00010000000000000003], loss:2.2990400791168213\n",
      "Epoch: 39, batch index: 636, learning rate: [0.00010000000000000003], loss:2.266695976257324\n",
      "Epoch: 39, batch index: 637, learning rate: [0.00010000000000000003], loss:2.262967109680176\n",
      "Epoch: 39, batch index: 638, learning rate: [0.00010000000000000003], loss:2.2712979316711426\n",
      "Epoch: 39, batch index: 639, learning rate: [0.00010000000000000003], loss:2.2708473205566406\n",
      "Epoch 39, validation accuracy score0.322265625\n",
      "Epoch: 40, batch index: 640, learning rate: [0.00010000000000000003], loss:2.2867820262908936\n",
      "Epoch: 40, batch index: 641, learning rate: [0.00010000000000000003], loss:2.2857351303100586\n",
      "Epoch: 40, batch index: 642, learning rate: [0.00010000000000000003], loss:2.2688403129577637\n",
      "Epoch: 40, batch index: 643, learning rate: [0.00010000000000000003], loss:2.268476724624634\n",
      "Epoch: 40, batch index: 644, learning rate: [0.00010000000000000003], loss:2.2688374519348145\n",
      "Epoch: 40, batch index: 645, learning rate: [0.00010000000000000003], loss:2.281811237335205\n",
      "Epoch: 40, batch index: 646, learning rate: [0.00010000000000000003], loss:2.2553741931915283\n",
      "Epoch: 40, batch index: 647, learning rate: [0.00010000000000000003], loss:2.292206287384033\n",
      "Epoch: 40, batch index: 648, learning rate: [0.00010000000000000003], loss:2.3183209896087646\n",
      "Epoch: 40, batch index: 649, learning rate: [0.00010000000000000003], loss:2.266634702682495\n",
      "Epoch: 40, batch index: 650, learning rate: [0.00010000000000000003], loss:2.288048028945923\n",
      "Epoch: 40, batch index: 651, learning rate: [0.00010000000000000003], loss:2.2860562801361084\n",
      "Epoch: 40, batch index: 652, learning rate: [0.00010000000000000003], loss:2.2586865425109863\n",
      "Epoch: 40, batch index: 653, learning rate: [0.00010000000000000003], loss:2.2014317512512207\n",
      "Epoch: 40, batch index: 654, learning rate: [0.00010000000000000003], loss:2.2404234409332275\n",
      "Epoch: 40, batch index: 655, learning rate: [0.00010000000000000003], loss:2.232762575149536\n",
      "Epoch 40, validation accuracy score0.32421875\n",
      "Epoch: 41, batch index: 656, learning rate: [0.00010000000000000003], loss:2.263277053833008\n",
      "Epoch: 41, batch index: 657, learning rate: [0.00010000000000000003], loss:2.29606294631958\n",
      "Epoch: 41, batch index: 658, learning rate: [0.00010000000000000003], loss:2.24324893951416\n",
      "Epoch: 41, batch index: 659, learning rate: [0.00010000000000000003], loss:2.271756172180176\n",
      "Epoch: 41, batch index: 660, learning rate: [0.00010000000000000003], loss:2.2810356616973877\n",
      "Epoch: 41, batch index: 661, learning rate: [0.00010000000000000003], loss:2.273116111755371\n",
      "Epoch: 41, batch index: 662, learning rate: [0.00010000000000000003], loss:2.2873191833496094\n",
      "Epoch: 41, batch index: 663, learning rate: [0.00010000000000000003], loss:2.2519216537475586\n",
      "Epoch: 41, batch index: 664, learning rate: [0.00010000000000000003], loss:2.251444101333618\n",
      "Epoch: 41, batch index: 665, learning rate: [0.00010000000000000003], loss:2.263496160507202\n",
      "Epoch: 41, batch index: 666, learning rate: [0.00010000000000000003], loss:2.2881851196289062\n",
      "Epoch: 41, batch index: 667, learning rate: [0.00010000000000000003], loss:2.2775332927703857\n",
      "Epoch: 41, batch index: 668, learning rate: [0.00010000000000000003], loss:2.2521896362304688\n",
      "Epoch: 41, batch index: 669, learning rate: [0.00010000000000000003], loss:2.263350486755371\n",
      "Epoch: 41, batch index: 670, learning rate: [0.00010000000000000003], loss:2.289666175842285\n",
      "Epoch: 41, batch index: 671, learning rate: [0.00010000000000000003], loss:2.219264507293701\n",
      "Epoch 41, validation accuracy score0.314453125\n",
      "Epoch: 42, batch index: 672, learning rate: [0.00010000000000000003], loss:2.28855299949646\n",
      "Epoch: 42, batch index: 673, learning rate: [0.00010000000000000003], loss:2.314486026763916\n",
      "Epoch: 42, batch index: 674, learning rate: [0.00010000000000000003], loss:2.2660634517669678\n",
      "Epoch: 42, batch index: 675, learning rate: [0.00010000000000000003], loss:2.2321701049804688\n",
      "Epoch: 42, batch index: 676, learning rate: [0.00010000000000000003], loss:2.284355401992798\n",
      "Epoch: 42, batch index: 677, learning rate: [0.00010000000000000003], loss:2.2965810298919678\n",
      "Epoch: 42, batch index: 678, learning rate: [0.00010000000000000003], loss:2.3171563148498535\n",
      "Epoch: 42, batch index: 679, learning rate: [0.00010000000000000003], loss:2.2732598781585693\n",
      "Epoch: 42, batch index: 680, learning rate: [0.00010000000000000003], loss:2.2736034393310547\n",
      "Epoch: 42, batch index: 681, learning rate: [0.00010000000000000003], loss:2.271709680557251\n",
      "Epoch: 42, batch index: 682, learning rate: [0.00010000000000000003], loss:2.2439002990722656\n",
      "Epoch: 42, batch index: 683, learning rate: [0.00010000000000000003], loss:2.270773410797119\n",
      "Epoch: 42, batch index: 684, learning rate: [0.00010000000000000003], loss:2.268390655517578\n",
      "Epoch: 42, batch index: 685, learning rate: [0.00010000000000000003], loss:2.2596960067749023\n",
      "Epoch: 42, batch index: 686, learning rate: [0.00010000000000000003], loss:2.2409167289733887\n",
      "Epoch: 42, batch index: 687, learning rate: [0.00010000000000000003], loss:2.241041660308838\n",
      "Epoch 42, validation accuracy score0.33984375\n",
      "Epoch: 43, batch index: 688, learning rate: [0.00010000000000000003], loss:2.2370479106903076\n",
      "Epoch: 43, batch index: 689, learning rate: [0.00010000000000000003], loss:2.230782985687256\n",
      "Epoch: 43, batch index: 690, learning rate: [0.00010000000000000003], loss:2.25915265083313\n",
      "Epoch: 43, batch index: 691, learning rate: [0.00010000000000000003], loss:2.2841403484344482\n",
      "Epoch: 43, batch index: 692, learning rate: [0.00010000000000000003], loss:2.2524359226226807\n",
      "Epoch: 43, batch index: 693, learning rate: [0.00010000000000000003], loss:2.2772457599639893\n",
      "Epoch: 43, batch index: 694, learning rate: [0.00010000000000000003], loss:2.2850966453552246\n",
      "Epoch: 43, batch index: 695, learning rate: [0.00010000000000000003], loss:2.2709336280822754\n",
      "Epoch: 43, batch index: 696, learning rate: [0.00010000000000000003], loss:2.269310235977173\n",
      "Epoch: 43, batch index: 697, learning rate: [0.00010000000000000003], loss:2.2593321800231934\n",
      "Epoch: 43, batch index: 698, learning rate: [0.00010000000000000003], loss:2.296579599380493\n",
      "Epoch: 43, batch index: 699, learning rate: [0.00010000000000000003], loss:2.2635300159454346\n",
      "Epoch: 43, batch index: 700, learning rate: [0.00010000000000000003], loss:2.2877585887908936\n",
      "Epoch: 43, batch index: 701, learning rate: [0.00010000000000000003], loss:2.2743725776672363\n",
      "Epoch: 43, batch index: 702, learning rate: [0.00010000000000000003], loss:2.2946043014526367\n",
      "Epoch: 43, batch index: 703, learning rate: [0.00010000000000000003], loss:2.274630069732666\n",
      "Epoch 43, validation accuracy score0.28515625\n",
      "Epoch: 44, batch index: 704, learning rate: [0.00010000000000000003], loss:2.2865326404571533\n",
      "Epoch: 44, batch index: 705, learning rate: [1.0000000000000004e-05], loss:2.2381386756896973\n",
      "Epoch: 44, batch index: 706, learning rate: [1.0000000000000004e-05], loss:2.260953903198242\n",
      "Epoch: 44, batch index: 707, learning rate: [1.0000000000000004e-05], loss:2.2863566875457764\n",
      "Epoch: 44, batch index: 708, learning rate: [1.0000000000000004e-05], loss:2.26554274559021\n",
      "Epoch: 44, batch index: 709, learning rate: [1.0000000000000004e-05], loss:2.2916462421417236\n",
      "Epoch: 44, batch index: 710, learning rate: [1.0000000000000004e-05], loss:2.245238780975342\n",
      "Epoch: 44, batch index: 711, learning rate: [1.0000000000000004e-05], loss:2.2567760944366455\n",
      "Epoch: 44, batch index: 712, learning rate: [1.0000000000000004e-05], loss:2.3130240440368652\n",
      "Epoch: 44, batch index: 713, learning rate: [1.0000000000000004e-05], loss:2.262796401977539\n",
      "Epoch: 44, batch index: 714, learning rate: [1.0000000000000004e-05], loss:2.2747373580932617\n",
      "Epoch: 44, batch index: 715, learning rate: [1.0000000000000004e-05], loss:2.260659694671631\n",
      "Epoch: 44, batch index: 716, learning rate: [1.0000000000000004e-05], loss:2.2866740226745605\n",
      "Epoch: 44, batch index: 717, learning rate: [1.0000000000000004e-05], loss:2.2939248085021973\n",
      "Epoch: 44, batch index: 718, learning rate: [1.0000000000000004e-05], loss:2.282683849334717\n",
      "Epoch: 44, batch index: 719, learning rate: [1.0000000000000004e-05], loss:2.280062198638916\n",
      "Epoch 44, validation accuracy score0.3515625\n",
      "Epoch: 45, batch index: 720, learning rate: [1.0000000000000004e-05], loss:2.2783637046813965\n",
      "Epoch: 45, batch index: 721, learning rate: [1.0000000000000004e-05], loss:2.308028221130371\n",
      "Epoch: 45, batch index: 722, learning rate: [1.0000000000000004e-05], loss:2.2539639472961426\n",
      "Epoch: 45, batch index: 723, learning rate: [1.0000000000000004e-05], loss:2.2575771808624268\n",
      "Epoch: 45, batch index: 724, learning rate: [1.0000000000000004e-05], loss:2.291661024093628\n",
      "Epoch: 45, batch index: 725, learning rate: [1.0000000000000004e-05], loss:2.2708094120025635\n",
      "Epoch: 45, batch index: 726, learning rate: [1.0000000000000004e-05], loss:2.2773525714874268\n",
      "Epoch: 45, batch index: 727, learning rate: [1.0000000000000004e-05], loss:2.2544684410095215\n",
      "Epoch: 45, batch index: 728, learning rate: [1.0000000000000004e-05], loss:2.2689435482025146\n",
      "Epoch: 45, batch index: 729, learning rate: [1.0000000000000004e-05], loss:2.2729501724243164\n",
      "Epoch: 45, batch index: 730, learning rate: [1.0000000000000004e-05], loss:2.264543294906616\n",
      "Epoch: 45, batch index: 731, learning rate: [1.0000000000000004e-05], loss:2.2671985626220703\n",
      "Epoch: 45, batch index: 732, learning rate: [1.0000000000000004e-05], loss:2.271930456161499\n",
      "Epoch: 45, batch index: 733, learning rate: [1.0000000000000004e-05], loss:2.271836280822754\n",
      "Epoch: 45, batch index: 734, learning rate: [1.0000000000000004e-05], loss:2.2703325748443604\n",
      "Epoch: 45, batch index: 735, learning rate: [1.0000000000000004e-05], loss:2.2255053520202637\n",
      "Epoch 45, validation accuracy score0.330078125\n",
      "Epoch: 46, batch index: 736, learning rate: [1.0000000000000004e-05], loss:2.2302606105804443\n",
      "Epoch: 46, batch index: 737, learning rate: [1.0000000000000004e-05], loss:2.288707733154297\n",
      "Epoch: 46, batch index: 738, learning rate: [1.0000000000000004e-05], loss:2.271888256072998\n",
      "Epoch: 46, batch index: 739, learning rate: [1.0000000000000004e-05], loss:2.278496265411377\n",
      "Epoch: 46, batch index: 740, learning rate: [1.0000000000000004e-05], loss:2.2501137256622314\n",
      "Epoch: 46, batch index: 741, learning rate: [1.0000000000000004e-05], loss:2.295466184616089\n",
      "Epoch: 46, batch index: 742, learning rate: [1.0000000000000004e-05], loss:2.256944179534912\n",
      "Epoch: 46, batch index: 743, learning rate: [1.0000000000000004e-05], loss:2.2806789875030518\n",
      "Epoch: 46, batch index: 744, learning rate: [1.0000000000000004e-05], loss:2.2866015434265137\n",
      "Epoch: 46, batch index: 745, learning rate: [1.0000000000000004e-05], loss:2.2728967666625977\n",
      "Epoch: 46, batch index: 746, learning rate: [1.0000000000000004e-05], loss:2.2775814533233643\n",
      "Epoch: 46, batch index: 747, learning rate: [1.0000000000000004e-05], loss:2.284245014190674\n",
      "Epoch: 46, batch index: 748, learning rate: [1.0000000000000004e-05], loss:2.2237045764923096\n",
      "Epoch: 46, batch index: 749, learning rate: [1.0000000000000004e-05], loss:2.2887794971466064\n",
      "Epoch: 46, batch index: 750, learning rate: [1.0000000000000004e-05], loss:2.277890920639038\n",
      "Epoch: 46, batch index: 751, learning rate: [1.0000000000000004e-05], loss:2.2366907596588135\n",
      "Epoch 46, validation accuracy score0.279296875\n",
      "Epoch: 47, batch index: 752, learning rate: [1.0000000000000004e-05], loss:2.251215696334839\n",
      "Epoch: 47, batch index: 753, learning rate: [1.0000000000000004e-05], loss:2.2517478466033936\n",
      "Epoch: 47, batch index: 754, learning rate: [1.0000000000000004e-05], loss:2.278873920440674\n",
      "Epoch: 47, batch index: 755, learning rate: [1.0000000000000004e-05], loss:2.266967535018921\n",
      "Epoch: 47, batch index: 756, learning rate: [1.0000000000000004e-05], loss:2.263165235519409\n",
      "Epoch: 47, batch index: 757, learning rate: [1.0000000000000004e-05], loss:2.271122455596924\n",
      "Epoch: 47, batch index: 758, learning rate: [1.0000000000000004e-05], loss:2.251936435699463\n",
      "Epoch: 47, batch index: 759, learning rate: [1.0000000000000004e-05], loss:2.260209321975708\n",
      "Epoch: 47, batch index: 760, learning rate: [1.0000000000000004e-05], loss:2.2264320850372314\n",
      "Epoch: 47, batch index: 761, learning rate: [1.0000000000000004e-05], loss:2.2891409397125244\n",
      "Epoch: 47, batch index: 762, learning rate: [1.0000000000000004e-05], loss:2.2707290649414062\n",
      "Epoch: 47, batch index: 763, learning rate: [1.0000000000000004e-05], loss:2.3035993576049805\n",
      "Epoch: 47, batch index: 764, learning rate: [1.0000000000000004e-05], loss:2.2544631958007812\n",
      "Epoch: 47, batch index: 765, learning rate: [1.0000000000000004e-05], loss:2.2780349254608154\n",
      "Epoch: 47, batch index: 766, learning rate: [1.0000000000000004e-05], loss:2.3001883029937744\n",
      "Epoch: 47, batch index: 767, learning rate: [1.0000000000000004e-05], loss:2.2636141777038574\n",
      "Epoch 47, validation accuracy score0.326171875\n",
      "Epoch: 48, batch index: 768, learning rate: [1.0000000000000004e-05], loss:2.2786240577697754\n",
      "Epoch: 48, batch index: 769, learning rate: [1.0000000000000004e-05], loss:2.307995080947876\n",
      "Epoch: 48, batch index: 770, learning rate: [1.0000000000000004e-05], loss:2.2758467197418213\n",
      "Epoch: 48, batch index: 771, learning rate: [1.0000000000000004e-05], loss:2.287986993789673\n",
      "Epoch: 48, batch index: 772, learning rate: [1.0000000000000004e-05], loss:2.2413878440856934\n",
      "Epoch: 48, batch index: 773, learning rate: [1.0000000000000004e-05], loss:2.2588484287261963\n",
      "Epoch: 48, batch index: 774, learning rate: [1.0000000000000004e-05], loss:2.2286248207092285\n",
      "Epoch: 48, batch index: 775, learning rate: [1.0000000000000004e-05], loss:2.287637948989868\n",
      "Epoch: 48, batch index: 776, learning rate: [1.0000000000000004e-05], loss:2.2767481803894043\n",
      "Epoch: 48, batch index: 777, learning rate: [1.0000000000000004e-05], loss:2.271578073501587\n",
      "Epoch: 48, batch index: 778, learning rate: [1.0000000000000004e-05], loss:2.3131802082061768\n",
      "Epoch: 48, batch index: 779, learning rate: [1.0000000000000004e-05], loss:2.2933506965637207\n",
      "Epoch: 48, batch index: 780, learning rate: [1.0000000000000004e-05], loss:2.2425308227539062\n",
      "Epoch: 48, batch index: 781, learning rate: [1.0000000000000004e-05], loss:2.2656610012054443\n",
      "Epoch: 48, batch index: 782, learning rate: [1.0000000000000004e-05], loss:2.257172107696533\n",
      "Epoch: 48, batch index: 783, learning rate: [1.0000000000000004e-05], loss:2.3327910900115967\n",
      "Epoch 48, validation accuracy score0.294921875\n",
      "Epoch: 49, batch index: 784, learning rate: [1.0000000000000004e-05], loss:2.2711691856384277\n",
      "Epoch: 49, batch index: 785, learning rate: [1.0000000000000004e-05], loss:2.274345874786377\n",
      "Epoch: 49, batch index: 786, learning rate: [1.0000000000000004e-06], loss:2.2872674465179443\n",
      "Epoch: 49, batch index: 787, learning rate: [1.0000000000000004e-06], loss:2.257002592086792\n",
      "Epoch: 49, batch index: 788, learning rate: [1.0000000000000004e-06], loss:2.259458541870117\n",
      "Epoch: 49, batch index: 789, learning rate: [1.0000000000000004e-06], loss:2.259261131286621\n",
      "Epoch: 49, batch index: 790, learning rate: [1.0000000000000004e-06], loss:2.294063091278076\n",
      "Epoch: 49, batch index: 791, learning rate: [1.0000000000000004e-06], loss:2.24052357673645\n",
      "Epoch: 49, batch index: 792, learning rate: [1.0000000000000004e-06], loss:2.275320291519165\n",
      "Epoch: 49, batch index: 793, learning rate: [1.0000000000000004e-06], loss:2.2900068759918213\n",
      "Epoch: 49, batch index: 794, learning rate: [1.0000000000000004e-06], loss:2.2374892234802246\n",
      "Epoch: 49, batch index: 795, learning rate: [1.0000000000000004e-06], loss:2.249603271484375\n",
      "Epoch: 49, batch index: 796, learning rate: [1.0000000000000004e-06], loss:2.270590305328369\n",
      "Epoch: 49, batch index: 797, learning rate: [1.0000000000000004e-06], loss:2.28589129447937\n",
      "Epoch: 49, batch index: 798, learning rate: [1.0000000000000004e-06], loss:2.2560179233551025\n",
      "Epoch: 49, batch index: 799, learning rate: [1.0000000000000004e-06], loss:2.3067517280578613\n",
      "Epoch 49, validation accuracy score0.3125\n",
      "Epoch: 50, batch index: 800, learning rate: [1.0000000000000004e-06], loss:2.2804670333862305\n",
      "Epoch: 50, batch index: 801, learning rate: [1.0000000000000004e-06], loss:2.3044121265411377\n",
      "Epoch: 50, batch index: 802, learning rate: [1.0000000000000004e-06], loss:2.2587695121765137\n",
      "Epoch: 50, batch index: 803, learning rate: [1.0000000000000004e-06], loss:2.258737325668335\n",
      "Epoch: 50, batch index: 804, learning rate: [1.0000000000000004e-06], loss:2.2838330268859863\n",
      "Epoch: 50, batch index: 805, learning rate: [1.0000000000000004e-06], loss:2.259706497192383\n",
      "Epoch: 50, batch index: 806, learning rate: [1.0000000000000004e-06], loss:2.293780565261841\n",
      "Epoch: 50, batch index: 807, learning rate: [1.0000000000000004e-06], loss:2.2260451316833496\n",
      "Epoch: 50, batch index: 808, learning rate: [1.0000000000000004e-06], loss:2.2811949253082275\n",
      "Epoch: 50, batch index: 809, learning rate: [1.0000000000000004e-06], loss:2.235447406768799\n",
      "Epoch: 50, batch index: 810, learning rate: [1.0000000000000004e-06], loss:2.2906906604766846\n",
      "Epoch: 50, batch index: 811, learning rate: [1.0000000000000004e-06], loss:2.2970852851867676\n",
      "Epoch: 50, batch index: 812, learning rate: [1.0000000000000004e-06], loss:2.2446939945220947\n",
      "Epoch: 50, batch index: 813, learning rate: [1.0000000000000004e-06], loss:2.2673330307006836\n",
      "Epoch: 50, batch index: 814, learning rate: [1.0000000000000004e-06], loss:2.2550835609436035\n",
      "Epoch: 50, batch index: 815, learning rate: [1.0000000000000004e-06], loss:2.254354953765869\n",
      "Epoch 50, validation accuracy score0.294921875\n",
      "0\n",
      "Epoch: 51, batch index: 816, learning rate: [1.0000000000000004e-06], loss:2.233093023300171\n",
      "Epoch: 51, batch index: 817, learning rate: [1.0000000000000004e-06], loss:2.2883529663085938\n",
      "Epoch: 51, batch index: 818, learning rate: [1.0000000000000004e-06], loss:2.245431423187256\n",
      "Epoch: 51, batch index: 819, learning rate: [1.0000000000000004e-06], loss:2.2630155086517334\n",
      "Epoch: 51, batch index: 820, learning rate: [1.0000000000000004e-06], loss:2.249568462371826\n",
      "Epoch: 51, batch index: 821, learning rate: [1.0000000000000004e-06], loss:2.2861104011535645\n",
      "Epoch: 51, batch index: 822, learning rate: [1.0000000000000004e-06], loss:2.2525556087493896\n",
      "Epoch: 51, batch index: 823, learning rate: [1.0000000000000004e-06], loss:2.312985897064209\n",
      "Epoch: 51, batch index: 824, learning rate: [1.0000000000000004e-06], loss:2.284670829772949\n",
      "Epoch: 51, batch index: 825, learning rate: [1.0000000000000004e-06], loss:2.2727291584014893\n",
      "Epoch: 51, batch index: 826, learning rate: [1.0000000000000004e-06], loss:2.2876393795013428\n",
      "Epoch: 51, batch index: 827, learning rate: [1.0000000000000004e-06], loss:2.280886650085449\n",
      "Epoch: 51, batch index: 828, learning rate: [1.0000000000000004e-06], loss:2.2965495586395264\n",
      "Epoch: 51, batch index: 829, learning rate: [1.0000000000000004e-06], loss:2.2676825523376465\n",
      "Epoch: 51, batch index: 830, learning rate: [1.0000000000000004e-06], loss:2.2417752742767334\n",
      "Epoch: 51, batch index: 831, learning rate: [1.0000000000000004e-06], loss:2.2372188568115234\n",
      "Epoch 51, validation accuracy score0.365234375\n",
      "Epoch: 52, batch index: 832, learning rate: [1.0000000000000004e-06], loss:2.2599685192108154\n",
      "Epoch: 52, batch index: 833, learning rate: [1.0000000000000004e-06], loss:2.281416654586792\n",
      "Epoch: 52, batch index: 834, learning rate: [1.0000000000000004e-06], loss:2.2761621475219727\n",
      "Epoch: 52, batch index: 835, learning rate: [1.0000000000000004e-06], loss:2.2219009399414062\n",
      "Epoch: 52, batch index: 836, learning rate: [1.0000000000000004e-06], loss:2.2619621753692627\n",
      "Epoch: 52, batch index: 837, learning rate: [1.0000000000000004e-06], loss:2.277914047241211\n",
      "Epoch: 52, batch index: 838, learning rate: [1.0000000000000004e-06], loss:2.282144546508789\n",
      "Epoch: 52, batch index: 839, learning rate: [1.0000000000000004e-06], loss:2.2614152431488037\n",
      "Epoch: 52, batch index: 840, learning rate: [1.0000000000000004e-06], loss:2.251298666000366\n",
      "Epoch: 52, batch index: 841, learning rate: [1.0000000000000004e-06], loss:2.27997088432312\n",
      "Epoch: 52, batch index: 842, learning rate: [1.0000000000000004e-06], loss:2.2868175506591797\n",
      "Epoch: 52, batch index: 843, learning rate: [1.0000000000000004e-06], loss:2.2711596488952637\n",
      "Epoch: 52, batch index: 844, learning rate: [1.0000000000000004e-06], loss:2.2726073265075684\n",
      "Epoch: 52, batch index: 845, learning rate: [1.0000000000000004e-06], loss:2.295625686645508\n",
      "Epoch: 52, batch index: 846, learning rate: [1.0000000000000004e-06], loss:2.2685649394989014\n",
      "Epoch: 52, batch index: 847, learning rate: [1.0000000000000004e-06], loss:2.2642276287078857\n",
      "Epoch 52, validation accuracy score0.310546875\n",
      "Epoch: 53, batch index: 848, learning rate: [1.0000000000000004e-06], loss:2.2655956745147705\n",
      "Epoch: 53, batch index: 849, learning rate: [1.0000000000000004e-06], loss:2.2519986629486084\n",
      "Epoch: 53, batch index: 850, learning rate: [1.0000000000000004e-06], loss:2.25166916847229\n",
      "Epoch: 53, batch index: 851, learning rate: [1.0000000000000004e-06], loss:2.283769369125366\n",
      "Epoch: 53, batch index: 852, learning rate: [1.0000000000000004e-06], loss:2.2735955715179443\n",
      "Epoch: 53, batch index: 853, learning rate: [1.0000000000000004e-06], loss:2.292069911956787\n",
      "Epoch: 53, batch index: 854, learning rate: [1.0000000000000004e-06], loss:2.280149221420288\n",
      "Epoch: 53, batch index: 855, learning rate: [1.0000000000000004e-06], loss:2.260974645614624\n",
      "Epoch: 53, batch index: 856, learning rate: [1.0000000000000004e-06], loss:2.2778778076171875\n",
      "Epoch: 53, batch index: 857, learning rate: [1.0000000000000004e-06], loss:2.2697908878326416\n",
      "Epoch: 53, batch index: 858, learning rate: [1.0000000000000004e-06], loss:2.290578603744507\n",
      "Epoch: 53, batch index: 859, learning rate: [1.0000000000000004e-06], loss:2.2761728763580322\n",
      "Epoch: 53, batch index: 860, learning rate: [1.0000000000000004e-06], loss:2.2830703258514404\n",
      "Epoch: 53, batch index: 861, learning rate: [1.0000000000000004e-06], loss:2.2523674964904785\n",
      "Epoch: 53, batch index: 862, learning rate: [1.0000000000000004e-06], loss:2.2440481185913086\n",
      "Epoch: 53, batch index: 863, learning rate: [1.0000000000000004e-06], loss:2.283430576324463\n",
      "Epoch 53, validation accuracy score0.3046875\n",
      "Epoch: 54, batch index: 864, learning rate: [1.0000000000000004e-06], loss:2.2679996490478516\n",
      "Epoch: 54, batch index: 865, learning rate: [1.0000000000000004e-06], loss:2.2658703327178955\n",
      "Epoch: 54, batch index: 866, learning rate: [1.0000000000000004e-06], loss:2.312412977218628\n",
      "Epoch: 54, batch index: 867, learning rate: [1.0000000000000005e-07], loss:2.296454906463623\n",
      "Epoch: 54, batch index: 868, learning rate: [1.0000000000000005e-07], loss:2.247225761413574\n",
      "Epoch: 54, batch index: 869, learning rate: [1.0000000000000005e-07], loss:2.269303560256958\n",
      "Epoch: 54, batch index: 870, learning rate: [1.0000000000000005e-07], loss:2.2912771701812744\n",
      "Epoch: 54, batch index: 871, learning rate: [1.0000000000000005e-07], loss:2.263221263885498\n",
      "Epoch: 54, batch index: 872, learning rate: [1.0000000000000005e-07], loss:2.2624399662017822\n",
      "Epoch: 54, batch index: 873, learning rate: [1.0000000000000005e-07], loss:2.2666919231414795\n",
      "Epoch: 54, batch index: 874, learning rate: [1.0000000000000005e-07], loss:2.2646846771240234\n",
      "Epoch: 54, batch index: 875, learning rate: [1.0000000000000005e-07], loss:2.27315092086792\n",
      "Epoch: 54, batch index: 876, learning rate: [1.0000000000000005e-07], loss:2.2577133178710938\n",
      "Epoch: 54, batch index: 877, learning rate: [1.0000000000000005e-07], loss:2.257408857345581\n",
      "Epoch: 54, batch index: 878, learning rate: [1.0000000000000005e-07], loss:2.2593109607696533\n",
      "Epoch: 54, batch index: 879, learning rate: [1.0000000000000005e-07], loss:2.3016700744628906\n",
      "Epoch 54, validation accuracy score0.31640625\n",
      "Epoch: 55, batch index: 880, learning rate: [1.0000000000000005e-07], loss:2.2980942726135254\n",
      "Epoch: 55, batch index: 881, learning rate: [1.0000000000000005e-07], loss:2.274014949798584\n",
      "Epoch: 55, batch index: 882, learning rate: [1.0000000000000005e-07], loss:2.2671267986297607\n",
      "Epoch: 55, batch index: 883, learning rate: [1.0000000000000005e-07], loss:2.248990535736084\n",
      "Epoch: 55, batch index: 884, learning rate: [1.0000000000000005e-07], loss:2.2775604724884033\n",
      "Epoch: 55, batch index: 885, learning rate: [1.0000000000000005e-07], loss:2.266350507736206\n",
      "Epoch: 55, batch index: 886, learning rate: [1.0000000000000005e-07], loss:2.243500232696533\n",
      "Epoch: 55, batch index: 887, learning rate: [1.0000000000000005e-07], loss:2.264915704727173\n",
      "Epoch: 55, batch index: 888, learning rate: [1.0000000000000005e-07], loss:2.2546513080596924\n",
      "Epoch: 55, batch index: 889, learning rate: [1.0000000000000005e-07], loss:2.2847189903259277\n",
      "Epoch: 55, batch index: 890, learning rate: [1.0000000000000005e-07], loss:2.335141658782959\n",
      "Epoch: 55, batch index: 891, learning rate: [1.0000000000000005e-07], loss:2.2565882205963135\n",
      "Epoch: 55, batch index: 892, learning rate: [1.0000000000000005e-07], loss:2.2543065547943115\n",
      "Epoch: 55, batch index: 893, learning rate: [1.0000000000000005e-07], loss:2.2738091945648193\n",
      "Epoch: 55, batch index: 894, learning rate: [1.0000000000000005e-07], loss:2.2969443798065186\n",
      "Epoch: 55, batch index: 895, learning rate: [1.0000000000000005e-07], loss:2.2929599285125732\n",
      "Epoch 55, validation accuracy score0.341796875\n",
      "Epoch: 56, batch index: 896, learning rate: [1.0000000000000005e-07], loss:2.26448130607605\n",
      "Epoch: 56, batch index: 897, learning rate: [1.0000000000000005e-07], loss:2.270380973815918\n",
      "Epoch: 56, batch index: 898, learning rate: [1.0000000000000005e-07], loss:2.286482810974121\n",
      "Epoch: 56, batch index: 899, learning rate: [1.0000000000000005e-07], loss:2.239696741104126\n",
      "Epoch: 56, batch index: 900, learning rate: [1.0000000000000005e-07], loss:2.2555594444274902\n",
      "Epoch: 56, batch index: 901, learning rate: [1.0000000000000005e-07], loss:2.3000309467315674\n",
      "Epoch: 56, batch index: 902, learning rate: [1.0000000000000005e-07], loss:2.286027431488037\n",
      "Epoch: 56, batch index: 903, learning rate: [1.0000000000000005e-07], loss:2.253138780593872\n",
      "Epoch: 56, batch index: 904, learning rate: [1.0000000000000005e-07], loss:2.2932353019714355\n",
      "Epoch: 56, batch index: 905, learning rate: [1.0000000000000005e-07], loss:2.288365364074707\n",
      "Epoch: 56, batch index: 906, learning rate: [1.0000000000000005e-07], loss:2.26263689994812\n",
      "Epoch: 56, batch index: 907, learning rate: [1.0000000000000005e-07], loss:2.2648720741271973\n",
      "Epoch: 56, batch index: 908, learning rate: [1.0000000000000005e-07], loss:2.276059150695801\n",
      "Epoch: 56, batch index: 909, learning rate: [1.0000000000000005e-07], loss:2.3118484020233154\n",
      "Epoch: 56, batch index: 910, learning rate: [1.0000000000000005e-07], loss:2.230344295501709\n",
      "Epoch: 56, batch index: 911, learning rate: [1.0000000000000005e-07], loss:2.257664680480957\n",
      "Epoch 56, validation accuracy score0.369140625\n",
      "Epoch: 57, batch index: 912, learning rate: [1.0000000000000005e-07], loss:2.2848246097564697\n",
      "Epoch: 57, batch index: 913, learning rate: [1.0000000000000005e-07], loss:2.255740165710449\n",
      "Epoch: 57, batch index: 914, learning rate: [1.0000000000000005e-07], loss:2.2928097248077393\n",
      "Epoch: 57, batch index: 915, learning rate: [1.0000000000000005e-07], loss:2.2785725593566895\n",
      "Epoch: 57, batch index: 916, learning rate: [1.0000000000000005e-07], loss:2.2706234455108643\n",
      "Epoch: 57, batch index: 917, learning rate: [1.0000000000000005e-07], loss:2.2702078819274902\n",
      "Epoch: 57, batch index: 918, learning rate: [1.0000000000000005e-07], loss:2.2918097972869873\n",
      "Epoch: 57, batch index: 919, learning rate: [1.0000000000000005e-07], loss:2.2545325756073\n",
      "Epoch: 57, batch index: 920, learning rate: [1.0000000000000005e-07], loss:2.2648890018463135\n",
      "Epoch: 57, batch index: 921, learning rate: [1.0000000000000005e-07], loss:2.262925624847412\n",
      "Epoch: 57, batch index: 922, learning rate: [1.0000000000000005e-07], loss:2.2858002185821533\n",
      "Epoch: 57, batch index: 923, learning rate: [1.0000000000000005e-07], loss:2.287684202194214\n",
      "Epoch: 57, batch index: 924, learning rate: [1.0000000000000005e-07], loss:2.2650861740112305\n",
      "Epoch: 57, batch index: 925, learning rate: [1.0000000000000005e-07], loss:2.2566487789154053\n",
      "Epoch: 57, batch index: 926, learning rate: [1.0000000000000005e-07], loss:2.290567398071289\n",
      "Epoch: 57, batch index: 927, learning rate: [1.0000000000000005e-07], loss:2.2835776805877686\n",
      "Epoch 57, validation accuracy score0.291015625\n",
      "Epoch: 58, batch index: 928, learning rate: [1.0000000000000005e-07], loss:2.2915971279144287\n",
      "Epoch: 58, batch index: 929, learning rate: [1.0000000000000005e-07], loss:2.2988734245300293\n",
      "Epoch: 58, batch index: 930, learning rate: [1.0000000000000005e-07], loss:2.2684993743896484\n",
      "Epoch: 58, batch index: 931, learning rate: [1.0000000000000005e-07], loss:2.275806188583374\n",
      "Epoch: 58, batch index: 932, learning rate: [1.0000000000000005e-07], loss:2.2677597999572754\n",
      "Epoch: 58, batch index: 933, learning rate: [1.0000000000000005e-07], loss:2.2711362838745117\n",
      "Epoch: 58, batch index: 934, learning rate: [1.0000000000000005e-07], loss:2.2694995403289795\n",
      "Epoch: 58, batch index: 935, learning rate: [1.0000000000000005e-07], loss:2.2802011966705322\n",
      "Epoch: 58, batch index: 936, learning rate: [1.0000000000000005e-07], loss:2.2502410411834717\n",
      "Epoch: 58, batch index: 937, learning rate: [1.0000000000000005e-07], loss:2.2487127780914307\n",
      "Epoch: 58, batch index: 938, learning rate: [1.0000000000000005e-07], loss:2.270700693130493\n",
      "Epoch: 58, batch index: 939, learning rate: [1.0000000000000005e-07], loss:2.2730727195739746\n",
      "Epoch: 58, batch index: 940, learning rate: [1.0000000000000005e-07], loss:2.321432113647461\n",
      "Epoch: 58, batch index: 941, learning rate: [1.0000000000000005e-07], loss:2.2412073612213135\n",
      "Epoch: 58, batch index: 942, learning rate: [1.0000000000000005e-07], loss:2.279064416885376\n",
      "Epoch: 58, batch index: 943, learning rate: [1.0000000000000005e-07], loss:2.2321949005126953\n",
      "Epoch 58, validation accuracy score0.34765625\n",
      "Epoch: 59, batch index: 944, learning rate: [1.0000000000000005e-07], loss:2.246509552001953\n",
      "Epoch: 59, batch index: 945, learning rate: [1.0000000000000005e-07], loss:2.232936143875122\n",
      "Epoch: 59, batch index: 946, learning rate: [1.0000000000000005e-07], loss:2.255399703979492\n",
      "Epoch: 59, batch index: 947, learning rate: [1.0000000000000005e-07], loss:2.313690185546875\n",
      "Epoch: 59, batch index: 948, learning rate: [1.0000000000000005e-08], loss:2.2388322353363037\n",
      "Epoch: 59, batch index: 949, learning rate: [1.0000000000000005e-08], loss:2.260439157485962\n",
      "Epoch: 59, batch index: 950, learning rate: [1.0000000000000005e-08], loss:2.281693696975708\n",
      "Epoch: 59, batch index: 951, learning rate: [1.0000000000000005e-08], loss:2.231962203979492\n",
      "Epoch: 59, batch index: 952, learning rate: [1.0000000000000005e-08], loss:2.252534866333008\n",
      "Epoch: 59, batch index: 953, learning rate: [1.0000000000000005e-08], loss:2.2564339637756348\n",
      "Epoch: 59, batch index: 954, learning rate: [1.0000000000000005e-08], loss:2.2887063026428223\n",
      "Epoch: 59, batch index: 955, learning rate: [1.0000000000000005e-08], loss:2.3160972595214844\n",
      "Epoch: 59, batch index: 956, learning rate: [1.0000000000000005e-08], loss:2.267184257507324\n",
      "Epoch: 59, batch index: 957, learning rate: [1.0000000000000005e-08], loss:2.2401304244995117\n",
      "Epoch: 59, batch index: 958, learning rate: [1.0000000000000005e-08], loss:2.2973225116729736\n",
      "Epoch: 59, batch index: 959, learning rate: [1.0000000000000005e-08], loss:2.2807648181915283\n",
      "Epoch 59, validation accuracy score0.353515625\n",
      "Epoch: 60, batch index: 960, learning rate: [1.0000000000000005e-08], loss:2.303816556930542\n",
      "Epoch: 60, batch index: 961, learning rate: [1.0000000000000005e-08], loss:2.234645128250122\n",
      "Epoch: 60, batch index: 962, learning rate: [1.0000000000000005e-08], loss:2.2650959491729736\n",
      "Epoch: 60, batch index: 963, learning rate: [1.0000000000000005e-08], loss:2.2534775733947754\n",
      "Epoch: 60, batch index: 964, learning rate: [1.0000000000000005e-08], loss:2.2918906211853027\n",
      "Epoch: 60, batch index: 965, learning rate: [1.0000000000000005e-08], loss:2.2881782054901123\n",
      "Epoch: 60, batch index: 966, learning rate: [1.0000000000000005e-08], loss:2.260016679763794\n",
      "Epoch: 60, batch index: 967, learning rate: [1.0000000000000005e-08], loss:2.279858112335205\n",
      "Epoch: 60, batch index: 968, learning rate: [1.0000000000000005e-08], loss:2.2881979942321777\n",
      "Epoch: 60, batch index: 969, learning rate: [1.0000000000000005e-08], loss:2.2602503299713135\n",
      "Epoch: 60, batch index: 970, learning rate: [1.0000000000000005e-08], loss:2.238670587539673\n",
      "Epoch: 60, batch index: 971, learning rate: [1.0000000000000005e-08], loss:2.2774109840393066\n",
      "Epoch: 60, batch index: 972, learning rate: [1.0000000000000005e-08], loss:2.2445199489593506\n",
      "Epoch: 60, batch index: 973, learning rate: [1.0000000000000005e-08], loss:2.2662620544433594\n",
      "Epoch: 60, batch index: 974, learning rate: [1.0000000000000005e-08], loss:2.2654449939727783\n",
      "Epoch: 60, batch index: 975, learning rate: [1.0000000000000005e-08], loss:2.286731719970703\n",
      "Epoch 60, validation accuracy score0.314453125\n",
      "Epoch: 61, batch index: 976, learning rate: [1.0000000000000005e-08], loss:2.291869640350342\n",
      "Epoch: 61, batch index: 977, learning rate: [1.0000000000000005e-08], loss:2.265160322189331\n",
      "Epoch: 61, batch index: 978, learning rate: [1.0000000000000005e-08], loss:2.2857613563537598\n",
      "Epoch: 61, batch index: 979, learning rate: [1.0000000000000005e-08], loss:2.295626401901245\n",
      "Epoch: 61, batch index: 980, learning rate: [1.0000000000000005e-08], loss:2.312657594680786\n",
      "Epoch: 61, batch index: 981, learning rate: [1.0000000000000005e-08], loss:2.2481324672698975\n",
      "Epoch: 61, batch index: 982, learning rate: [1.0000000000000005e-08], loss:2.2841453552246094\n",
      "Epoch: 61, batch index: 983, learning rate: [1.0000000000000005e-08], loss:2.242701292037964\n",
      "Epoch: 61, batch index: 984, learning rate: [1.0000000000000005e-08], loss:2.2635838985443115\n",
      "Epoch: 61, batch index: 985, learning rate: [1.0000000000000005e-08], loss:2.2998383045196533\n",
      "Epoch: 61, batch index: 986, learning rate: [1.0000000000000005e-08], loss:2.2440576553344727\n",
      "Epoch: 61, batch index: 987, learning rate: [1.0000000000000005e-08], loss:2.2355170249938965\n",
      "Epoch: 61, batch index: 988, learning rate: [1.0000000000000005e-08], loss:2.256776809692383\n",
      "Epoch: 61, batch index: 989, learning rate: [1.0000000000000005e-08], loss:2.283951997756958\n",
      "Epoch: 61, batch index: 990, learning rate: [1.0000000000000005e-08], loss:2.252772331237793\n",
      "Epoch: 61, batch index: 991, learning rate: [1.0000000000000005e-08], loss:2.2522454261779785\n",
      "Epoch 61, validation accuracy score0.359375\n",
      "Epoch: 62, batch index: 992, learning rate: [1.0000000000000005e-08], loss:2.286449432373047\n",
      "Epoch: 62, batch index: 993, learning rate: [1.0000000000000005e-08], loss:2.296630859375\n",
      "Epoch: 62, batch index: 994, learning rate: [1.0000000000000005e-08], loss:2.2739768028259277\n",
      "Epoch: 62, batch index: 995, learning rate: [1.0000000000000005e-08], loss:2.276038885116577\n",
      "Epoch: 62, batch index: 996, learning rate: [1.0000000000000005e-08], loss:2.2912545204162598\n",
      "Epoch: 62, batch index: 997, learning rate: [1.0000000000000005e-08], loss:2.269547462463379\n",
      "Epoch: 62, batch index: 998, learning rate: [1.0000000000000005e-08], loss:2.299605131149292\n",
      "Epoch: 62, batch index: 999, learning rate: [1.0000000000000005e-08], loss:2.2405104637145996\n",
      "Epoch: 62, batch index: 1000, learning rate: [1.0000000000000005e-08], loss:2.2632908821105957\n",
      "Epoch: 62, batch index: 1001, learning rate: [1.0000000000000005e-08], loss:2.2741007804870605\n",
      "Epoch: 62, batch index: 1002, learning rate: [1.0000000000000005e-08], loss:2.2585878372192383\n",
      "Epoch: 62, batch index: 1003, learning rate: [1.0000000000000005e-08], loss:2.2266159057617188\n",
      "Epoch: 62, batch index: 1004, learning rate: [1.0000000000000005e-08], loss:2.2640013694763184\n",
      "Epoch: 62, batch index: 1005, learning rate: [1.0000000000000005e-08], loss:2.297318696975708\n",
      "Epoch: 62, batch index: 1006, learning rate: [1.0000000000000005e-08], loss:2.2514147758483887\n",
      "Epoch: 62, batch index: 1007, learning rate: [1.0000000000000005e-08], loss:2.2594404220581055\n",
      "Epoch 62, validation accuracy score0.30859375\n",
      "Epoch: 63, batch index: 1008, learning rate: [1.0000000000000005e-08], loss:2.265686273574829\n",
      "Epoch: 63, batch index: 1009, learning rate: [1.0000000000000005e-08], loss:2.2752668857574463\n",
      "Epoch: 63, batch index: 1010, learning rate: [1.0000000000000005e-08], loss:2.2629003524780273\n",
      "Epoch: 63, batch index: 1011, learning rate: [1.0000000000000005e-08], loss:2.2907674312591553\n",
      "Epoch: 63, batch index: 1012, learning rate: [1.0000000000000005e-08], loss:2.2888269424438477\n",
      "Epoch: 63, batch index: 1013, learning rate: [1.0000000000000005e-08], loss:2.267990827560425\n",
      "Epoch: 63, batch index: 1014, learning rate: [1.0000000000000005e-08], loss:2.296795129776001\n",
      "Epoch: 63, batch index: 1015, learning rate: [1.0000000000000005e-08], loss:2.248450756072998\n",
      "Epoch: 63, batch index: 1016, learning rate: [1.0000000000000005e-08], loss:2.2984390258789062\n",
      "Epoch: 63, batch index: 1017, learning rate: [1.0000000000000005e-08], loss:2.2524337768554688\n",
      "Epoch: 63, batch index: 1018, learning rate: [1.0000000000000005e-08], loss:2.293884038925171\n",
      "Epoch: 63, batch index: 1019, learning rate: [1.0000000000000005e-08], loss:2.3002829551696777\n",
      "Epoch: 63, batch index: 1020, learning rate: [1.0000000000000005e-08], loss:2.233762741088867\n",
      "Epoch: 63, batch index: 1021, learning rate: [1.0000000000000005e-08], loss:2.2457847595214844\n",
      "Epoch: 63, batch index: 1022, learning rate: [1.0000000000000005e-08], loss:2.297816276550293\n",
      "Epoch: 63, batch index: 1023, learning rate: [1.0000000000000005e-08], loss:2.2382242679595947\n",
      "Epoch 63, validation accuracy score0.3359375\n",
      "Epoch: 64, batch index: 1024, learning rate: [1.0000000000000005e-08], loss:2.2646899223327637\n",
      "Epoch: 64, batch index: 1025, learning rate: [1.0000000000000005e-08], loss:2.292182207107544\n",
      "Epoch: 64, batch index: 1026, learning rate: [1.0000000000000005e-08], loss:2.285749912261963\n",
      "Epoch: 64, batch index: 1027, learning rate: [1.0000000000000005e-08], loss:2.2634341716766357\n",
      "Epoch: 64, batch index: 1028, learning rate: [1.0000000000000005e-08], loss:2.278339385986328\n",
      "Epoch: 64, batch index: 1029, learning rate: [1.0000000000000005e-08], loss:2.272430419921875\n",
      "Epoch: 64, batch index: 1030, learning rate: [1.0000000000000005e-08], loss:2.268784999847412\n",
      "Epoch: 64, batch index: 1031, learning rate: [1.0000000000000005e-08], loss:2.2681386470794678\n",
      "Epoch: 64, batch index: 1032, learning rate: [1.0000000000000005e-08], loss:2.2739083766937256\n",
      "Epoch: 64, batch index: 1033, learning rate: [1.0000000000000005e-08], loss:2.2766025066375732\n",
      "Epoch: 64, batch index: 1034, learning rate: [1.0000000000000005e-08], loss:2.2807223796844482\n",
      "Epoch: 64, batch index: 1035, learning rate: [1.0000000000000005e-08], loss:2.279355525970459\n",
      "Epoch: 64, batch index: 1036, learning rate: [1.0000000000000005e-08], loss:2.2569243907928467\n",
      "Epoch: 64, batch index: 1037, learning rate: [1.0000000000000005e-08], loss:2.2669551372528076\n",
      "Epoch: 64, batch index: 1038, learning rate: [1.0000000000000005e-08], loss:2.2847206592559814\n",
      "Epoch: 64, batch index: 1039, learning rate: [1.0000000000000005e-08], loss:2.266706705093384\n",
      "Epoch 64, validation accuracy score0.361328125\n",
      "Epoch: 65, batch index: 1040, learning rate: [1.0000000000000005e-08], loss:2.2363388538360596\n",
      "Epoch: 65, batch index: 1041, learning rate: [1.0000000000000005e-08], loss:2.270982503890991\n",
      "Epoch: 65, batch index: 1042, learning rate: [1.0000000000000005e-08], loss:2.2744758129119873\n",
      "Epoch: 65, batch index: 1043, learning rate: [1.0000000000000005e-08], loss:2.296802043914795\n",
      "Epoch: 65, batch index: 1044, learning rate: [1.0000000000000005e-08], loss:2.267777919769287\n",
      "Epoch: 65, batch index: 1045, learning rate: [1.0000000000000005e-08], loss:2.2584240436553955\n",
      "Epoch: 65, batch index: 1046, learning rate: [1.0000000000000005e-08], loss:2.289569139480591\n",
      "Epoch: 65, batch index: 1047, learning rate: [1.0000000000000005e-08], loss:2.23652720451355\n",
      "Epoch: 65, batch index: 1048, learning rate: [1.0000000000000005e-08], loss:2.2774901390075684\n",
      "Epoch: 65, batch index: 1049, learning rate: [1.0000000000000005e-08], loss:2.265209674835205\n",
      "Epoch: 65, batch index: 1050, learning rate: [1.0000000000000005e-08], loss:2.2571680545806885\n",
      "Epoch: 65, batch index: 1051, learning rate: [1.0000000000000005e-08], loss:2.270993709564209\n",
      "Epoch: 65, batch index: 1052, learning rate: [1.0000000000000005e-08], loss:2.3024115562438965\n",
      "Epoch: 65, batch index: 1053, learning rate: [1.0000000000000005e-08], loss:2.2615468502044678\n",
      "Epoch: 65, batch index: 1054, learning rate: [1.0000000000000005e-08], loss:2.2680602073669434\n",
      "Epoch: 65, batch index: 1055, learning rate: [1.0000000000000005e-08], loss:2.2796216011047363\n",
      "Epoch 65, validation accuracy score0.349609375\n",
      "Epoch: 66, batch index: 1056, learning rate: [1.0000000000000005e-08], loss:2.2872536182403564\n",
      "Epoch: 66, batch index: 1057, learning rate: [1.0000000000000005e-08], loss:2.2678520679473877\n",
      "Epoch: 66, batch index: 1058, learning rate: [1.0000000000000005e-08], loss:2.2418265342712402\n",
      "Epoch: 66, batch index: 1059, learning rate: [1.0000000000000005e-08], loss:2.3054115772247314\n",
      "Epoch: 66, batch index: 1060, learning rate: [1.0000000000000005e-08], loss:2.281837224960327\n",
      "Epoch: 66, batch index: 1061, learning rate: [1.0000000000000005e-08], loss:2.2537307739257812\n",
      "Epoch: 66, batch index: 1062, learning rate: [1.0000000000000005e-08], loss:2.2596051692962646\n",
      "Epoch: 66, batch index: 1063, learning rate: [1.0000000000000005e-08], loss:2.29386305809021\n",
      "Epoch: 66, batch index: 1064, learning rate: [1.0000000000000005e-08], loss:2.254291296005249\n",
      "Epoch: 66, batch index: 1065, learning rate: [1.0000000000000005e-08], loss:2.3270833492279053\n",
      "Epoch: 66, batch index: 1066, learning rate: [1.0000000000000005e-08], loss:2.2746078968048096\n",
      "Epoch: 66, batch index: 1067, learning rate: [1.0000000000000005e-08], loss:2.2791848182678223\n",
      "Epoch: 66, batch index: 1068, learning rate: [1.0000000000000005e-08], loss:2.263087034225464\n",
      "Epoch: 66, batch index: 1069, learning rate: [1.0000000000000005e-08], loss:2.285032272338867\n",
      "Epoch: 66, batch index: 1070, learning rate: [1.0000000000000005e-08], loss:2.2649974822998047\n",
      "Epoch: 66, batch index: 1071, learning rate: [1.0000000000000005e-08], loss:2.2745063304901123\n",
      "Epoch 66, validation accuracy score0.306640625\n",
      "Epoch: 67, batch index: 1072, learning rate: [1.0000000000000005e-08], loss:2.2795910835266113\n",
      "Epoch: 67, batch index: 1073, learning rate: [1.0000000000000005e-08], loss:2.249683141708374\n",
      "Epoch: 67, batch index: 1074, learning rate: [1.0000000000000005e-08], loss:2.2585113048553467\n",
      "Epoch: 67, batch index: 1075, learning rate: [1.0000000000000005e-08], loss:2.289077043533325\n",
      "Epoch: 67, batch index: 1076, learning rate: [1.0000000000000005e-08], loss:2.2662956714630127\n",
      "Epoch: 67, batch index: 1077, learning rate: [1.0000000000000005e-08], loss:2.287496566772461\n",
      "Epoch: 67, batch index: 1078, learning rate: [1.0000000000000005e-08], loss:2.2800168991088867\n",
      "Epoch: 67, batch index: 1079, learning rate: [1.0000000000000005e-08], loss:2.2678961753845215\n",
      "Epoch: 67, batch index: 1080, learning rate: [1.0000000000000005e-08], loss:2.269914388656616\n",
      "Epoch: 67, batch index: 1081, learning rate: [1.0000000000000005e-08], loss:2.25909423828125\n",
      "Epoch: 67, batch index: 1082, learning rate: [1.0000000000000005e-08], loss:2.2577290534973145\n",
      "Epoch: 67, batch index: 1083, learning rate: [1.0000000000000005e-08], loss:2.2741081714630127\n",
      "Epoch: 67, batch index: 1084, learning rate: [1.0000000000000005e-08], loss:2.2740912437438965\n",
      "Epoch: 67, batch index: 1085, learning rate: [1.0000000000000005e-08], loss:2.2831461429595947\n",
      "Epoch: 67, batch index: 1086, learning rate: [1.0000000000000005e-08], loss:2.2736597061157227\n",
      "Epoch: 67, batch index: 1087, learning rate: [1.0000000000000005e-08], loss:2.233855724334717\n",
      "Epoch 67, validation accuracy score0.3203125\n",
      "Epoch: 68, batch index: 1088, learning rate: [1.0000000000000005e-08], loss:2.2758119106292725\n",
      "Epoch: 68, batch index: 1089, learning rate: [1.0000000000000005e-08], loss:2.2861273288726807\n",
      "Epoch: 68, batch index: 1090, learning rate: [1.0000000000000005e-08], loss:2.2570464611053467\n",
      "Epoch: 68, batch index: 1091, learning rate: [1.0000000000000005e-08], loss:2.27032732963562\n",
      "Epoch: 68, batch index: 1092, learning rate: [1.0000000000000005e-08], loss:2.2601850032806396\n",
      "Epoch: 68, batch index: 1093, learning rate: [1.0000000000000005e-08], loss:2.2648496627807617\n",
      "Epoch: 68, batch index: 1094, learning rate: [1.0000000000000005e-08], loss:2.3072314262390137\n",
      "Epoch: 68, batch index: 1095, learning rate: [1.0000000000000005e-08], loss:2.2778470516204834\n",
      "Epoch: 68, batch index: 1096, learning rate: [1.0000000000000005e-08], loss:2.2424306869506836\n",
      "Epoch: 68, batch index: 1097, learning rate: [1.0000000000000005e-08], loss:2.258253812789917\n",
      "Epoch: 68, batch index: 1098, learning rate: [1.0000000000000005e-08], loss:2.2469067573547363\n",
      "Epoch: 68, batch index: 1099, learning rate: [1.0000000000000005e-08], loss:2.2901864051818848\n",
      "Epoch: 68, batch index: 1100, learning rate: [1.0000000000000005e-08], loss:2.268935441970825\n",
      "Epoch: 68, batch index: 1101, learning rate: [1.0000000000000005e-08], loss:2.253835916519165\n",
      "Epoch: 68, batch index: 1102, learning rate: [1.0000000000000005e-08], loss:2.2713279724121094\n",
      "Epoch: 68, batch index: 1103, learning rate: [1.0000000000000005e-08], loss:2.250016689300537\n",
      "Epoch 68, validation accuracy score0.326171875\n",
      "Epoch: 69, batch index: 1104, learning rate: [1.0000000000000005e-08], loss:2.265329599380493\n",
      "Epoch: 69, batch index: 1105, learning rate: [1.0000000000000005e-08], loss:2.267603874206543\n",
      "Epoch: 69, batch index: 1106, learning rate: [1.0000000000000005e-08], loss:2.2617573738098145\n",
      "Epoch: 69, batch index: 1107, learning rate: [1.0000000000000005e-08], loss:2.2800817489624023\n",
      "Epoch: 69, batch index: 1108, learning rate: [1.0000000000000005e-08], loss:2.2966411113739014\n",
      "Epoch: 69, batch index: 1109, learning rate: [1.0000000000000005e-08], loss:2.2599122524261475\n",
      "Epoch: 69, batch index: 1110, learning rate: [1.0000000000000005e-08], loss:2.284633159637451\n",
      "Epoch: 69, batch index: 1111, learning rate: [1.0000000000000005e-08], loss:2.2496488094329834\n",
      "Epoch: 69, batch index: 1112, learning rate: [1.0000000000000005e-08], loss:2.2626781463623047\n",
      "Epoch: 69, batch index: 1113, learning rate: [1.0000000000000005e-08], loss:2.2731728553771973\n",
      "Epoch: 69, batch index: 1114, learning rate: [1.0000000000000005e-08], loss:2.268366813659668\n",
      "Epoch: 69, batch index: 1115, learning rate: [1.0000000000000005e-08], loss:2.292379856109619\n",
      "Epoch: 69, batch index: 1116, learning rate: [1.0000000000000005e-08], loss:2.2296485900878906\n",
      "Epoch: 69, batch index: 1117, learning rate: [1.0000000000000005e-08], loss:2.2894718647003174\n",
      "Epoch: 69, batch index: 1118, learning rate: [1.0000000000000005e-08], loss:2.273167371749878\n",
      "Epoch: 69, batch index: 1119, learning rate: [1.0000000000000005e-08], loss:2.2638018131256104\n",
      "Epoch 69, validation accuracy score0.32421875\n",
      "Epoch: 70, batch index: 1120, learning rate: [1.0000000000000005e-08], loss:2.271434783935547\n",
      "Epoch: 70, batch index: 1121, learning rate: [1.0000000000000005e-08], loss:2.2664878368377686\n",
      "Epoch: 70, batch index: 1122, learning rate: [1.0000000000000005e-08], loss:2.2511744499206543\n",
      "Epoch: 70, batch index: 1123, learning rate: [1.0000000000000005e-08], loss:2.276867628097534\n",
      "Epoch: 70, batch index: 1124, learning rate: [1.0000000000000005e-08], loss:2.281578302383423\n",
      "Epoch: 70, batch index: 1125, learning rate: [1.0000000000000005e-08], loss:2.2687673568725586\n",
      "Epoch: 70, batch index: 1126, learning rate: [1.0000000000000005e-08], loss:2.2632312774658203\n",
      "Epoch: 70, batch index: 1127, learning rate: [1.0000000000000005e-08], loss:2.2761833667755127\n",
      "Epoch: 70, batch index: 1128, learning rate: [1.0000000000000005e-08], loss:2.267163038253784\n",
      "Epoch: 70, batch index: 1129, learning rate: [1.0000000000000005e-08], loss:2.289259195327759\n",
      "Epoch: 70, batch index: 1130, learning rate: [1.0000000000000005e-08], loss:2.2382633686065674\n",
      "Epoch: 70, batch index: 1131, learning rate: [1.0000000000000005e-08], loss:2.2569353580474854\n",
      "Epoch: 70, batch index: 1132, learning rate: [1.0000000000000005e-08], loss:2.269890546798706\n",
      "Epoch: 70, batch index: 1133, learning rate: [1.0000000000000005e-08], loss:2.2830474376678467\n",
      "Epoch: 70, batch index: 1134, learning rate: [1.0000000000000005e-08], loss:2.292268991470337\n",
      "Epoch: 70, batch index: 1135, learning rate: [1.0000000000000005e-08], loss:2.2505900859832764\n",
      "Epoch 70, validation accuracy score0.3359375\n",
      "Epoch: 71, batch index: 1136, learning rate: [1.0000000000000005e-08], loss:2.28108811378479\n",
      "Epoch: 71, batch index: 1137, learning rate: [1.0000000000000005e-08], loss:2.3030669689178467\n",
      "Epoch: 71, batch index: 1138, learning rate: [1.0000000000000005e-08], loss:2.235471725463867\n",
      "Epoch: 71, batch index: 1139, learning rate: [1.0000000000000005e-08], loss:2.28440523147583\n",
      "Epoch: 71, batch index: 1140, learning rate: [1.0000000000000005e-08], loss:2.2610843181610107\n",
      "Epoch: 71, batch index: 1141, learning rate: [1.0000000000000005e-08], loss:2.258624792098999\n",
      "Epoch: 71, batch index: 1142, learning rate: [1.0000000000000005e-08], loss:2.313790798187256\n",
      "Epoch: 71, batch index: 1143, learning rate: [1.0000000000000005e-08], loss:2.268415689468384\n",
      "Epoch: 71, batch index: 1144, learning rate: [1.0000000000000005e-08], loss:2.3100509643554688\n",
      "Epoch: 71, batch index: 1145, learning rate: [1.0000000000000005e-08], loss:2.2691328525543213\n",
      "Epoch: 71, batch index: 1146, learning rate: [1.0000000000000005e-08], loss:2.2917680740356445\n",
      "Epoch: 71, batch index: 1147, learning rate: [1.0000000000000005e-08], loss:2.2445664405822754\n",
      "Epoch: 71, batch index: 1148, learning rate: [1.0000000000000005e-08], loss:2.267918109893799\n",
      "Epoch: 71, batch index: 1149, learning rate: [1.0000000000000005e-08], loss:2.248600482940674\n",
      "Epoch: 71, batch index: 1150, learning rate: [1.0000000000000005e-08], loss:2.2627980709075928\n",
      "Epoch: 71, batch index: 1151, learning rate: [1.0000000000000005e-08], loss:2.2401630878448486\n",
      "Epoch 71, validation accuracy score0.333984375\n",
      "Epoch: 72, batch index: 1152, learning rate: [1.0000000000000005e-08], loss:2.2860000133514404\n",
      "Epoch: 72, batch index: 1153, learning rate: [1.0000000000000005e-08], loss:2.291084051132202\n",
      "Epoch: 72, batch index: 1154, learning rate: [1.0000000000000005e-08], loss:2.239741563796997\n",
      "Epoch: 72, batch index: 1155, learning rate: [1.0000000000000005e-08], loss:2.262896776199341\n",
      "Epoch: 72, batch index: 1156, learning rate: [1.0000000000000005e-08], loss:2.242163896560669\n",
      "Epoch: 72, batch index: 1157, learning rate: [1.0000000000000005e-08], loss:2.268927574157715\n",
      "Epoch: 72, batch index: 1158, learning rate: [1.0000000000000005e-08], loss:2.2689075469970703\n",
      "Epoch: 72, batch index: 1159, learning rate: [1.0000000000000005e-08], loss:2.293522357940674\n",
      "Epoch: 72, batch index: 1160, learning rate: [1.0000000000000005e-08], loss:2.2393622398376465\n",
      "Epoch: 72, batch index: 1161, learning rate: [1.0000000000000005e-08], loss:2.3190205097198486\n",
      "Epoch: 72, batch index: 1162, learning rate: [1.0000000000000005e-08], loss:2.278857946395874\n",
      "Epoch: 72, batch index: 1163, learning rate: [1.0000000000000005e-08], loss:2.278796672821045\n",
      "Epoch: 72, batch index: 1164, learning rate: [1.0000000000000005e-08], loss:2.254673480987549\n",
      "Epoch: 72, batch index: 1165, learning rate: [1.0000000000000005e-08], loss:2.287872791290283\n",
      "Epoch: 72, batch index: 1166, learning rate: [1.0000000000000005e-08], loss:2.303936004638672\n",
      "Epoch: 72, batch index: 1167, learning rate: [1.0000000000000005e-08], loss:2.2876102924346924\n",
      "Epoch 72, validation accuracy score0.2890625\n",
      "Epoch: 73, batch index: 1168, learning rate: [1.0000000000000005e-08], loss:2.285520315170288\n",
      "Epoch: 73, batch index: 1169, learning rate: [1.0000000000000005e-08], loss:2.2553560733795166\n",
      "Epoch: 73, batch index: 1170, learning rate: [1.0000000000000005e-08], loss:2.2265686988830566\n",
      "Epoch: 73, batch index: 1171, learning rate: [1.0000000000000005e-08], loss:2.270066976547241\n",
      "Epoch: 73, batch index: 1172, learning rate: [1.0000000000000005e-08], loss:2.2661404609680176\n",
      "Epoch: 73, batch index: 1173, learning rate: [1.0000000000000005e-08], loss:2.26936411857605\n",
      "Epoch: 73, batch index: 1174, learning rate: [1.0000000000000005e-08], loss:2.2701053619384766\n",
      "Epoch: 73, batch index: 1175, learning rate: [1.0000000000000005e-08], loss:2.2775113582611084\n",
      "Epoch: 73, batch index: 1176, learning rate: [1.0000000000000005e-08], loss:2.274435520172119\n",
      "Epoch: 73, batch index: 1177, learning rate: [1.0000000000000005e-08], loss:2.2896275520324707\n",
      "Epoch: 73, batch index: 1178, learning rate: [1.0000000000000005e-08], loss:2.2781693935394287\n",
      "Epoch: 73, batch index: 1179, learning rate: [1.0000000000000005e-08], loss:2.265089511871338\n",
      "Epoch: 73, batch index: 1180, learning rate: [1.0000000000000005e-08], loss:2.2525105476379395\n",
      "Epoch: 73, batch index: 1181, learning rate: [1.0000000000000005e-08], loss:2.285456657409668\n",
      "Epoch: 73, batch index: 1182, learning rate: [1.0000000000000005e-08], loss:2.2657206058502197\n",
      "Epoch: 73, batch index: 1183, learning rate: [1.0000000000000005e-08], loss:2.27156662940979\n",
      "Epoch 73, validation accuracy score0.326171875\n",
      "Epoch: 74, batch index: 1184, learning rate: [1.0000000000000005e-08], loss:2.261221408843994\n",
      "Epoch: 74, batch index: 1185, learning rate: [1.0000000000000005e-08], loss:2.247239112854004\n",
      "Epoch: 74, batch index: 1186, learning rate: [1.0000000000000005e-08], loss:2.27087140083313\n",
      "Epoch: 74, batch index: 1187, learning rate: [1.0000000000000005e-08], loss:2.3011765480041504\n",
      "Epoch: 74, batch index: 1188, learning rate: [1.0000000000000005e-08], loss:2.252587080001831\n",
      "Epoch: 74, batch index: 1189, learning rate: [1.0000000000000005e-08], loss:2.299471855163574\n",
      "Epoch: 74, batch index: 1190, learning rate: [1.0000000000000005e-08], loss:2.286569595336914\n",
      "Epoch: 74, batch index: 1191, learning rate: [1.0000000000000005e-08], loss:2.291180372238159\n",
      "Epoch: 74, batch index: 1192, learning rate: [1.0000000000000005e-08], loss:2.290513515472412\n",
      "Epoch: 74, batch index: 1193, learning rate: [1.0000000000000005e-08], loss:2.2612290382385254\n",
      "Epoch: 74, batch index: 1194, learning rate: [1.0000000000000005e-08], loss:2.2809109687805176\n",
      "Epoch: 74, batch index: 1195, learning rate: [1.0000000000000005e-08], loss:2.2705702781677246\n",
      "Epoch: 74, batch index: 1196, learning rate: [1.0000000000000005e-08], loss:2.288381814956665\n",
      "Epoch: 74, batch index: 1197, learning rate: [1.0000000000000005e-08], loss:2.2390975952148438\n",
      "Epoch: 74, batch index: 1198, learning rate: [1.0000000000000005e-08], loss:2.2658636569976807\n",
      "Epoch: 74, batch index: 1199, learning rate: [1.0000000000000005e-08], loss:2.266087532043457\n",
      "Epoch 74, validation accuracy score0.341796875\n",
      "Epoch: 75, batch index: 1200, learning rate: [1.0000000000000005e-08], loss:2.2967729568481445\n",
      "Epoch: 75, batch index: 1201, learning rate: [1.0000000000000005e-08], loss:2.267935037612915\n",
      "Epoch: 75, batch index: 1202, learning rate: [1.0000000000000005e-08], loss:2.280343532562256\n",
      "Epoch: 75, batch index: 1203, learning rate: [1.0000000000000005e-08], loss:2.278942584991455\n",
      "Epoch: 75, batch index: 1204, learning rate: [1.0000000000000005e-08], loss:2.2623629570007324\n",
      "Epoch: 75, batch index: 1205, learning rate: [1.0000000000000005e-08], loss:2.2692670822143555\n",
      "Epoch: 75, batch index: 1206, learning rate: [1.0000000000000005e-08], loss:2.280503988265991\n",
      "Epoch: 75, batch index: 1207, learning rate: [1.0000000000000005e-08], loss:2.254533052444458\n",
      "Epoch: 75, batch index: 1208, learning rate: [1.0000000000000005e-08], loss:2.248119592666626\n",
      "Epoch: 75, batch index: 1209, learning rate: [1.0000000000000005e-08], loss:2.2304775714874268\n",
      "Epoch: 75, batch index: 1210, learning rate: [1.0000000000000005e-08], loss:2.2402000427246094\n",
      "Epoch: 75, batch index: 1211, learning rate: [1.0000000000000005e-08], loss:2.265010356903076\n",
      "Epoch: 75, batch index: 1212, learning rate: [1.0000000000000005e-08], loss:2.2961816787719727\n",
      "Epoch: 75, batch index: 1213, learning rate: [1.0000000000000005e-08], loss:2.2493669986724854\n",
      "Epoch: 75, batch index: 1214, learning rate: [1.0000000000000005e-08], loss:2.2716574668884277\n",
      "Epoch: 75, batch index: 1215, learning rate: [1.0000000000000005e-08], loss:2.232011556625366\n",
      "Epoch 75, validation accuracy score0.345703125\n",
      "Epoch: 76, batch index: 1216, learning rate: [1.0000000000000005e-08], loss:2.273538112640381\n",
      "Epoch: 76, batch index: 1217, learning rate: [1.0000000000000005e-08], loss:2.2513113021850586\n",
      "Epoch: 76, batch index: 1218, learning rate: [1.0000000000000005e-08], loss:2.269285202026367\n",
      "Epoch: 76, batch index: 1219, learning rate: [1.0000000000000005e-08], loss:2.267204523086548\n",
      "Epoch: 76, batch index: 1220, learning rate: [1.0000000000000005e-08], loss:2.282829999923706\n",
      "Epoch: 76, batch index: 1221, learning rate: [1.0000000000000005e-08], loss:2.295498847961426\n",
      "Epoch: 76, batch index: 1222, learning rate: [1.0000000000000005e-08], loss:2.272775888442993\n",
      "Epoch: 76, batch index: 1223, learning rate: [1.0000000000000005e-08], loss:2.2786195278167725\n",
      "Epoch: 76, batch index: 1224, learning rate: [1.0000000000000005e-08], loss:2.272677421569824\n",
      "Epoch: 76, batch index: 1225, learning rate: [1.0000000000000005e-08], loss:2.2492470741271973\n",
      "Epoch: 76, batch index: 1226, learning rate: [1.0000000000000005e-08], loss:2.2913737297058105\n",
      "Epoch: 76, batch index: 1227, learning rate: [1.0000000000000005e-08], loss:2.278738498687744\n",
      "Epoch: 76, batch index: 1228, learning rate: [1.0000000000000005e-08], loss:2.2590136528015137\n",
      "Epoch: 76, batch index: 1229, learning rate: [1.0000000000000005e-08], loss:2.290959596633911\n",
      "Epoch: 76, batch index: 1230, learning rate: [1.0000000000000005e-08], loss:2.2493481636047363\n",
      "Epoch: 76, batch index: 1231, learning rate: [1.0000000000000005e-08], loss:2.2499027252197266\n",
      "Epoch 76, validation accuracy score0.30859375\n",
      "Epoch: 77, batch index: 1232, learning rate: [1.0000000000000005e-08], loss:2.2696893215179443\n",
      "Epoch: 77, batch index: 1233, learning rate: [1.0000000000000005e-08], loss:2.273164749145508\n",
      "Epoch: 77, batch index: 1234, learning rate: [1.0000000000000005e-08], loss:2.2838566303253174\n",
      "Epoch: 77, batch index: 1235, learning rate: [1.0000000000000005e-08], loss:2.272749900817871\n",
      "Epoch: 77, batch index: 1236, learning rate: [1.0000000000000005e-08], loss:2.2592883110046387\n",
      "Epoch: 77, batch index: 1237, learning rate: [1.0000000000000005e-08], loss:2.268977642059326\n",
      "Epoch: 77, batch index: 1238, learning rate: [1.0000000000000005e-08], loss:2.277082681655884\n",
      "Epoch: 77, batch index: 1239, learning rate: [1.0000000000000005e-08], loss:2.266116142272949\n",
      "Epoch: 77, batch index: 1240, learning rate: [1.0000000000000005e-08], loss:2.2549686431884766\n",
      "Epoch: 77, batch index: 1241, learning rate: [1.0000000000000005e-08], loss:2.255840539932251\n",
      "Epoch: 77, batch index: 1242, learning rate: [1.0000000000000005e-08], loss:2.260795831680298\n",
      "Epoch: 77, batch index: 1243, learning rate: [1.0000000000000005e-08], loss:2.3008663654327393\n",
      "Epoch: 77, batch index: 1244, learning rate: [1.0000000000000005e-08], loss:2.26393461227417\n",
      "Epoch: 77, batch index: 1245, learning rate: [1.0000000000000005e-08], loss:2.2784152030944824\n",
      "Epoch: 77, batch index: 1246, learning rate: [1.0000000000000005e-08], loss:2.248600482940674\n",
      "Epoch: 77, batch index: 1247, learning rate: [1.0000000000000005e-08], loss:2.261568069458008\n",
      "Epoch 77, validation accuracy score0.369140625\n",
      "Epoch: 78, batch index: 1248, learning rate: [1.0000000000000005e-08], loss:2.246049165725708\n",
      "Epoch: 78, batch index: 1249, learning rate: [1.0000000000000005e-08], loss:2.2780981063842773\n",
      "Epoch: 78, batch index: 1250, learning rate: [1.0000000000000005e-08], loss:2.2468385696411133\n",
      "Epoch: 78, batch index: 1251, learning rate: [1.0000000000000005e-08], loss:2.245164394378662\n",
      "Epoch: 78, batch index: 1252, learning rate: [1.0000000000000005e-08], loss:2.2724409103393555\n",
      "Epoch: 78, batch index: 1253, learning rate: [1.0000000000000005e-08], loss:2.2764384746551514\n",
      "Epoch: 78, batch index: 1254, learning rate: [1.0000000000000005e-08], loss:2.248352527618408\n",
      "Epoch: 78, batch index: 1255, learning rate: [1.0000000000000005e-08], loss:2.258688449859619\n",
      "Epoch: 78, batch index: 1256, learning rate: [1.0000000000000005e-08], loss:2.2724199295043945\n",
      "Epoch: 78, batch index: 1257, learning rate: [1.0000000000000005e-08], loss:2.258615016937256\n",
      "Epoch: 78, batch index: 1258, learning rate: [1.0000000000000005e-08], loss:2.258852005004883\n",
      "Epoch: 78, batch index: 1259, learning rate: [1.0000000000000005e-08], loss:2.3076024055480957\n",
      "Epoch: 78, batch index: 1260, learning rate: [1.0000000000000005e-08], loss:2.2498691082000732\n",
      "Epoch: 78, batch index: 1261, learning rate: [1.0000000000000005e-08], loss:2.2873096466064453\n",
      "Epoch: 78, batch index: 1262, learning rate: [1.0000000000000005e-08], loss:2.2756593227386475\n",
      "Epoch: 78, batch index: 1263, learning rate: [1.0000000000000005e-08], loss:2.25100040435791\n",
      "Epoch 78, validation accuracy score0.31640625\n",
      "Epoch: 79, batch index: 1264, learning rate: [1.0000000000000005e-08], loss:2.2983615398406982\n",
      "Epoch: 79, batch index: 1265, learning rate: [1.0000000000000005e-08], loss:2.2829031944274902\n",
      "Epoch: 79, batch index: 1266, learning rate: [1.0000000000000005e-08], loss:2.260164737701416\n",
      "Epoch: 79, batch index: 1267, learning rate: [1.0000000000000005e-08], loss:2.2927632331848145\n",
      "Epoch: 79, batch index: 1268, learning rate: [1.0000000000000005e-08], loss:2.274869918823242\n",
      "Epoch: 79, batch index: 1269, learning rate: [1.0000000000000005e-08], loss:2.2484371662139893\n",
      "Epoch: 79, batch index: 1270, learning rate: [1.0000000000000005e-08], loss:2.2869017124176025\n",
      "Epoch: 79, batch index: 1271, learning rate: [1.0000000000000005e-08], loss:2.2516937255859375\n",
      "Epoch: 79, batch index: 1272, learning rate: [1.0000000000000005e-08], loss:2.260782480239868\n",
      "Epoch: 79, batch index: 1273, learning rate: [1.0000000000000005e-08], loss:2.3065178394317627\n",
      "Epoch: 79, batch index: 1274, learning rate: [1.0000000000000005e-08], loss:2.2765965461730957\n",
      "Epoch: 79, batch index: 1275, learning rate: [1.0000000000000005e-08], loss:2.252133369445801\n",
      "Epoch: 79, batch index: 1276, learning rate: [1.0000000000000005e-08], loss:2.2262415885925293\n",
      "Epoch: 79, batch index: 1277, learning rate: [1.0000000000000005e-08], loss:2.2556185722351074\n",
      "Epoch: 79, batch index: 1278, learning rate: [1.0000000000000005e-08], loss:2.3138997554779053\n",
      "Epoch: 79, batch index: 1279, learning rate: [1.0000000000000005e-08], loss:2.241718053817749\n",
      "Epoch 79, validation accuracy score0.34375\n",
      "Epoch: 80, batch index: 1280, learning rate: [1.0000000000000005e-08], loss:2.284885883331299\n",
      "Epoch: 80, batch index: 1281, learning rate: [1.0000000000000005e-08], loss:2.272237777709961\n",
      "Epoch: 80, batch index: 1282, learning rate: [1.0000000000000005e-08], loss:2.3186254501342773\n",
      "Epoch: 80, batch index: 1283, learning rate: [1.0000000000000005e-08], loss:2.298250913619995\n",
      "Epoch: 80, batch index: 1284, learning rate: [1.0000000000000005e-08], loss:2.305950164794922\n",
      "Epoch: 80, batch index: 1285, learning rate: [1.0000000000000005e-08], loss:2.2883219718933105\n",
      "Epoch: 80, batch index: 1286, learning rate: [1.0000000000000005e-08], loss:2.2631876468658447\n",
      "Epoch: 80, batch index: 1287, learning rate: [1.0000000000000005e-08], loss:2.2689929008483887\n",
      "Epoch: 80, batch index: 1288, learning rate: [1.0000000000000005e-08], loss:2.260206699371338\n",
      "Epoch: 80, batch index: 1289, learning rate: [1.0000000000000005e-08], loss:2.2381787300109863\n",
      "Epoch: 80, batch index: 1290, learning rate: [1.0000000000000005e-08], loss:2.216609239578247\n",
      "Epoch: 80, batch index: 1291, learning rate: [1.0000000000000005e-08], loss:2.2480151653289795\n",
      "Epoch: 80, batch index: 1292, learning rate: [1.0000000000000005e-08], loss:2.305250406265259\n",
      "Epoch: 80, batch index: 1293, learning rate: [1.0000000000000005e-08], loss:2.2743351459503174\n",
      "Epoch: 80, batch index: 1294, learning rate: [1.0000000000000005e-08], loss:2.272209882736206\n",
      "Epoch: 80, batch index: 1295, learning rate: [1.0000000000000005e-08], loss:2.2305047512054443\n",
      "Epoch 80, validation accuracy score0.326171875\n",
      "Epoch: 81, batch index: 1296, learning rate: [1.0000000000000005e-08], loss:2.279716730117798\n",
      "Epoch: 81, batch index: 1297, learning rate: [1.0000000000000005e-08], loss:2.2917346954345703\n",
      "Epoch: 81, batch index: 1298, learning rate: [1.0000000000000005e-08], loss:2.2648732662200928\n",
      "Epoch: 81, batch index: 1299, learning rate: [1.0000000000000005e-08], loss:2.2691709995269775\n",
      "Epoch: 81, batch index: 1300, learning rate: [1.0000000000000005e-08], loss:2.2385916709899902\n",
      "Epoch: 81, batch index: 1301, learning rate: [1.0000000000000005e-08], loss:2.2852022647857666\n",
      "Epoch: 81, batch index: 1302, learning rate: [1.0000000000000005e-08], loss:2.293771266937256\n",
      "Epoch: 81, batch index: 1303, learning rate: [1.0000000000000005e-08], loss:2.24853253364563\n",
      "Epoch: 81, batch index: 1304, learning rate: [1.0000000000000005e-08], loss:2.297747850418091\n",
      "Epoch: 81, batch index: 1305, learning rate: [1.0000000000000005e-08], loss:2.2570199966430664\n",
      "Epoch: 81, batch index: 1306, learning rate: [1.0000000000000005e-08], loss:2.2508182525634766\n",
      "Epoch: 81, batch index: 1307, learning rate: [1.0000000000000005e-08], loss:2.2897050380706787\n",
      "Epoch: 81, batch index: 1308, learning rate: [1.0000000000000005e-08], loss:2.2696475982666016\n",
      "Epoch: 81, batch index: 1309, learning rate: [1.0000000000000005e-08], loss:2.2618813514709473\n",
      "Epoch: 81, batch index: 1310, learning rate: [1.0000000000000005e-08], loss:2.2705447673797607\n",
      "Epoch: 81, batch index: 1311, learning rate: [1.0000000000000005e-08], loss:2.2480337619781494\n",
      "Epoch 81, validation accuracy score0.314453125\n",
      "Epoch: 82, batch index: 1312, learning rate: [1.0000000000000005e-08], loss:2.2769625186920166\n",
      "Epoch: 82, batch index: 1313, learning rate: [1.0000000000000005e-08], loss:2.2250258922576904\n",
      "Epoch: 82, batch index: 1314, learning rate: [1.0000000000000005e-08], loss:2.298809766769409\n",
      "Epoch: 82, batch index: 1315, learning rate: [1.0000000000000005e-08], loss:2.274456262588501\n",
      "Epoch: 82, batch index: 1316, learning rate: [1.0000000000000005e-08], loss:2.268425464630127\n",
      "Epoch: 82, batch index: 1317, learning rate: [1.0000000000000005e-08], loss:2.283841133117676\n",
      "Epoch: 82, batch index: 1318, learning rate: [1.0000000000000005e-08], loss:2.3043510913848877\n",
      "Epoch: 82, batch index: 1319, learning rate: [1.0000000000000005e-08], loss:2.2635293006896973\n",
      "Epoch: 82, batch index: 1320, learning rate: [1.0000000000000005e-08], loss:2.266292095184326\n",
      "Epoch: 82, batch index: 1321, learning rate: [1.0000000000000005e-08], loss:2.277174234390259\n",
      "Epoch: 82, batch index: 1322, learning rate: [1.0000000000000005e-08], loss:2.2997703552246094\n",
      "Epoch: 82, batch index: 1323, learning rate: [1.0000000000000005e-08], loss:2.2571237087249756\n",
      "Epoch: 82, batch index: 1324, learning rate: [1.0000000000000005e-08], loss:2.2690422534942627\n",
      "Epoch: 82, batch index: 1325, learning rate: [1.0000000000000005e-08], loss:2.2726707458496094\n",
      "Epoch: 82, batch index: 1326, learning rate: [1.0000000000000005e-08], loss:2.27286434173584\n",
      "Epoch: 82, batch index: 1327, learning rate: [1.0000000000000005e-08], loss:2.299384117126465\n",
      "Epoch 82, validation accuracy score0.3359375\n",
      "Epoch: 83, batch index: 1328, learning rate: [1.0000000000000005e-08], loss:2.2954065799713135\n",
      "Epoch: 83, batch index: 1329, learning rate: [1.0000000000000005e-08], loss:2.259786367416382\n",
      "Epoch: 83, batch index: 1330, learning rate: [1.0000000000000005e-08], loss:2.2632737159729004\n",
      "Epoch: 83, batch index: 1331, learning rate: [1.0000000000000005e-08], loss:2.2947373390197754\n",
      "Epoch: 83, batch index: 1332, learning rate: [1.0000000000000005e-08], loss:2.276198625564575\n",
      "Epoch: 83, batch index: 1333, learning rate: [1.0000000000000005e-08], loss:2.237592935562134\n",
      "Epoch: 83, batch index: 1334, learning rate: [1.0000000000000005e-08], loss:2.3040082454681396\n",
      "Epoch: 83, batch index: 1335, learning rate: [1.0000000000000005e-08], loss:2.269233226776123\n",
      "Epoch: 83, batch index: 1336, learning rate: [1.0000000000000005e-08], loss:2.297612190246582\n",
      "Epoch: 83, batch index: 1337, learning rate: [1.0000000000000005e-08], loss:2.2464780807495117\n",
      "Epoch: 83, batch index: 1338, learning rate: [1.0000000000000005e-08], loss:2.2290842533111572\n",
      "Epoch: 83, batch index: 1339, learning rate: [1.0000000000000005e-08], loss:2.2294044494628906\n",
      "Epoch: 83, batch index: 1340, learning rate: [1.0000000000000005e-08], loss:2.263421058654785\n",
      "Epoch: 83, batch index: 1341, learning rate: [1.0000000000000005e-08], loss:2.3095312118530273\n",
      "Epoch: 83, batch index: 1342, learning rate: [1.0000000000000005e-08], loss:2.2568411827087402\n",
      "Epoch: 83, batch index: 1343, learning rate: [1.0000000000000005e-08], loss:2.2895703315734863\n",
      "Epoch 83, validation accuracy score0.3203125\n",
      "Epoch: 84, batch index: 1344, learning rate: [1.0000000000000005e-08], loss:2.218224287033081\n",
      "Epoch: 84, batch index: 1345, learning rate: [1.0000000000000005e-08], loss:2.265108823776245\n",
      "Epoch: 84, batch index: 1346, learning rate: [1.0000000000000005e-08], loss:2.249950885772705\n",
      "Epoch: 84, batch index: 1347, learning rate: [1.0000000000000005e-08], loss:2.315924882888794\n",
      "Epoch: 84, batch index: 1348, learning rate: [1.0000000000000005e-08], loss:2.268644094467163\n",
      "Epoch: 84, batch index: 1349, learning rate: [1.0000000000000005e-08], loss:2.263212203979492\n",
      "Epoch: 84, batch index: 1350, learning rate: [1.0000000000000005e-08], loss:2.2724454402923584\n",
      "Epoch: 84, batch index: 1351, learning rate: [1.0000000000000005e-08], loss:2.2631120681762695\n",
      "Epoch: 84, batch index: 1352, learning rate: [1.0000000000000005e-08], loss:2.2897634506225586\n",
      "Epoch: 84, batch index: 1353, learning rate: [1.0000000000000005e-08], loss:2.249558448791504\n",
      "Epoch: 84, batch index: 1354, learning rate: [1.0000000000000005e-08], loss:2.257387638092041\n",
      "Epoch: 84, batch index: 1355, learning rate: [1.0000000000000005e-08], loss:2.2241477966308594\n",
      "Epoch: 84, batch index: 1356, learning rate: [1.0000000000000005e-08], loss:2.2794830799102783\n",
      "Epoch: 84, batch index: 1357, learning rate: [1.0000000000000005e-08], loss:2.297697067260742\n",
      "Epoch: 84, batch index: 1358, learning rate: [1.0000000000000005e-08], loss:2.2683489322662354\n",
      "Epoch: 84, batch index: 1359, learning rate: [1.0000000000000005e-08], loss:2.2674155235290527\n",
      "Epoch 84, validation accuracy score0.33203125\n",
      "Epoch: 85, batch index: 1360, learning rate: [1.0000000000000005e-08], loss:2.25211238861084\n",
      "Epoch: 85, batch index: 1361, learning rate: [1.0000000000000005e-08], loss:2.254638195037842\n",
      "Epoch: 85, batch index: 1362, learning rate: [1.0000000000000005e-08], loss:2.227678060531616\n",
      "Epoch: 85, batch index: 1363, learning rate: [1.0000000000000005e-08], loss:2.2769277095794678\n",
      "Epoch: 85, batch index: 1364, learning rate: [1.0000000000000005e-08], loss:2.308459758758545\n",
      "Epoch: 85, batch index: 1365, learning rate: [1.0000000000000005e-08], loss:2.278512954711914\n",
      "Epoch: 85, batch index: 1366, learning rate: [1.0000000000000005e-08], loss:2.3001270294189453\n",
      "Epoch: 85, batch index: 1367, learning rate: [1.0000000000000005e-08], loss:2.2363650798797607\n",
      "Epoch: 85, batch index: 1368, learning rate: [1.0000000000000005e-08], loss:2.2955524921417236\n",
      "Epoch: 85, batch index: 1369, learning rate: [1.0000000000000005e-08], loss:2.2808189392089844\n",
      "Epoch: 85, batch index: 1370, learning rate: [1.0000000000000005e-08], loss:2.258939027786255\n",
      "Epoch: 85, batch index: 1371, learning rate: [1.0000000000000005e-08], loss:2.29050350189209\n",
      "Epoch: 85, batch index: 1372, learning rate: [1.0000000000000005e-08], loss:2.27880597114563\n",
      "Epoch: 85, batch index: 1373, learning rate: [1.0000000000000005e-08], loss:2.2563366889953613\n",
      "Epoch: 85, batch index: 1374, learning rate: [1.0000000000000005e-08], loss:2.277275323867798\n",
      "Epoch: 85, batch index: 1375, learning rate: [1.0000000000000005e-08], loss:2.2959554195404053\n",
      "Epoch 85, validation accuracy score0.3359375\n",
      "Epoch: 86, batch index: 1376, learning rate: [1.0000000000000005e-08], loss:2.272968053817749\n",
      "Epoch: 86, batch index: 1377, learning rate: [1.0000000000000005e-08], loss:2.282548427581787\n",
      "Epoch: 86, batch index: 1378, learning rate: [1.0000000000000005e-08], loss:2.2616939544677734\n",
      "Epoch: 86, batch index: 1379, learning rate: [1.0000000000000005e-08], loss:2.250185966491699\n",
      "Epoch: 86, batch index: 1380, learning rate: [1.0000000000000005e-08], loss:2.275803327560425\n",
      "Epoch: 86, batch index: 1381, learning rate: [1.0000000000000005e-08], loss:2.3018453121185303\n",
      "Epoch: 86, batch index: 1382, learning rate: [1.0000000000000005e-08], loss:2.2655649185180664\n",
      "Epoch: 86, batch index: 1383, learning rate: [1.0000000000000005e-08], loss:2.269094467163086\n",
      "Epoch: 86, batch index: 1384, learning rate: [1.0000000000000005e-08], loss:2.2488081455230713\n",
      "Epoch: 86, batch index: 1385, learning rate: [1.0000000000000005e-08], loss:2.265258312225342\n",
      "Epoch: 86, batch index: 1386, learning rate: [1.0000000000000005e-08], loss:2.284669876098633\n",
      "Epoch: 86, batch index: 1387, learning rate: [1.0000000000000005e-08], loss:2.283519983291626\n",
      "Epoch: 86, batch index: 1388, learning rate: [1.0000000000000005e-08], loss:2.2473437786102295\n",
      "Epoch: 86, batch index: 1389, learning rate: [1.0000000000000005e-08], loss:2.279163360595703\n",
      "Epoch: 86, batch index: 1390, learning rate: [1.0000000000000005e-08], loss:2.2762691974639893\n",
      "Epoch: 86, batch index: 1391, learning rate: [1.0000000000000005e-08], loss:2.270249843597412\n",
      "Epoch 86, validation accuracy score0.3359375\n",
      "Epoch: 87, batch index: 1392, learning rate: [1.0000000000000005e-08], loss:2.241523027420044\n",
      "Epoch: 87, batch index: 1393, learning rate: [1.0000000000000005e-08], loss:2.2726831436157227\n",
      "Epoch: 87, batch index: 1394, learning rate: [1.0000000000000005e-08], loss:2.28527569770813\n",
      "Epoch: 87, batch index: 1395, learning rate: [1.0000000000000005e-08], loss:2.27169132232666\n",
      "Epoch: 87, batch index: 1396, learning rate: [1.0000000000000005e-08], loss:2.275707244873047\n",
      "Epoch: 87, batch index: 1397, learning rate: [1.0000000000000005e-08], loss:2.2344448566436768\n",
      "Epoch: 87, batch index: 1398, learning rate: [1.0000000000000005e-08], loss:2.2652487754821777\n",
      "Epoch: 87, batch index: 1399, learning rate: [1.0000000000000005e-08], loss:2.2383460998535156\n",
      "Epoch: 87, batch index: 1400, learning rate: [1.0000000000000005e-08], loss:2.3005433082580566\n",
      "Epoch: 87, batch index: 1401, learning rate: [1.0000000000000005e-08], loss:2.300105333328247\n",
      "Epoch: 87, batch index: 1402, learning rate: [1.0000000000000005e-08], loss:2.2421071529388428\n",
      "Epoch: 87, batch index: 1403, learning rate: [1.0000000000000005e-08], loss:2.2558343410491943\n",
      "Epoch: 87, batch index: 1404, learning rate: [1.0000000000000005e-08], loss:2.2536139488220215\n",
      "Epoch: 87, batch index: 1405, learning rate: [1.0000000000000005e-08], loss:2.2390048503875732\n",
      "Epoch: 87, batch index: 1406, learning rate: [1.0000000000000005e-08], loss:2.297445058822632\n",
      "Epoch: 87, batch index: 1407, learning rate: [1.0000000000000005e-08], loss:2.301870822906494\n",
      "Epoch 87, validation accuracy score0.345703125\n",
      "Epoch: 88, batch index: 1408, learning rate: [1.0000000000000005e-08], loss:2.290234088897705\n",
      "Epoch: 88, batch index: 1409, learning rate: [1.0000000000000005e-08], loss:2.259673833847046\n",
      "Epoch: 88, batch index: 1410, learning rate: [1.0000000000000005e-08], loss:2.2633790969848633\n",
      "Epoch: 88, batch index: 1411, learning rate: [1.0000000000000005e-08], loss:2.2665750980377197\n",
      "Epoch: 88, batch index: 1412, learning rate: [1.0000000000000005e-08], loss:2.2481536865234375\n",
      "Epoch: 88, batch index: 1413, learning rate: [1.0000000000000005e-08], loss:2.2544167041778564\n",
      "Epoch: 88, batch index: 1414, learning rate: [1.0000000000000005e-08], loss:2.2633113861083984\n",
      "Epoch: 88, batch index: 1415, learning rate: [1.0000000000000005e-08], loss:2.264101982116699\n",
      "Epoch: 88, batch index: 1416, learning rate: [1.0000000000000005e-08], loss:2.3036723136901855\n",
      "Epoch: 88, batch index: 1417, learning rate: [1.0000000000000005e-08], loss:2.2834463119506836\n",
      "Epoch: 88, batch index: 1418, learning rate: [1.0000000000000005e-08], loss:2.2898025512695312\n",
      "Epoch: 88, batch index: 1419, learning rate: [1.0000000000000005e-08], loss:2.2579402923583984\n",
      "Epoch: 88, batch index: 1420, learning rate: [1.0000000000000005e-08], loss:2.2506511211395264\n",
      "Epoch: 88, batch index: 1421, learning rate: [1.0000000000000005e-08], loss:2.248643636703491\n",
      "Epoch: 88, batch index: 1422, learning rate: [1.0000000000000005e-08], loss:2.2611749172210693\n",
      "Epoch: 88, batch index: 1423, learning rate: [1.0000000000000005e-08], loss:2.345524787902832\n",
      "Epoch 88, validation accuracy score0.3125\n",
      "Epoch: 89, batch index: 1424, learning rate: [1.0000000000000005e-08], loss:2.223737955093384\n",
      "Epoch: 89, batch index: 1425, learning rate: [1.0000000000000005e-08], loss:2.3085994720458984\n",
      "Epoch: 89, batch index: 1426, learning rate: [1.0000000000000005e-08], loss:2.2955501079559326\n",
      "Epoch: 89, batch index: 1427, learning rate: [1.0000000000000005e-08], loss:2.258333683013916\n",
      "Epoch: 89, batch index: 1428, learning rate: [1.0000000000000005e-08], loss:2.267056465148926\n",
      "Epoch: 89, batch index: 1429, learning rate: [1.0000000000000005e-08], loss:2.282029151916504\n",
      "Epoch: 89, batch index: 1430, learning rate: [1.0000000000000005e-08], loss:2.252139091491699\n",
      "Epoch: 89, batch index: 1431, learning rate: [1.0000000000000005e-08], loss:2.3035166263580322\n",
      "Epoch: 89, batch index: 1432, learning rate: [1.0000000000000005e-08], loss:2.276912212371826\n",
      "Epoch: 89, batch index: 1433, learning rate: [1.0000000000000005e-08], loss:2.2869503498077393\n",
      "Epoch: 89, batch index: 1434, learning rate: [1.0000000000000005e-08], loss:2.2872114181518555\n",
      "Epoch: 89, batch index: 1435, learning rate: [1.0000000000000005e-08], loss:2.2318015098571777\n",
      "Epoch: 89, batch index: 1436, learning rate: [1.0000000000000005e-08], loss:2.2679312229156494\n",
      "Epoch: 89, batch index: 1437, learning rate: [1.0000000000000005e-08], loss:2.2620856761932373\n",
      "Epoch: 89, batch index: 1438, learning rate: [1.0000000000000005e-08], loss:2.2374024391174316\n",
      "Epoch: 89, batch index: 1439, learning rate: [1.0000000000000005e-08], loss:2.2301578521728516\n",
      "Epoch 89, validation accuracy score0.341796875\n",
      "Epoch: 90, batch index: 1440, learning rate: [1.0000000000000005e-08], loss:2.290266275405884\n",
      "Epoch: 90, batch index: 1441, learning rate: [1.0000000000000005e-08], loss:2.295034170150757\n",
      "Epoch: 90, batch index: 1442, learning rate: [1.0000000000000005e-08], loss:2.24222993850708\n",
      "Epoch: 90, batch index: 1443, learning rate: [1.0000000000000005e-08], loss:2.2713639736175537\n",
      "Epoch: 90, batch index: 1444, learning rate: [1.0000000000000005e-08], loss:2.2759203910827637\n",
      "Epoch: 90, batch index: 1445, learning rate: [1.0000000000000005e-08], loss:2.2380709648132324\n",
      "Epoch: 90, batch index: 1446, learning rate: [1.0000000000000005e-08], loss:2.2413785457611084\n",
      "Epoch: 90, batch index: 1447, learning rate: [1.0000000000000005e-08], loss:2.265449047088623\n",
      "Epoch: 90, batch index: 1448, learning rate: [1.0000000000000005e-08], loss:2.266418695449829\n",
      "Epoch: 90, batch index: 1449, learning rate: [1.0000000000000005e-08], loss:2.2689714431762695\n",
      "Epoch: 90, batch index: 1450, learning rate: [1.0000000000000005e-08], loss:2.2552738189697266\n",
      "Epoch: 90, batch index: 1451, learning rate: [1.0000000000000005e-08], loss:2.265364646911621\n",
      "Epoch: 90, batch index: 1452, learning rate: [1.0000000000000005e-08], loss:2.257362127304077\n",
      "Epoch: 90, batch index: 1453, learning rate: [1.0000000000000005e-08], loss:2.2813665866851807\n",
      "Epoch: 90, batch index: 1454, learning rate: [1.0000000000000005e-08], loss:2.3009393215179443\n",
      "Epoch: 90, batch index: 1455, learning rate: [1.0000000000000005e-08], loss:2.2793610095977783\n",
      "Epoch 90, validation accuracy score0.337890625\n",
      "Epoch: 91, batch index: 1456, learning rate: [1.0000000000000005e-08], loss:2.2722928524017334\n",
      "Epoch: 91, batch index: 1457, learning rate: [1.0000000000000005e-08], loss:2.2715048789978027\n",
      "Epoch: 91, batch index: 1458, learning rate: [1.0000000000000005e-08], loss:2.248734951019287\n",
      "Epoch: 91, batch index: 1459, learning rate: [1.0000000000000005e-08], loss:2.2490105628967285\n",
      "Epoch: 91, batch index: 1460, learning rate: [1.0000000000000005e-08], loss:2.2700552940368652\n",
      "Epoch: 91, batch index: 1461, learning rate: [1.0000000000000005e-08], loss:2.230039358139038\n",
      "Epoch: 91, batch index: 1462, learning rate: [1.0000000000000005e-08], loss:2.2840816974639893\n",
      "Epoch: 91, batch index: 1463, learning rate: [1.0000000000000005e-08], loss:2.247946262359619\n",
      "Epoch: 91, batch index: 1464, learning rate: [1.0000000000000005e-08], loss:2.264413356781006\n",
      "Epoch: 91, batch index: 1465, learning rate: [1.0000000000000005e-08], loss:2.2541913986206055\n",
      "Epoch: 91, batch index: 1466, learning rate: [1.0000000000000005e-08], loss:2.2736222743988037\n",
      "Epoch: 91, batch index: 1467, learning rate: [1.0000000000000005e-08], loss:2.3429617881774902\n",
      "Epoch: 91, batch index: 1468, learning rate: [1.0000000000000005e-08], loss:2.272331714630127\n",
      "Epoch: 91, batch index: 1469, learning rate: [1.0000000000000005e-08], loss:2.303635835647583\n",
      "Epoch: 91, batch index: 1470, learning rate: [1.0000000000000005e-08], loss:2.2712366580963135\n",
      "Epoch: 91, batch index: 1471, learning rate: [1.0000000000000005e-08], loss:2.3173763751983643\n",
      "Epoch 91, validation accuracy score0.337890625\n",
      "Epoch: 92, batch index: 1472, learning rate: [1.0000000000000005e-08], loss:2.2687935829162598\n",
      "Epoch: 92, batch index: 1473, learning rate: [1.0000000000000005e-08], loss:2.2960164546966553\n",
      "Epoch: 92, batch index: 1474, learning rate: [1.0000000000000005e-08], loss:2.284437417984009\n",
      "Epoch: 92, batch index: 1475, learning rate: [1.0000000000000005e-08], loss:2.277405261993408\n",
      "Epoch: 92, batch index: 1476, learning rate: [1.0000000000000005e-08], loss:2.3030529022216797\n",
      "Epoch: 92, batch index: 1477, learning rate: [1.0000000000000005e-08], loss:2.285470962524414\n",
      "Epoch: 92, batch index: 1478, learning rate: [1.0000000000000005e-08], loss:2.2525203227996826\n",
      "Epoch: 92, batch index: 1479, learning rate: [1.0000000000000005e-08], loss:2.2400283813476562\n",
      "Epoch: 92, batch index: 1480, learning rate: [1.0000000000000005e-08], loss:2.2652785778045654\n",
      "Epoch: 92, batch index: 1481, learning rate: [1.0000000000000005e-08], loss:2.276822328567505\n",
      "Epoch: 92, batch index: 1482, learning rate: [1.0000000000000005e-08], loss:2.2328715324401855\n",
      "Epoch: 92, batch index: 1483, learning rate: [1.0000000000000005e-08], loss:2.280402898788452\n",
      "Epoch: 92, batch index: 1484, learning rate: [1.0000000000000005e-08], loss:2.2766711711883545\n",
      "Epoch: 92, batch index: 1485, learning rate: [1.0000000000000005e-08], loss:2.2881505489349365\n",
      "Epoch: 92, batch index: 1486, learning rate: [1.0000000000000005e-08], loss:2.290677785873413\n",
      "Epoch: 92, batch index: 1487, learning rate: [1.0000000000000005e-08], loss:2.238987922668457\n",
      "Epoch 92, validation accuracy score0.361328125\n",
      "Epoch: 93, batch index: 1488, learning rate: [1.0000000000000005e-08], loss:2.2759320735931396\n",
      "Epoch: 93, batch index: 1489, learning rate: [1.0000000000000005e-08], loss:2.2433979511260986\n",
      "Epoch: 93, batch index: 1490, learning rate: [1.0000000000000005e-08], loss:2.2925832271575928\n",
      "Epoch: 93, batch index: 1491, learning rate: [1.0000000000000005e-08], loss:2.2703356742858887\n",
      "Epoch: 93, batch index: 1492, learning rate: [1.0000000000000005e-08], loss:2.249781370162964\n",
      "Epoch: 93, batch index: 1493, learning rate: [1.0000000000000005e-08], loss:2.2641496658325195\n",
      "Epoch: 93, batch index: 1494, learning rate: [1.0000000000000005e-08], loss:2.2679443359375\n",
      "Epoch: 93, batch index: 1495, learning rate: [1.0000000000000005e-08], loss:2.269331216812134\n",
      "Epoch: 93, batch index: 1496, learning rate: [1.0000000000000005e-08], loss:2.250364303588867\n",
      "Epoch: 93, batch index: 1497, learning rate: [1.0000000000000005e-08], loss:2.279503345489502\n",
      "Epoch: 93, batch index: 1498, learning rate: [1.0000000000000005e-08], loss:2.2873833179473877\n",
      "Epoch: 93, batch index: 1499, learning rate: [1.0000000000000005e-08], loss:2.285045862197876\n",
      "Epoch: 93, batch index: 1500, learning rate: [1.0000000000000005e-08], loss:2.300251007080078\n",
      "Epoch: 93, batch index: 1501, learning rate: [1.0000000000000005e-08], loss:2.260281801223755\n",
      "Epoch: 93, batch index: 1502, learning rate: [1.0000000000000005e-08], loss:2.2584962844848633\n",
      "Epoch: 93, batch index: 1503, learning rate: [1.0000000000000005e-08], loss:2.3336853981018066\n",
      "Epoch 93, validation accuracy score0.31640625\n",
      "Epoch: 94, batch index: 1504, learning rate: [1.0000000000000005e-08], loss:2.252852439880371\n",
      "Epoch: 94, batch index: 1505, learning rate: [1.0000000000000005e-08], loss:2.30268931388855\n",
      "Epoch: 94, batch index: 1506, learning rate: [1.0000000000000005e-08], loss:2.2669365406036377\n",
      "Epoch: 94, batch index: 1507, learning rate: [1.0000000000000005e-08], loss:2.275205373764038\n",
      "Epoch: 94, batch index: 1508, learning rate: [1.0000000000000005e-08], loss:2.2946014404296875\n",
      "Epoch: 94, batch index: 1509, learning rate: [1.0000000000000005e-08], loss:2.2486419677734375\n",
      "Epoch: 94, batch index: 1510, learning rate: [1.0000000000000005e-08], loss:2.312532901763916\n",
      "Epoch: 94, batch index: 1511, learning rate: [1.0000000000000005e-08], loss:2.244924545288086\n",
      "Epoch: 94, batch index: 1512, learning rate: [1.0000000000000005e-08], loss:2.294031858444214\n",
      "Epoch: 94, batch index: 1513, learning rate: [1.0000000000000005e-08], loss:2.2586281299591064\n",
      "Epoch: 94, batch index: 1514, learning rate: [1.0000000000000005e-08], loss:2.239194393157959\n",
      "Epoch: 94, batch index: 1515, learning rate: [1.0000000000000005e-08], loss:2.2815558910369873\n",
      "Epoch: 94, batch index: 1516, learning rate: [1.0000000000000005e-08], loss:2.2731027603149414\n",
      "Epoch: 94, batch index: 1517, learning rate: [1.0000000000000005e-08], loss:2.280179500579834\n",
      "Epoch: 94, batch index: 1518, learning rate: [1.0000000000000005e-08], loss:2.2636470794677734\n",
      "Epoch: 94, batch index: 1519, learning rate: [1.0000000000000005e-08], loss:2.2608842849731445\n",
      "Epoch 94, validation accuracy score0.32421875\n",
      "Epoch: 95, batch index: 1520, learning rate: [1.0000000000000005e-08], loss:2.244743824005127\n",
      "Epoch: 95, batch index: 1521, learning rate: [1.0000000000000005e-08], loss:2.2747697830200195\n",
      "Epoch: 95, batch index: 1522, learning rate: [1.0000000000000005e-08], loss:2.2686750888824463\n",
      "Epoch: 95, batch index: 1523, learning rate: [1.0000000000000005e-08], loss:2.2537431716918945\n",
      "Epoch: 95, batch index: 1524, learning rate: [1.0000000000000005e-08], loss:2.2555999755859375\n",
      "Epoch: 95, batch index: 1525, learning rate: [1.0000000000000005e-08], loss:2.3091318607330322\n",
      "Epoch: 95, batch index: 1526, learning rate: [1.0000000000000005e-08], loss:2.2848000526428223\n",
      "Epoch: 95, batch index: 1527, learning rate: [1.0000000000000005e-08], loss:2.267040967941284\n",
      "Epoch: 95, batch index: 1528, learning rate: [1.0000000000000005e-08], loss:2.2614126205444336\n",
      "Epoch: 95, batch index: 1529, learning rate: [1.0000000000000005e-08], loss:2.2414252758026123\n",
      "Epoch: 95, batch index: 1530, learning rate: [1.0000000000000005e-08], loss:2.2511789798736572\n",
      "Epoch: 95, batch index: 1531, learning rate: [1.0000000000000005e-08], loss:2.287287712097168\n",
      "Epoch: 95, batch index: 1532, learning rate: [1.0000000000000005e-08], loss:2.2909328937530518\n",
      "Epoch: 95, batch index: 1533, learning rate: [1.0000000000000005e-08], loss:2.295991897583008\n",
      "Epoch: 95, batch index: 1534, learning rate: [1.0000000000000005e-08], loss:2.298130512237549\n",
      "Epoch: 95, batch index: 1535, learning rate: [1.0000000000000005e-08], loss:2.2867300510406494\n",
      "Epoch 95, validation accuracy score0.326171875\n",
      "Epoch: 96, batch index: 1536, learning rate: [1.0000000000000005e-08], loss:2.2372632026672363\n",
      "Epoch: 96, batch index: 1537, learning rate: [1.0000000000000005e-08], loss:2.257368803024292\n",
      "Epoch: 96, batch index: 1538, learning rate: [1.0000000000000005e-08], loss:2.2545413970947266\n",
      "Epoch: 96, batch index: 1539, learning rate: [1.0000000000000005e-08], loss:2.2717957496643066\n",
      "Epoch: 96, batch index: 1540, learning rate: [1.0000000000000005e-08], loss:2.254694700241089\n",
      "Epoch: 96, batch index: 1541, learning rate: [1.0000000000000005e-08], loss:2.2865283489227295\n",
      "Epoch: 96, batch index: 1542, learning rate: [1.0000000000000005e-08], loss:2.247896432876587\n",
      "Epoch: 96, batch index: 1543, learning rate: [1.0000000000000005e-08], loss:2.275228977203369\n",
      "Epoch: 96, batch index: 1544, learning rate: [1.0000000000000005e-08], loss:2.2863337993621826\n",
      "Epoch: 96, batch index: 1545, learning rate: [1.0000000000000005e-08], loss:2.281582832336426\n",
      "Epoch: 96, batch index: 1546, learning rate: [1.0000000000000005e-08], loss:2.3115334510803223\n",
      "Epoch: 96, batch index: 1547, learning rate: [1.0000000000000005e-08], loss:2.2995877265930176\n",
      "Epoch: 96, batch index: 1548, learning rate: [1.0000000000000005e-08], loss:2.2667019367218018\n",
      "Epoch: 96, batch index: 1549, learning rate: [1.0000000000000005e-08], loss:2.246204376220703\n",
      "Epoch: 96, batch index: 1550, learning rate: [1.0000000000000005e-08], loss:2.2604329586029053\n",
      "Epoch: 96, batch index: 1551, learning rate: [1.0000000000000005e-08], loss:2.256258249282837\n",
      "Epoch 96, validation accuracy score0.333984375\n",
      "Epoch: 97, batch index: 1552, learning rate: [1.0000000000000005e-08], loss:2.312481641769409\n",
      "Epoch: 97, batch index: 1553, learning rate: [1.0000000000000005e-08], loss:2.2542834281921387\n",
      "Epoch: 97, batch index: 1554, learning rate: [1.0000000000000005e-08], loss:2.25482177734375\n",
      "Epoch: 97, batch index: 1555, learning rate: [1.0000000000000005e-08], loss:2.27970814704895\n",
      "Epoch: 97, batch index: 1556, learning rate: [1.0000000000000005e-08], loss:2.25368070602417\n",
      "Epoch: 97, batch index: 1557, learning rate: [1.0000000000000005e-08], loss:2.292438507080078\n",
      "Epoch: 97, batch index: 1558, learning rate: [1.0000000000000005e-08], loss:2.273023843765259\n",
      "Epoch: 97, batch index: 1559, learning rate: [1.0000000000000005e-08], loss:2.2990846633911133\n",
      "Epoch: 97, batch index: 1560, learning rate: [1.0000000000000005e-08], loss:2.258509874343872\n",
      "Epoch: 97, batch index: 1561, learning rate: [1.0000000000000005e-08], loss:2.2536306381225586\n",
      "Epoch: 97, batch index: 1562, learning rate: [1.0000000000000005e-08], loss:2.255202531814575\n",
      "Epoch: 97, batch index: 1563, learning rate: [1.0000000000000005e-08], loss:2.277524471282959\n",
      "Epoch: 97, batch index: 1564, learning rate: [1.0000000000000005e-08], loss:2.25993275642395\n",
      "Epoch: 97, batch index: 1565, learning rate: [1.0000000000000005e-08], loss:2.2535107135772705\n",
      "Epoch: 97, batch index: 1566, learning rate: [1.0000000000000005e-08], loss:2.2776334285736084\n",
      "Epoch: 97, batch index: 1567, learning rate: [1.0000000000000005e-08], loss:2.2518579959869385\n",
      "Epoch 97, validation accuracy score0.298828125\n",
      "Epoch: 98, batch index: 1568, learning rate: [1.0000000000000005e-08], loss:2.2463154792785645\n",
      "Epoch: 98, batch index: 1569, learning rate: [1.0000000000000005e-08], loss:2.240684986114502\n",
      "Epoch: 98, batch index: 1570, learning rate: [1.0000000000000005e-08], loss:2.2601890563964844\n",
      "Epoch: 98, batch index: 1571, learning rate: [1.0000000000000005e-08], loss:2.298978328704834\n",
      "Epoch: 98, batch index: 1572, learning rate: [1.0000000000000005e-08], loss:2.3077456951141357\n",
      "Epoch: 98, batch index: 1573, learning rate: [1.0000000000000005e-08], loss:2.251624822616577\n",
      "Epoch: 98, batch index: 1574, learning rate: [1.0000000000000005e-08], loss:2.2788586616516113\n",
      "Epoch: 98, batch index: 1575, learning rate: [1.0000000000000005e-08], loss:2.233227014541626\n",
      "Epoch: 98, batch index: 1576, learning rate: [1.0000000000000005e-08], loss:2.250480890274048\n",
      "Epoch: 98, batch index: 1577, learning rate: [1.0000000000000005e-08], loss:2.2583858966827393\n",
      "Epoch: 98, batch index: 1578, learning rate: [1.0000000000000005e-08], loss:2.2494373321533203\n",
      "Epoch: 98, batch index: 1579, learning rate: [1.0000000000000005e-08], loss:2.290844202041626\n",
      "Epoch: 98, batch index: 1580, learning rate: [1.0000000000000005e-08], loss:2.2826552391052246\n",
      "Epoch: 98, batch index: 1581, learning rate: [1.0000000000000005e-08], loss:2.285670518875122\n",
      "Epoch: 98, batch index: 1582, learning rate: [1.0000000000000005e-08], loss:2.3230535984039307\n",
      "Epoch: 98, batch index: 1583, learning rate: [1.0000000000000005e-08], loss:2.266702175140381\n",
      "Epoch 98, validation accuracy score0.35546875\n",
      "Epoch: 99, batch index: 1584, learning rate: [1.0000000000000005e-08], loss:2.2813475131988525\n",
      "Epoch: 99, batch index: 1585, learning rate: [1.0000000000000005e-08], loss:2.2618966102600098\n",
      "Epoch: 99, batch index: 1586, learning rate: [1.0000000000000005e-08], loss:2.26267671585083\n",
      "Epoch: 99, batch index: 1587, learning rate: [1.0000000000000005e-08], loss:2.29190993309021\n",
      "Epoch: 99, batch index: 1588, learning rate: [1.0000000000000005e-08], loss:2.2772295475006104\n",
      "Epoch: 99, batch index: 1589, learning rate: [1.0000000000000005e-08], loss:2.25697660446167\n",
      "Epoch: 99, batch index: 1590, learning rate: [1.0000000000000005e-08], loss:2.286970853805542\n",
      "Epoch: 99, batch index: 1591, learning rate: [1.0000000000000005e-08], loss:2.2617504596710205\n",
      "Epoch: 99, batch index: 1592, learning rate: [1.0000000000000005e-08], loss:2.261556386947632\n",
      "Epoch: 99, batch index: 1593, learning rate: [1.0000000000000005e-08], loss:2.2825984954833984\n",
      "Epoch: 99, batch index: 1594, learning rate: [1.0000000000000005e-08], loss:2.270704746246338\n",
      "Epoch: 99, batch index: 1595, learning rate: [1.0000000000000005e-08], loss:2.2653186321258545\n",
      "Epoch: 99, batch index: 1596, learning rate: [1.0000000000000005e-08], loss:2.2457213401794434\n",
      "Epoch: 99, batch index: 1597, learning rate: [1.0000000000000005e-08], loss:2.2956149578094482\n",
      "Epoch: 99, batch index: 1598, learning rate: [1.0000000000000005e-08], loss:2.271028995513916\n",
      "Epoch: 99, batch index: 1599, learning rate: [1.0000000000000005e-08], loss:2.2542829513549805\n",
      "Epoch 99, validation accuracy score0.337890625\n",
      "Epoch: 100, batch index: 1600, learning rate: [1.0000000000000005e-08], loss:2.253849983215332\n",
      "Epoch: 100, batch index: 1601, learning rate: [1.0000000000000005e-08], loss:2.250471830368042\n",
      "Epoch: 100, batch index: 1602, learning rate: [1.0000000000000005e-08], loss:2.3042049407958984\n",
      "Epoch: 100, batch index: 1603, learning rate: [1.0000000000000005e-08], loss:2.2596874237060547\n",
      "Epoch: 100, batch index: 1604, learning rate: [1.0000000000000005e-08], loss:2.284281015396118\n",
      "Epoch: 100, batch index: 1605, learning rate: [1.0000000000000005e-08], loss:2.2949676513671875\n",
      "Epoch: 100, batch index: 1606, learning rate: [1.0000000000000005e-08], loss:2.2714123725891113\n",
      "Epoch: 100, batch index: 1607, learning rate: [1.0000000000000005e-08], loss:2.2794506549835205\n",
      "Epoch: 100, batch index: 1608, learning rate: [1.0000000000000005e-08], loss:2.2752344608306885\n",
      "Epoch: 100, batch index: 1609, learning rate: [1.0000000000000005e-08], loss:2.2697489261627197\n",
      "Epoch: 100, batch index: 1610, learning rate: [1.0000000000000005e-08], loss:2.2929837703704834\n",
      "Epoch: 100, batch index: 1611, learning rate: [1.0000000000000005e-08], loss:2.237185001373291\n",
      "Epoch: 100, batch index: 1612, learning rate: [1.0000000000000005e-08], loss:2.3008179664611816\n",
      "Epoch: 100, batch index: 1613, learning rate: [1.0000000000000005e-08], loss:2.2718987464904785\n",
      "Epoch: 100, batch index: 1614, learning rate: [1.0000000000000005e-08], loss:2.2640461921691895\n",
      "Epoch: 100, batch index: 1615, learning rate: [1.0000000000000005e-08], loss:2.2851040363311768\n",
      "Epoch 100, validation accuracy score0.296875\n",
      "0\n",
      "Epoch: 101, batch index: 1616, learning rate: [1.0000000000000005e-08], loss:2.2890982627868652\n",
      "Epoch: 101, batch index: 1617, learning rate: [1.0000000000000005e-08], loss:2.278320074081421\n",
      "Epoch: 101, batch index: 1618, learning rate: [1.0000000000000005e-08], loss:2.280616521835327\n",
      "Epoch: 101, batch index: 1619, learning rate: [1.0000000000000005e-08], loss:2.2494449615478516\n",
      "Epoch: 101, batch index: 1620, learning rate: [1.0000000000000005e-08], loss:2.228959083557129\n",
      "Epoch: 101, batch index: 1621, learning rate: [1.0000000000000005e-08], loss:2.249340295791626\n",
      "Epoch: 101, batch index: 1622, learning rate: [1.0000000000000005e-08], loss:2.2691328525543213\n",
      "Epoch: 101, batch index: 1623, learning rate: [1.0000000000000005e-08], loss:2.25541353225708\n",
      "Epoch: 101, batch index: 1624, learning rate: [1.0000000000000005e-08], loss:2.2795584201812744\n",
      "Epoch: 101, batch index: 1625, learning rate: [1.0000000000000005e-08], loss:2.2163352966308594\n",
      "Epoch: 101, batch index: 1626, learning rate: [1.0000000000000005e-08], loss:2.289590835571289\n",
      "Epoch: 101, batch index: 1627, learning rate: [1.0000000000000005e-08], loss:2.2930822372436523\n",
      "Epoch: 101, batch index: 1628, learning rate: [1.0000000000000005e-08], loss:2.2568247318267822\n",
      "Epoch: 101, batch index: 1629, learning rate: [1.0000000000000005e-08], loss:2.3108999729156494\n",
      "Epoch: 101, batch index: 1630, learning rate: [1.0000000000000005e-08], loss:2.26668381690979\n",
      "Epoch: 101, batch index: 1631, learning rate: [1.0000000000000005e-08], loss:2.247720956802368\n",
      "Epoch 101, validation accuracy score0.353515625\n",
      "Epoch: 102, batch index: 1632, learning rate: [1.0000000000000005e-08], loss:2.2474586963653564\n",
      "Epoch: 102, batch index: 1633, learning rate: [1.0000000000000005e-08], loss:2.25947642326355\n",
      "Epoch: 102, batch index: 1634, learning rate: [1.0000000000000005e-08], loss:2.2535409927368164\n",
      "Epoch: 102, batch index: 1635, learning rate: [1.0000000000000005e-08], loss:2.2554543018341064\n",
      "Epoch: 102, batch index: 1636, learning rate: [1.0000000000000005e-08], loss:2.2823362350463867\n",
      "Epoch: 102, batch index: 1637, learning rate: [1.0000000000000005e-08], loss:2.289344549179077\n",
      "Epoch: 102, batch index: 1638, learning rate: [1.0000000000000005e-08], loss:2.2646992206573486\n",
      "Epoch: 102, batch index: 1639, learning rate: [1.0000000000000005e-08], loss:2.254918098449707\n",
      "Epoch: 102, batch index: 1640, learning rate: [1.0000000000000005e-08], loss:2.2595436573028564\n",
      "Epoch: 102, batch index: 1641, learning rate: [1.0000000000000005e-08], loss:2.251371383666992\n",
      "Epoch: 102, batch index: 1642, learning rate: [1.0000000000000005e-08], loss:2.2799696922302246\n",
      "Epoch: 102, batch index: 1643, learning rate: [1.0000000000000005e-08], loss:2.2745134830474854\n",
      "Epoch: 102, batch index: 1644, learning rate: [1.0000000000000005e-08], loss:2.2722418308258057\n",
      "Epoch: 102, batch index: 1645, learning rate: [1.0000000000000005e-08], loss:2.2617902755737305\n",
      "Epoch: 102, batch index: 1646, learning rate: [1.0000000000000005e-08], loss:2.2821741104125977\n",
      "Epoch: 102, batch index: 1647, learning rate: [1.0000000000000005e-08], loss:2.282409191131592\n",
      "Epoch 102, validation accuracy score0.328125\n",
      "Epoch: 103, batch index: 1648, learning rate: [1.0000000000000005e-08], loss:2.2527236938476562\n",
      "Epoch: 103, batch index: 1649, learning rate: [1.0000000000000005e-08], loss:2.288196086883545\n",
      "Epoch: 103, batch index: 1650, learning rate: [1.0000000000000005e-08], loss:2.256258010864258\n",
      "Epoch: 103, batch index: 1651, learning rate: [1.0000000000000005e-08], loss:2.2883591651916504\n",
      "Epoch: 103, batch index: 1652, learning rate: [1.0000000000000005e-08], loss:2.2997500896453857\n",
      "Epoch: 103, batch index: 1653, learning rate: [1.0000000000000005e-08], loss:2.284250020980835\n",
      "Epoch: 103, batch index: 1654, learning rate: [1.0000000000000005e-08], loss:2.259350299835205\n",
      "Epoch: 103, batch index: 1655, learning rate: [1.0000000000000005e-08], loss:2.256488561630249\n",
      "Epoch: 103, batch index: 1656, learning rate: [1.0000000000000005e-08], loss:2.2552647590637207\n",
      "Epoch: 103, batch index: 1657, learning rate: [1.0000000000000005e-08], loss:2.233292579650879\n",
      "Epoch: 103, batch index: 1658, learning rate: [1.0000000000000005e-08], loss:2.2621941566467285\n",
      "Epoch: 103, batch index: 1659, learning rate: [1.0000000000000005e-08], loss:2.271077871322632\n",
      "Epoch: 103, batch index: 1660, learning rate: [1.0000000000000005e-08], loss:2.2702953815460205\n",
      "Epoch: 103, batch index: 1661, learning rate: [1.0000000000000005e-08], loss:2.3018851280212402\n",
      "Epoch: 103, batch index: 1662, learning rate: [1.0000000000000005e-08], loss:2.2655138969421387\n",
      "Epoch: 103, batch index: 1663, learning rate: [1.0000000000000005e-08], loss:2.2404487133026123\n",
      "Epoch 103, validation accuracy score0.3359375\n",
      "Epoch: 104, batch index: 1664, learning rate: [1.0000000000000005e-08], loss:2.2551159858703613\n",
      "Epoch: 104, batch index: 1665, learning rate: [1.0000000000000005e-08], loss:2.3043460845947266\n",
      "Epoch: 104, batch index: 1666, learning rate: [1.0000000000000005e-08], loss:2.2982864379882812\n",
      "Epoch: 104, batch index: 1667, learning rate: [1.0000000000000005e-08], loss:2.2914693355560303\n",
      "Epoch: 104, batch index: 1668, learning rate: [1.0000000000000005e-08], loss:2.3274993896484375\n",
      "Epoch: 104, batch index: 1669, learning rate: [1.0000000000000005e-08], loss:2.291691780090332\n",
      "Epoch: 104, batch index: 1670, learning rate: [1.0000000000000005e-08], loss:2.271345615386963\n",
      "Epoch: 104, batch index: 1671, learning rate: [1.0000000000000005e-08], loss:2.230396032333374\n",
      "Epoch: 104, batch index: 1672, learning rate: [1.0000000000000005e-08], loss:2.279859781265259\n",
      "Epoch: 104, batch index: 1673, learning rate: [1.0000000000000005e-08], loss:2.246718168258667\n",
      "Epoch: 104, batch index: 1674, learning rate: [1.0000000000000005e-08], loss:2.236565589904785\n",
      "Epoch: 104, batch index: 1675, learning rate: [1.0000000000000005e-08], loss:2.2768020629882812\n",
      "Epoch: 104, batch index: 1676, learning rate: [1.0000000000000005e-08], loss:2.2582740783691406\n",
      "Epoch: 104, batch index: 1677, learning rate: [1.0000000000000005e-08], loss:2.256500244140625\n",
      "Epoch: 104, batch index: 1678, learning rate: [1.0000000000000005e-08], loss:2.3004844188690186\n",
      "Epoch: 104, batch index: 1679, learning rate: [1.0000000000000005e-08], loss:2.230590581893921\n",
      "Epoch 104, validation accuracy score0.318359375\n",
      "Epoch: 105, batch index: 1680, learning rate: [1.0000000000000005e-08], loss:2.2880990505218506\n",
      "Epoch: 105, batch index: 1681, learning rate: [1.0000000000000005e-08], loss:2.294928789138794\n",
      "Epoch: 105, batch index: 1682, learning rate: [1.0000000000000005e-08], loss:2.2751271724700928\n",
      "Epoch: 105, batch index: 1683, learning rate: [1.0000000000000005e-08], loss:2.274752140045166\n",
      "Epoch: 105, batch index: 1684, learning rate: [1.0000000000000005e-08], loss:2.2912750244140625\n",
      "Epoch: 105, batch index: 1685, learning rate: [1.0000000000000005e-08], loss:2.2586658000946045\n",
      "Epoch: 105, batch index: 1686, learning rate: [1.0000000000000005e-08], loss:2.2323074340820312\n",
      "Epoch: 105, batch index: 1687, learning rate: [1.0000000000000005e-08], loss:2.283738136291504\n",
      "Epoch: 105, batch index: 1688, learning rate: [1.0000000000000005e-08], loss:2.285144567489624\n",
      "Epoch: 105, batch index: 1689, learning rate: [1.0000000000000005e-08], loss:2.2628931999206543\n",
      "Epoch: 105, batch index: 1690, learning rate: [1.0000000000000005e-08], loss:2.28781795501709\n",
      "Epoch: 105, batch index: 1691, learning rate: [1.0000000000000005e-08], loss:2.282862663269043\n",
      "Epoch: 105, batch index: 1692, learning rate: [1.0000000000000005e-08], loss:2.230851650238037\n",
      "Epoch: 105, batch index: 1693, learning rate: [1.0000000000000005e-08], loss:2.262908935546875\n",
      "Epoch: 105, batch index: 1694, learning rate: [1.0000000000000005e-08], loss:2.2749075889587402\n",
      "Epoch: 105, batch index: 1695, learning rate: [1.0000000000000005e-08], loss:2.220080614089966\n",
      "Epoch 105, validation accuracy score0.298828125\n",
      "Epoch: 106, batch index: 1696, learning rate: [1.0000000000000005e-08], loss:2.235621213912964\n",
      "Epoch: 106, batch index: 1697, learning rate: [1.0000000000000005e-08], loss:2.2616584300994873\n",
      "Epoch: 106, batch index: 1698, learning rate: [1.0000000000000005e-08], loss:2.2671754360198975\n",
      "Epoch: 106, batch index: 1699, learning rate: [1.0000000000000005e-08], loss:2.25620698928833\n",
      "Epoch: 106, batch index: 1700, learning rate: [1.0000000000000005e-08], loss:2.261232852935791\n",
      "Epoch: 106, batch index: 1701, learning rate: [1.0000000000000005e-08], loss:2.272286891937256\n",
      "Epoch: 106, batch index: 1702, learning rate: [1.0000000000000005e-08], loss:2.2599728107452393\n",
      "Epoch: 106, batch index: 1703, learning rate: [1.0000000000000005e-08], loss:2.2868831157684326\n",
      "Epoch: 106, batch index: 1704, learning rate: [1.0000000000000005e-08], loss:2.30856990814209\n",
      "Epoch: 106, batch index: 1705, learning rate: [1.0000000000000005e-08], loss:2.2723164558410645\n",
      "Epoch: 106, batch index: 1706, learning rate: [1.0000000000000005e-08], loss:2.2574350833892822\n",
      "Epoch: 106, batch index: 1707, learning rate: [1.0000000000000005e-08], loss:2.237499237060547\n",
      "Epoch: 106, batch index: 1708, learning rate: [1.0000000000000005e-08], loss:2.2538461685180664\n",
      "Epoch: 106, batch index: 1709, learning rate: [1.0000000000000005e-08], loss:2.2572455406188965\n",
      "Epoch: 106, batch index: 1710, learning rate: [1.0000000000000005e-08], loss:2.263772964477539\n",
      "Epoch: 106, batch index: 1711, learning rate: [1.0000000000000005e-08], loss:2.3112521171569824\n",
      "Epoch 106, validation accuracy score0.306640625\n",
      "Epoch: 107, batch index: 1712, learning rate: [1.0000000000000005e-08], loss:2.211836576461792\n",
      "Epoch: 107, batch index: 1713, learning rate: [1.0000000000000005e-08], loss:2.2767255306243896\n",
      "Epoch: 107, batch index: 1714, learning rate: [1.0000000000000005e-08], loss:2.2873759269714355\n",
      "Epoch: 107, batch index: 1715, learning rate: [1.0000000000000005e-08], loss:2.249152898788452\n",
      "Epoch: 107, batch index: 1716, learning rate: [1.0000000000000005e-08], loss:2.248488426208496\n",
      "Epoch: 107, batch index: 1717, learning rate: [1.0000000000000005e-08], loss:2.257568120956421\n",
      "Epoch: 107, batch index: 1718, learning rate: [1.0000000000000005e-08], loss:2.2693493366241455\n",
      "Epoch: 107, batch index: 1719, learning rate: [1.0000000000000005e-08], loss:2.2816452980041504\n",
      "Epoch: 107, batch index: 1720, learning rate: [1.0000000000000005e-08], loss:2.2541534900665283\n",
      "Epoch: 107, batch index: 1721, learning rate: [1.0000000000000005e-08], loss:2.2454073429107666\n",
      "Epoch: 107, batch index: 1722, learning rate: [1.0000000000000005e-08], loss:2.282609462738037\n",
      "Epoch: 107, batch index: 1723, learning rate: [1.0000000000000005e-08], loss:2.296145439147949\n",
      "Epoch: 107, batch index: 1724, learning rate: [1.0000000000000005e-08], loss:2.2809841632843018\n",
      "Epoch: 107, batch index: 1725, learning rate: [1.0000000000000005e-08], loss:2.2368338108062744\n",
      "Epoch: 107, batch index: 1726, learning rate: [1.0000000000000005e-08], loss:2.279578447341919\n",
      "Epoch: 107, batch index: 1727, learning rate: [1.0000000000000005e-08], loss:2.276292324066162\n",
      "Epoch 107, validation accuracy score0.345703125\n",
      "Epoch: 108, batch index: 1728, learning rate: [1.0000000000000005e-08], loss:2.2458367347717285\n",
      "Epoch: 108, batch index: 1729, learning rate: [1.0000000000000005e-08], loss:2.2837486267089844\n",
      "Epoch: 108, batch index: 1730, learning rate: [1.0000000000000005e-08], loss:2.2747089862823486\n",
      "Epoch: 108, batch index: 1731, learning rate: [1.0000000000000005e-08], loss:2.290041446685791\n",
      "Epoch: 108, batch index: 1732, learning rate: [1.0000000000000005e-08], loss:2.2581396102905273\n",
      "Epoch: 108, batch index: 1733, learning rate: [1.0000000000000005e-08], loss:2.2135019302368164\n",
      "Epoch: 108, batch index: 1734, learning rate: [1.0000000000000005e-08], loss:2.249668598175049\n",
      "Epoch: 108, batch index: 1735, learning rate: [1.0000000000000005e-08], loss:2.292679786682129\n",
      "Epoch: 108, batch index: 1736, learning rate: [1.0000000000000005e-08], loss:2.277865409851074\n",
      "Epoch: 108, batch index: 1737, learning rate: [1.0000000000000005e-08], loss:2.2745625972747803\n",
      "Epoch: 108, batch index: 1738, learning rate: [1.0000000000000005e-08], loss:2.279050588607788\n",
      "Epoch: 108, batch index: 1739, learning rate: [1.0000000000000005e-08], loss:2.3231444358825684\n",
      "Epoch: 108, batch index: 1740, learning rate: [1.0000000000000005e-08], loss:2.2654311656951904\n",
      "Epoch: 108, batch index: 1741, learning rate: [1.0000000000000005e-08], loss:2.2567455768585205\n",
      "Epoch: 108, batch index: 1742, learning rate: [1.0000000000000005e-08], loss:2.2783925533294678\n",
      "Epoch: 108, batch index: 1743, learning rate: [1.0000000000000005e-08], loss:2.2797374725341797\n",
      "Epoch 108, validation accuracy score0.3203125\n",
      "Epoch: 109, batch index: 1744, learning rate: [1.0000000000000005e-08], loss:2.2619807720184326\n",
      "Epoch: 109, batch index: 1745, learning rate: [1.0000000000000005e-08], loss:2.2766828536987305\n",
      "Epoch: 109, batch index: 1746, learning rate: [1.0000000000000005e-08], loss:2.2397618293762207\n",
      "Epoch: 109, batch index: 1747, learning rate: [1.0000000000000005e-08], loss:2.263117551803589\n",
      "Epoch: 109, batch index: 1748, learning rate: [1.0000000000000005e-08], loss:2.2487404346466064\n",
      "Epoch: 109, batch index: 1749, learning rate: [1.0000000000000005e-08], loss:2.2544920444488525\n",
      "Epoch: 109, batch index: 1750, learning rate: [1.0000000000000005e-08], loss:2.2665934562683105\n",
      "Epoch: 109, batch index: 1751, learning rate: [1.0000000000000005e-08], loss:2.290109395980835\n",
      "Epoch: 109, batch index: 1752, learning rate: [1.0000000000000005e-08], loss:2.27760648727417\n",
      "Epoch: 109, batch index: 1753, learning rate: [1.0000000000000005e-08], loss:2.2529001235961914\n",
      "Epoch: 109, batch index: 1754, learning rate: [1.0000000000000005e-08], loss:2.2669053077697754\n",
      "Epoch: 109, batch index: 1755, learning rate: [1.0000000000000005e-08], loss:2.263051748275757\n",
      "Epoch: 109, batch index: 1756, learning rate: [1.0000000000000005e-08], loss:2.307093620300293\n",
      "Epoch: 109, batch index: 1757, learning rate: [1.0000000000000005e-08], loss:2.2697842121124268\n",
      "Epoch: 109, batch index: 1758, learning rate: [1.0000000000000005e-08], loss:2.2579548358917236\n",
      "Epoch: 109, batch index: 1759, learning rate: [1.0000000000000005e-08], loss:2.2628419399261475\n",
      "Epoch 109, validation accuracy score0.3046875\n",
      "Epoch: 110, batch index: 1760, learning rate: [1.0000000000000005e-08], loss:2.2774484157562256\n",
      "Epoch: 110, batch index: 1761, learning rate: [1.0000000000000005e-08], loss:2.2668752670288086\n",
      "Epoch: 110, batch index: 1762, learning rate: [1.0000000000000005e-08], loss:2.2802650928497314\n",
      "Epoch: 110, batch index: 1763, learning rate: [1.0000000000000005e-08], loss:2.2663722038269043\n",
      "Epoch: 110, batch index: 1764, learning rate: [1.0000000000000005e-08], loss:2.2892112731933594\n",
      "Epoch: 110, batch index: 1765, learning rate: [1.0000000000000005e-08], loss:2.291818141937256\n",
      "Epoch: 110, batch index: 1766, learning rate: [1.0000000000000005e-08], loss:2.2666313648223877\n",
      "Epoch: 110, batch index: 1767, learning rate: [1.0000000000000005e-08], loss:2.2457778453826904\n",
      "Epoch: 110, batch index: 1768, learning rate: [1.0000000000000005e-08], loss:2.2612216472625732\n",
      "Epoch: 110, batch index: 1769, learning rate: [1.0000000000000005e-08], loss:2.2472927570343018\n",
      "Epoch: 110, batch index: 1770, learning rate: [1.0000000000000005e-08], loss:2.294583320617676\n",
      "Epoch: 110, batch index: 1771, learning rate: [1.0000000000000005e-08], loss:2.2391908168792725\n",
      "Epoch: 110, batch index: 1772, learning rate: [1.0000000000000005e-08], loss:2.223752498626709\n",
      "Epoch: 110, batch index: 1773, learning rate: [1.0000000000000005e-08], loss:2.280966281890869\n",
      "Epoch: 110, batch index: 1774, learning rate: [1.0000000000000005e-08], loss:2.271354913711548\n",
      "Epoch: 110, batch index: 1775, learning rate: [1.0000000000000005e-08], loss:2.261509656906128\n",
      "Epoch 110, validation accuracy score0.310546875\n",
      "Epoch: 111, batch index: 1776, learning rate: [1.0000000000000005e-08], loss:2.2478835582733154\n",
      "Epoch: 111, batch index: 1777, learning rate: [1.0000000000000005e-08], loss:2.245643138885498\n",
      "Epoch: 111, batch index: 1778, learning rate: [1.0000000000000005e-08], loss:2.2741241455078125\n",
      "Epoch: 111, batch index: 1779, learning rate: [1.0000000000000005e-08], loss:2.288658857345581\n",
      "Epoch: 111, batch index: 1780, learning rate: [1.0000000000000005e-08], loss:2.292940855026245\n",
      "Epoch: 111, batch index: 1781, learning rate: [1.0000000000000005e-08], loss:2.2798075675964355\n",
      "Epoch: 111, batch index: 1782, learning rate: [1.0000000000000005e-08], loss:2.2476108074188232\n",
      "Epoch: 111, batch index: 1783, learning rate: [1.0000000000000005e-08], loss:2.286526918411255\n",
      "Epoch: 111, batch index: 1784, learning rate: [1.0000000000000005e-08], loss:2.253544807434082\n",
      "Epoch: 111, batch index: 1785, learning rate: [1.0000000000000005e-08], loss:2.2451374530792236\n",
      "Epoch: 111, batch index: 1786, learning rate: [1.0000000000000005e-08], loss:2.2664597034454346\n",
      "Epoch: 111, batch index: 1787, learning rate: [1.0000000000000005e-08], loss:2.270852565765381\n",
      "Epoch: 111, batch index: 1788, learning rate: [1.0000000000000005e-08], loss:2.2728281021118164\n",
      "Epoch: 111, batch index: 1789, learning rate: [1.0000000000000005e-08], loss:2.2703049182891846\n",
      "Epoch: 111, batch index: 1790, learning rate: [1.0000000000000005e-08], loss:2.2620084285736084\n",
      "Epoch: 111, batch index: 1791, learning rate: [1.0000000000000005e-08], loss:2.283749580383301\n",
      "Epoch 111, validation accuracy score0.330078125\n",
      "Epoch: 112, batch index: 1792, learning rate: [1.0000000000000005e-08], loss:2.2483835220336914\n",
      "Epoch: 112, batch index: 1793, learning rate: [1.0000000000000005e-08], loss:2.2855913639068604\n",
      "Epoch: 112, batch index: 1794, learning rate: [1.0000000000000005e-08], loss:2.300631523132324\n",
      "Epoch: 112, batch index: 1795, learning rate: [1.0000000000000005e-08], loss:2.2429213523864746\n",
      "Epoch: 112, batch index: 1796, learning rate: [1.0000000000000005e-08], loss:2.2458534240722656\n",
      "Epoch: 112, batch index: 1797, learning rate: [1.0000000000000005e-08], loss:2.2777485847473145\n",
      "Epoch: 112, batch index: 1798, learning rate: [1.0000000000000005e-08], loss:2.263838768005371\n",
      "Epoch: 112, batch index: 1799, learning rate: [1.0000000000000005e-08], loss:2.2750513553619385\n",
      "Epoch: 112, batch index: 1800, learning rate: [1.0000000000000005e-08], loss:2.2599711418151855\n",
      "Epoch: 112, batch index: 1801, learning rate: [1.0000000000000005e-08], loss:2.3025283813476562\n",
      "Epoch: 112, batch index: 1802, learning rate: [1.0000000000000005e-08], loss:2.253333568572998\n",
      "Epoch: 112, batch index: 1803, learning rate: [1.0000000000000005e-08], loss:2.275162935256958\n",
      "Epoch: 112, batch index: 1804, learning rate: [1.0000000000000005e-08], loss:2.2964956760406494\n",
      "Epoch: 112, batch index: 1805, learning rate: [1.0000000000000005e-08], loss:2.2637977600097656\n",
      "Epoch: 112, batch index: 1806, learning rate: [1.0000000000000005e-08], loss:2.243330955505371\n",
      "Epoch: 112, batch index: 1807, learning rate: [1.0000000000000005e-08], loss:2.271528482437134\n",
      "Epoch 112, validation accuracy score0.345703125\n",
      "Epoch: 113, batch index: 1808, learning rate: [1.0000000000000005e-08], loss:2.286309242248535\n",
      "Epoch: 113, batch index: 1809, learning rate: [1.0000000000000005e-08], loss:2.2542507648468018\n",
      "Epoch: 113, batch index: 1810, learning rate: [1.0000000000000005e-08], loss:2.295808792114258\n",
      "Epoch: 113, batch index: 1811, learning rate: [1.0000000000000005e-08], loss:2.2895917892456055\n",
      "Epoch: 113, batch index: 1812, learning rate: [1.0000000000000005e-08], loss:2.237184762954712\n",
      "Epoch: 113, batch index: 1813, learning rate: [1.0000000000000005e-08], loss:2.284903049468994\n",
      "Epoch: 113, batch index: 1814, learning rate: [1.0000000000000005e-08], loss:2.246628522872925\n",
      "Epoch: 113, batch index: 1815, learning rate: [1.0000000000000005e-08], loss:2.26645565032959\n",
      "Epoch: 113, batch index: 1816, learning rate: [1.0000000000000005e-08], loss:2.246800184249878\n",
      "Epoch: 113, batch index: 1817, learning rate: [1.0000000000000005e-08], loss:2.286482810974121\n",
      "Epoch: 113, batch index: 1818, learning rate: [1.0000000000000005e-08], loss:2.263000965118408\n",
      "Epoch: 113, batch index: 1819, learning rate: [1.0000000000000005e-08], loss:2.275447368621826\n",
      "Epoch: 113, batch index: 1820, learning rate: [1.0000000000000005e-08], loss:2.269216775894165\n",
      "Epoch: 113, batch index: 1821, learning rate: [1.0000000000000005e-08], loss:2.2864012718200684\n",
      "Epoch: 113, batch index: 1822, learning rate: [1.0000000000000005e-08], loss:2.2597198486328125\n",
      "Epoch: 113, batch index: 1823, learning rate: [1.0000000000000005e-08], loss:2.2889251708984375\n",
      "Epoch 113, validation accuracy score0.33984375\n",
      "Epoch: 114, batch index: 1824, learning rate: [1.0000000000000005e-08], loss:2.2336907386779785\n",
      "Epoch: 114, batch index: 1825, learning rate: [1.0000000000000005e-08], loss:2.287536859512329\n",
      "Epoch: 114, batch index: 1826, learning rate: [1.0000000000000005e-08], loss:2.2677195072174072\n",
      "Epoch: 114, batch index: 1827, learning rate: [1.0000000000000005e-08], loss:2.2367160320281982\n",
      "Epoch: 114, batch index: 1828, learning rate: [1.0000000000000005e-08], loss:2.291818380355835\n",
      "Epoch: 114, batch index: 1829, learning rate: [1.0000000000000005e-08], loss:2.286160945892334\n",
      "Epoch: 114, batch index: 1830, learning rate: [1.0000000000000005e-08], loss:2.2557289600372314\n",
      "Epoch: 114, batch index: 1831, learning rate: [1.0000000000000005e-08], loss:2.2580041885375977\n",
      "Epoch: 114, batch index: 1832, learning rate: [1.0000000000000005e-08], loss:2.2714948654174805\n",
      "Epoch: 114, batch index: 1833, learning rate: [1.0000000000000005e-08], loss:2.2881548404693604\n",
      "Epoch: 114, batch index: 1834, learning rate: [1.0000000000000005e-08], loss:2.3066205978393555\n",
      "Epoch: 114, batch index: 1835, learning rate: [1.0000000000000005e-08], loss:2.260359048843384\n",
      "Epoch: 114, batch index: 1836, learning rate: [1.0000000000000005e-08], loss:2.2896385192871094\n",
      "Epoch: 114, batch index: 1837, learning rate: [1.0000000000000005e-08], loss:2.289952278137207\n",
      "Epoch: 114, batch index: 1838, learning rate: [1.0000000000000005e-08], loss:2.280998945236206\n",
      "Epoch: 114, batch index: 1839, learning rate: [1.0000000000000005e-08], loss:2.276733875274658\n",
      "Epoch 114, validation accuracy score0.328125\n",
      "Epoch: 115, batch index: 1840, learning rate: [1.0000000000000005e-08], loss:2.3185086250305176\n",
      "Epoch: 115, batch index: 1841, learning rate: [1.0000000000000005e-08], loss:2.26446795463562\n",
      "Epoch: 115, batch index: 1842, learning rate: [1.0000000000000005e-08], loss:2.290663242340088\n",
      "Epoch: 115, batch index: 1843, learning rate: [1.0000000000000005e-08], loss:2.273230791091919\n",
      "Epoch: 115, batch index: 1844, learning rate: [1.0000000000000005e-08], loss:2.2570886611938477\n",
      "Epoch: 115, batch index: 1845, learning rate: [1.0000000000000005e-08], loss:2.252984046936035\n",
      "Epoch: 115, batch index: 1846, learning rate: [1.0000000000000005e-08], loss:2.2822864055633545\n",
      "Epoch: 115, batch index: 1847, learning rate: [1.0000000000000005e-08], loss:2.2606759071350098\n",
      "Epoch: 115, batch index: 1848, learning rate: [1.0000000000000005e-08], loss:2.2942469120025635\n",
      "Epoch: 115, batch index: 1849, learning rate: [1.0000000000000005e-08], loss:2.2802462577819824\n",
      "Epoch: 115, batch index: 1850, learning rate: [1.0000000000000005e-08], loss:2.228773593902588\n",
      "Epoch: 115, batch index: 1851, learning rate: [1.0000000000000005e-08], loss:2.2089574337005615\n",
      "Epoch: 115, batch index: 1852, learning rate: [1.0000000000000005e-08], loss:2.2739346027374268\n",
      "Epoch: 115, batch index: 1853, learning rate: [1.0000000000000005e-08], loss:2.2617428302764893\n",
      "Epoch: 115, batch index: 1854, learning rate: [1.0000000000000005e-08], loss:2.269773244857788\n",
      "Epoch: 115, batch index: 1855, learning rate: [1.0000000000000005e-08], loss:2.250467538833618\n",
      "Epoch 115, validation accuracy score0.302734375\n",
      "Epoch: 116, batch index: 1856, learning rate: [1.0000000000000005e-08], loss:2.250657081604004\n",
      "Epoch: 116, batch index: 1857, learning rate: [1.0000000000000005e-08], loss:2.262233018875122\n",
      "Epoch: 116, batch index: 1858, learning rate: [1.0000000000000005e-08], loss:2.2930500507354736\n",
      "Epoch: 116, batch index: 1859, learning rate: [1.0000000000000005e-08], loss:2.250155448913574\n",
      "Epoch: 116, batch index: 1860, learning rate: [1.0000000000000005e-08], loss:2.245144844055176\n",
      "Epoch: 116, batch index: 1861, learning rate: [1.0000000000000005e-08], loss:2.2710089683532715\n",
      "Epoch: 116, batch index: 1862, learning rate: [1.0000000000000005e-08], loss:2.278256416320801\n",
      "Epoch: 116, batch index: 1863, learning rate: [1.0000000000000005e-08], loss:2.274768352508545\n",
      "Epoch: 116, batch index: 1864, learning rate: [1.0000000000000005e-08], loss:2.2806689739227295\n",
      "Epoch: 116, batch index: 1865, learning rate: [1.0000000000000005e-08], loss:2.252707004547119\n",
      "Epoch: 116, batch index: 1866, learning rate: [1.0000000000000005e-08], loss:2.2991158962249756\n",
      "Epoch: 116, batch index: 1867, learning rate: [1.0000000000000005e-08], loss:2.2727181911468506\n",
      "Epoch: 116, batch index: 1868, learning rate: [1.0000000000000005e-08], loss:2.353022336959839\n",
      "Epoch: 116, batch index: 1869, learning rate: [1.0000000000000005e-08], loss:2.245448589324951\n",
      "Epoch: 116, batch index: 1870, learning rate: [1.0000000000000005e-08], loss:2.2430734634399414\n",
      "Epoch: 116, batch index: 1871, learning rate: [1.0000000000000005e-08], loss:2.2897701263427734\n",
      "Epoch 116, validation accuracy score0.330078125\n",
      "Epoch: 117, batch index: 1872, learning rate: [1.0000000000000005e-08], loss:2.2756781578063965\n",
      "Epoch: 117, batch index: 1873, learning rate: [1.0000000000000005e-08], loss:2.255889892578125\n",
      "Epoch: 117, batch index: 1874, learning rate: [1.0000000000000005e-08], loss:2.2543976306915283\n",
      "Epoch: 117, batch index: 1875, learning rate: [1.0000000000000005e-08], loss:2.2896320819854736\n",
      "Epoch: 117, batch index: 1876, learning rate: [1.0000000000000005e-08], loss:2.2927730083465576\n",
      "Epoch: 117, batch index: 1877, learning rate: [1.0000000000000005e-08], loss:2.2694904804229736\n",
      "Epoch: 117, batch index: 1878, learning rate: [1.0000000000000005e-08], loss:2.2539892196655273\n",
      "Epoch: 117, batch index: 1879, learning rate: [1.0000000000000005e-08], loss:2.273198366165161\n",
      "Epoch: 117, batch index: 1880, learning rate: [1.0000000000000005e-08], loss:2.2579686641693115\n",
      "Epoch: 117, batch index: 1881, learning rate: [1.0000000000000005e-08], loss:2.3115973472595215\n",
      "Epoch: 117, batch index: 1882, learning rate: [1.0000000000000005e-08], loss:2.2293612957000732\n",
      "Epoch: 117, batch index: 1883, learning rate: [1.0000000000000005e-08], loss:2.2418341636657715\n",
      "Epoch: 117, batch index: 1884, learning rate: [1.0000000000000005e-08], loss:2.270411968231201\n",
      "Epoch: 117, batch index: 1885, learning rate: [1.0000000000000005e-08], loss:2.259058713912964\n",
      "Epoch: 117, batch index: 1886, learning rate: [1.0000000000000005e-08], loss:2.270110607147217\n",
      "Epoch: 117, batch index: 1887, learning rate: [1.0000000000000005e-08], loss:2.2710535526275635\n",
      "Epoch 117, validation accuracy score0.318359375\n",
      "Epoch: 118, batch index: 1888, learning rate: [1.0000000000000005e-08], loss:2.263317823410034\n",
      "Epoch: 118, batch index: 1889, learning rate: [1.0000000000000005e-08], loss:2.2460122108459473\n",
      "Epoch: 118, batch index: 1890, learning rate: [1.0000000000000005e-08], loss:2.2607569694519043\n",
      "Epoch: 118, batch index: 1891, learning rate: [1.0000000000000005e-08], loss:2.273700714111328\n",
      "Epoch: 118, batch index: 1892, learning rate: [1.0000000000000005e-08], loss:2.2603063583374023\n",
      "Epoch: 118, batch index: 1893, learning rate: [1.0000000000000005e-08], loss:2.2261338233947754\n",
      "Epoch: 118, batch index: 1894, learning rate: [1.0000000000000005e-08], loss:2.2635750770568848\n",
      "Epoch: 118, batch index: 1895, learning rate: [1.0000000000000005e-08], loss:2.292412519454956\n",
      "Epoch: 118, batch index: 1896, learning rate: [1.0000000000000005e-08], loss:2.255580186843872\n",
      "Epoch: 118, batch index: 1897, learning rate: [1.0000000000000005e-08], loss:2.293398857116699\n",
      "Epoch: 118, batch index: 1898, learning rate: [1.0000000000000005e-08], loss:2.2719802856445312\n",
      "Epoch: 118, batch index: 1899, learning rate: [1.0000000000000005e-08], loss:2.2465732097625732\n",
      "Epoch: 118, batch index: 1900, learning rate: [1.0000000000000005e-08], loss:2.3159592151641846\n",
      "Epoch: 118, batch index: 1901, learning rate: [1.0000000000000005e-08], loss:2.3010802268981934\n",
      "Epoch: 118, batch index: 1902, learning rate: [1.0000000000000005e-08], loss:2.3010094165802\n",
      "Epoch: 118, batch index: 1903, learning rate: [1.0000000000000005e-08], loss:2.258244037628174\n",
      "Epoch 118, validation accuracy score0.361328125\n",
      "Epoch: 119, batch index: 1904, learning rate: [1.0000000000000005e-08], loss:2.268953561782837\n",
      "Epoch: 119, batch index: 1905, learning rate: [1.0000000000000005e-08], loss:2.279615640640259\n",
      "Epoch: 119, batch index: 1906, learning rate: [1.0000000000000005e-08], loss:2.2409684658050537\n",
      "Epoch: 119, batch index: 1907, learning rate: [1.0000000000000005e-08], loss:2.280303716659546\n",
      "Epoch: 119, batch index: 1908, learning rate: [1.0000000000000005e-08], loss:2.273341178894043\n",
      "Epoch: 119, batch index: 1909, learning rate: [1.0000000000000005e-08], loss:2.2775180339813232\n",
      "Epoch: 119, batch index: 1910, learning rate: [1.0000000000000005e-08], loss:2.255042552947998\n",
      "Epoch: 119, batch index: 1911, learning rate: [1.0000000000000005e-08], loss:2.2591989040374756\n",
      "Epoch: 119, batch index: 1912, learning rate: [1.0000000000000005e-08], loss:2.3097598552703857\n",
      "Epoch: 119, batch index: 1913, learning rate: [1.0000000000000005e-08], loss:2.2740824222564697\n",
      "Epoch: 119, batch index: 1914, learning rate: [1.0000000000000005e-08], loss:2.256103515625\n",
      "Epoch: 119, batch index: 1915, learning rate: [1.0000000000000005e-08], loss:2.281726837158203\n",
      "Epoch: 119, batch index: 1916, learning rate: [1.0000000000000005e-08], loss:2.290780544281006\n",
      "Epoch: 119, batch index: 1917, learning rate: [1.0000000000000005e-08], loss:2.246227979660034\n",
      "Epoch: 119, batch index: 1918, learning rate: [1.0000000000000005e-08], loss:2.273922920227051\n",
      "Epoch: 119, batch index: 1919, learning rate: [1.0000000000000005e-08], loss:2.2454679012298584\n",
      "Epoch 119, validation accuracy score0.330078125\n",
      "Epoch: 120, batch index: 1920, learning rate: [1.0000000000000005e-08], loss:2.240385055541992\n",
      "Epoch: 120, batch index: 1921, learning rate: [1.0000000000000005e-08], loss:2.293959140777588\n",
      "Epoch: 120, batch index: 1922, learning rate: [1.0000000000000005e-08], loss:2.267177104949951\n",
      "Epoch: 120, batch index: 1923, learning rate: [1.0000000000000005e-08], loss:2.293677568435669\n",
      "Epoch: 120, batch index: 1924, learning rate: [1.0000000000000005e-08], loss:2.24574613571167\n",
      "Epoch: 120, batch index: 1925, learning rate: [1.0000000000000005e-08], loss:2.298527479171753\n",
      "Epoch: 120, batch index: 1926, learning rate: [1.0000000000000005e-08], loss:2.2426140308380127\n",
      "Epoch: 120, batch index: 1927, learning rate: [1.0000000000000005e-08], loss:2.276048421859741\n",
      "Epoch: 120, batch index: 1928, learning rate: [1.0000000000000005e-08], loss:2.2910568714141846\n",
      "Epoch: 120, batch index: 1929, learning rate: [1.0000000000000005e-08], loss:2.2609050273895264\n",
      "Epoch: 120, batch index: 1930, learning rate: [1.0000000000000005e-08], loss:2.290895700454712\n",
      "Epoch: 120, batch index: 1931, learning rate: [1.0000000000000005e-08], loss:2.2441940307617188\n",
      "Epoch: 120, batch index: 1932, learning rate: [1.0000000000000005e-08], loss:2.281062602996826\n",
      "Epoch: 120, batch index: 1933, learning rate: [1.0000000000000005e-08], loss:2.239307403564453\n",
      "Epoch: 120, batch index: 1934, learning rate: [1.0000000000000005e-08], loss:2.272472381591797\n",
      "Epoch: 120, batch index: 1935, learning rate: [1.0000000000000005e-08], loss:2.240532875061035\n",
      "Epoch 120, validation accuracy score0.314453125\n",
      "Epoch: 121, batch index: 1936, learning rate: [1.0000000000000005e-08], loss:2.281569719314575\n",
      "Epoch: 121, batch index: 1937, learning rate: [1.0000000000000005e-08], loss:2.256889820098877\n",
      "Epoch: 121, batch index: 1938, learning rate: [1.0000000000000005e-08], loss:2.2649388313293457\n",
      "Epoch: 121, batch index: 1939, learning rate: [1.0000000000000005e-08], loss:2.2857096195220947\n",
      "Epoch: 121, batch index: 1940, learning rate: [1.0000000000000005e-08], loss:2.2582178115844727\n",
      "Epoch: 121, batch index: 1941, learning rate: [1.0000000000000005e-08], loss:2.2792105674743652\n",
      "Epoch: 121, batch index: 1942, learning rate: [1.0000000000000005e-08], loss:2.2457642555236816\n",
      "Epoch: 121, batch index: 1943, learning rate: [1.0000000000000005e-08], loss:2.2831778526306152\n",
      "Epoch: 121, batch index: 1944, learning rate: [1.0000000000000005e-08], loss:2.2910449504852295\n",
      "Epoch: 121, batch index: 1945, learning rate: [1.0000000000000005e-08], loss:2.2357614040374756\n",
      "Epoch: 121, batch index: 1946, learning rate: [1.0000000000000005e-08], loss:2.2720084190368652\n",
      "Epoch: 121, batch index: 1947, learning rate: [1.0000000000000005e-08], loss:2.2405292987823486\n",
      "Epoch: 121, batch index: 1948, learning rate: [1.0000000000000005e-08], loss:2.303579807281494\n",
      "Epoch: 121, batch index: 1949, learning rate: [1.0000000000000005e-08], loss:2.2622103691101074\n",
      "Epoch: 121, batch index: 1950, learning rate: [1.0000000000000005e-08], loss:2.2875633239746094\n",
      "Epoch: 121, batch index: 1951, learning rate: [1.0000000000000005e-08], loss:2.2578625679016113\n",
      "Epoch 121, validation accuracy score0.306640625\n",
      "Epoch: 122, batch index: 1952, learning rate: [1.0000000000000005e-08], loss:2.2632193565368652\n",
      "Epoch: 122, batch index: 1953, learning rate: [1.0000000000000005e-08], loss:2.2712996006011963\n",
      "Epoch: 122, batch index: 1954, learning rate: [1.0000000000000005e-08], loss:2.255697250366211\n",
      "Epoch: 122, batch index: 1955, learning rate: [1.0000000000000005e-08], loss:2.272555112838745\n",
      "Epoch: 122, batch index: 1956, learning rate: [1.0000000000000005e-08], loss:2.2470662593841553\n",
      "Epoch: 122, batch index: 1957, learning rate: [1.0000000000000005e-08], loss:2.276853561401367\n",
      "Epoch: 122, batch index: 1958, learning rate: [1.0000000000000005e-08], loss:2.256251096725464\n",
      "Epoch: 122, batch index: 1959, learning rate: [1.0000000000000005e-08], loss:2.2720024585723877\n",
      "Epoch: 122, batch index: 1960, learning rate: [1.0000000000000005e-08], loss:2.334838628768921\n",
      "Epoch: 122, batch index: 1961, learning rate: [1.0000000000000005e-08], loss:2.27724289894104\n",
      "Epoch: 122, batch index: 1962, learning rate: [1.0000000000000005e-08], loss:2.271498680114746\n",
      "Epoch: 122, batch index: 1963, learning rate: [1.0000000000000005e-08], loss:2.258991003036499\n",
      "Epoch: 122, batch index: 1964, learning rate: [1.0000000000000005e-08], loss:2.2881357669830322\n",
      "Epoch: 122, batch index: 1965, learning rate: [1.0000000000000005e-08], loss:2.2646427154541016\n",
      "Epoch: 122, batch index: 1966, learning rate: [1.0000000000000005e-08], loss:2.2686245441436768\n",
      "Epoch: 122, batch index: 1967, learning rate: [1.0000000000000005e-08], loss:2.2389578819274902\n",
      "Epoch 122, validation accuracy score0.302734375\n",
      "Epoch: 123, batch index: 1968, learning rate: [1.0000000000000005e-08], loss:2.2475595474243164\n",
      "Epoch: 123, batch index: 1969, learning rate: [1.0000000000000005e-08], loss:2.2569832801818848\n",
      "Epoch: 123, batch index: 1970, learning rate: [1.0000000000000005e-08], loss:2.266407012939453\n",
      "Epoch: 123, batch index: 1971, learning rate: [1.0000000000000005e-08], loss:2.2422266006469727\n",
      "Epoch: 123, batch index: 1972, learning rate: [1.0000000000000005e-08], loss:2.287196397781372\n",
      "Epoch: 123, batch index: 1973, learning rate: [1.0000000000000005e-08], loss:2.3027923107147217\n",
      "Epoch: 123, batch index: 1974, learning rate: [1.0000000000000005e-08], loss:2.2761518955230713\n",
      "Epoch: 123, batch index: 1975, learning rate: [1.0000000000000005e-08], loss:2.2583394050598145\n",
      "Epoch: 123, batch index: 1976, learning rate: [1.0000000000000005e-08], loss:2.2591874599456787\n",
      "Epoch: 123, batch index: 1977, learning rate: [1.0000000000000005e-08], loss:2.308497667312622\n",
      "Epoch: 123, batch index: 1978, learning rate: [1.0000000000000005e-08], loss:2.2848446369171143\n",
      "Epoch: 123, batch index: 1979, learning rate: [1.0000000000000005e-08], loss:2.2335455417633057\n",
      "Epoch: 123, batch index: 1980, learning rate: [1.0000000000000005e-08], loss:2.276705503463745\n",
      "Epoch: 123, batch index: 1981, learning rate: [1.0000000000000005e-08], loss:2.2548587322235107\n",
      "Epoch: 123, batch index: 1982, learning rate: [1.0000000000000005e-08], loss:2.2848353385925293\n",
      "Epoch: 123, batch index: 1983, learning rate: [1.0000000000000005e-08], loss:2.2904129028320312\n",
      "Epoch 123, validation accuracy score0.314453125\n",
      "Epoch: 124, batch index: 1984, learning rate: [1.0000000000000005e-08], loss:2.260031223297119\n",
      "Epoch: 124, batch index: 1985, learning rate: [1.0000000000000005e-08], loss:2.2726266384124756\n",
      "Epoch: 124, batch index: 1986, learning rate: [1.0000000000000005e-08], loss:2.2352705001831055\n",
      "Epoch: 124, batch index: 1987, learning rate: [1.0000000000000005e-08], loss:2.270090341567993\n",
      "Epoch: 124, batch index: 1988, learning rate: [1.0000000000000005e-08], loss:2.293118476867676\n",
      "Epoch: 124, batch index: 1989, learning rate: [1.0000000000000005e-08], loss:2.277700662612915\n",
      "Epoch: 124, batch index: 1990, learning rate: [1.0000000000000005e-08], loss:2.272153377532959\n",
      "Epoch: 124, batch index: 1991, learning rate: [1.0000000000000005e-08], loss:2.272801160812378\n",
      "Epoch: 124, batch index: 1992, learning rate: [1.0000000000000005e-08], loss:2.2944912910461426\n",
      "Epoch: 124, batch index: 1993, learning rate: [1.0000000000000005e-08], loss:2.2821054458618164\n",
      "Epoch: 124, batch index: 1994, learning rate: [1.0000000000000005e-08], loss:2.3067426681518555\n",
      "Epoch: 124, batch index: 1995, learning rate: [1.0000000000000005e-08], loss:2.253619909286499\n",
      "Epoch: 124, batch index: 1996, learning rate: [1.0000000000000005e-08], loss:2.244821786880493\n",
      "Epoch: 124, batch index: 1997, learning rate: [1.0000000000000005e-08], loss:2.2924273014068604\n",
      "Epoch: 124, batch index: 1998, learning rate: [1.0000000000000005e-08], loss:2.296218156814575\n",
      "Epoch: 124, batch index: 1999, learning rate: [1.0000000000000005e-08], loss:2.2397050857543945\n",
      "Epoch 124, validation accuracy score0.291015625\n",
      "Epoch: 125, batch index: 2000, learning rate: [1.0000000000000005e-08], loss:2.2738308906555176\n",
      "Epoch: 125, batch index: 2001, learning rate: [1.0000000000000005e-08], loss:2.247720956802368\n",
      "Epoch: 125, batch index: 2002, learning rate: [1.0000000000000005e-08], loss:2.2602603435516357\n",
      "Epoch: 125, batch index: 2003, learning rate: [1.0000000000000005e-08], loss:2.267982006072998\n",
      "Epoch: 125, batch index: 2004, learning rate: [1.0000000000000005e-08], loss:2.301016092300415\n",
      "Epoch: 125, batch index: 2005, learning rate: [1.0000000000000005e-08], loss:2.265042304992676\n",
      "Epoch: 125, batch index: 2006, learning rate: [1.0000000000000005e-08], loss:2.2468576431274414\n",
      "Epoch: 125, batch index: 2007, learning rate: [1.0000000000000005e-08], loss:2.2752816677093506\n",
      "Epoch: 125, batch index: 2008, learning rate: [1.0000000000000005e-08], loss:2.243896007537842\n",
      "Epoch: 125, batch index: 2009, learning rate: [1.0000000000000005e-08], loss:2.2758216857910156\n",
      "Epoch: 125, batch index: 2010, learning rate: [1.0000000000000005e-08], loss:2.29313063621521\n",
      "Epoch: 125, batch index: 2011, learning rate: [1.0000000000000005e-08], loss:2.2576534748077393\n",
      "Epoch: 125, batch index: 2012, learning rate: [1.0000000000000005e-08], loss:2.2623775005340576\n",
      "Epoch: 125, batch index: 2013, learning rate: [1.0000000000000005e-08], loss:2.2977194786071777\n",
      "Epoch: 125, batch index: 2014, learning rate: [1.0000000000000005e-08], loss:2.2614336013793945\n",
      "Epoch: 125, batch index: 2015, learning rate: [1.0000000000000005e-08], loss:2.3021321296691895\n",
      "Epoch 125, validation accuracy score0.337890625\n",
      "Epoch: 126, batch index: 2016, learning rate: [1.0000000000000005e-08], loss:2.2707247734069824\n",
      "Epoch: 126, batch index: 2017, learning rate: [1.0000000000000005e-08], loss:2.2642242908477783\n",
      "Epoch: 126, batch index: 2018, learning rate: [1.0000000000000005e-08], loss:2.3042571544647217\n",
      "Epoch: 126, batch index: 2019, learning rate: [1.0000000000000005e-08], loss:2.2462990283966064\n",
      "Epoch: 126, batch index: 2020, learning rate: [1.0000000000000005e-08], loss:2.285517454147339\n",
      "Epoch: 126, batch index: 2021, learning rate: [1.0000000000000005e-08], loss:2.247619867324829\n",
      "Epoch: 126, batch index: 2022, learning rate: [1.0000000000000005e-08], loss:2.2928953170776367\n",
      "Epoch: 126, batch index: 2023, learning rate: [1.0000000000000005e-08], loss:2.275508403778076\n",
      "Epoch: 126, batch index: 2024, learning rate: [1.0000000000000005e-08], loss:2.2661283016204834\n",
      "Epoch: 126, batch index: 2025, learning rate: [1.0000000000000005e-08], loss:2.2717018127441406\n",
      "Epoch: 126, batch index: 2026, learning rate: [1.0000000000000005e-08], loss:2.284163475036621\n",
      "Epoch: 126, batch index: 2027, learning rate: [1.0000000000000005e-08], loss:2.2833685874938965\n",
      "Epoch: 126, batch index: 2028, learning rate: [1.0000000000000005e-08], loss:2.2278831005096436\n",
      "Epoch: 126, batch index: 2029, learning rate: [1.0000000000000005e-08], loss:2.2922141551971436\n",
      "Epoch: 126, batch index: 2030, learning rate: [1.0000000000000005e-08], loss:2.2814061641693115\n",
      "Epoch: 126, batch index: 2031, learning rate: [1.0000000000000005e-08], loss:2.2681920528411865\n",
      "Epoch 126, validation accuracy score0.296875\n",
      "Epoch: 127, batch index: 2032, learning rate: [1.0000000000000005e-08], loss:2.249239206314087\n",
      "Epoch: 127, batch index: 2033, learning rate: [1.0000000000000005e-08], loss:2.2759499549865723\n",
      "Epoch: 127, batch index: 2034, learning rate: [1.0000000000000005e-08], loss:2.288893461227417\n",
      "Epoch: 127, batch index: 2035, learning rate: [1.0000000000000005e-08], loss:2.3078839778900146\n",
      "Epoch: 127, batch index: 2036, learning rate: [1.0000000000000005e-08], loss:2.2447237968444824\n",
      "Epoch: 127, batch index: 2037, learning rate: [1.0000000000000005e-08], loss:2.2647109031677246\n",
      "Epoch: 127, batch index: 2038, learning rate: [1.0000000000000005e-08], loss:2.2844085693359375\n",
      "Epoch: 127, batch index: 2039, learning rate: [1.0000000000000005e-08], loss:2.2514560222625732\n",
      "Epoch: 127, batch index: 2040, learning rate: [1.0000000000000005e-08], loss:2.2381186485290527\n",
      "Epoch: 127, batch index: 2041, learning rate: [1.0000000000000005e-08], loss:2.2914013862609863\n",
      "Epoch: 127, batch index: 2042, learning rate: [1.0000000000000005e-08], loss:2.284391164779663\n",
      "Epoch: 127, batch index: 2043, learning rate: [1.0000000000000005e-08], loss:2.312363386154175\n",
      "Epoch: 127, batch index: 2044, learning rate: [1.0000000000000005e-08], loss:2.230046033859253\n",
      "Epoch: 127, batch index: 2045, learning rate: [1.0000000000000005e-08], loss:2.2627949714660645\n",
      "Epoch: 127, batch index: 2046, learning rate: [1.0000000000000005e-08], loss:2.3052315711975098\n",
      "Epoch: 127, batch index: 2047, learning rate: [1.0000000000000005e-08], loss:2.245406150817871\n",
      "Epoch 127, validation accuracy score0.328125\n",
      "Epoch: 128, batch index: 2048, learning rate: [1.0000000000000005e-08], loss:2.2559337615966797\n",
      "Epoch: 128, batch index: 2049, learning rate: [1.0000000000000005e-08], loss:2.304811716079712\n",
      "Epoch: 128, batch index: 2050, learning rate: [1.0000000000000005e-08], loss:2.2469234466552734\n",
      "Epoch: 128, batch index: 2051, learning rate: [1.0000000000000005e-08], loss:2.222053050994873\n",
      "Epoch: 128, batch index: 2052, learning rate: [1.0000000000000005e-08], loss:2.300765037536621\n",
      "Epoch: 128, batch index: 2053, learning rate: [1.0000000000000005e-08], loss:2.290559768676758\n",
      "Epoch: 128, batch index: 2054, learning rate: [1.0000000000000005e-08], loss:2.2811965942382812\n",
      "Epoch: 128, batch index: 2055, learning rate: [1.0000000000000005e-08], loss:2.289703607559204\n",
      "Epoch: 128, batch index: 2056, learning rate: [1.0000000000000005e-08], loss:2.2625067234039307\n",
      "Epoch: 128, batch index: 2057, learning rate: [1.0000000000000005e-08], loss:2.2762253284454346\n",
      "Epoch: 128, batch index: 2058, learning rate: [1.0000000000000005e-08], loss:2.2496745586395264\n",
      "Epoch: 128, batch index: 2059, learning rate: [1.0000000000000005e-08], loss:2.247072219848633\n",
      "Epoch: 128, batch index: 2060, learning rate: [1.0000000000000005e-08], loss:2.2717444896698\n",
      "Epoch: 128, batch index: 2061, learning rate: [1.0000000000000005e-08], loss:2.2567877769470215\n",
      "Epoch: 128, batch index: 2062, learning rate: [1.0000000000000005e-08], loss:2.232375144958496\n",
      "Epoch: 128, batch index: 2063, learning rate: [1.0000000000000005e-08], loss:2.2594640254974365\n",
      "Epoch 128, validation accuracy score0.330078125\n",
      "Epoch: 129, batch index: 2064, learning rate: [1.0000000000000005e-08], loss:2.275378942489624\n",
      "Epoch: 129, batch index: 2065, learning rate: [1.0000000000000005e-08], loss:2.2393429279327393\n",
      "Epoch: 129, batch index: 2066, learning rate: [1.0000000000000005e-08], loss:2.2842628955841064\n",
      "Epoch: 129, batch index: 2067, learning rate: [1.0000000000000005e-08], loss:2.269355535507202\n",
      "Epoch: 129, batch index: 2068, learning rate: [1.0000000000000005e-08], loss:2.276362419128418\n",
      "Epoch: 129, batch index: 2069, learning rate: [1.0000000000000005e-08], loss:2.260298252105713\n",
      "Epoch: 129, batch index: 2070, learning rate: [1.0000000000000005e-08], loss:2.2667582035064697\n",
      "Epoch: 129, batch index: 2071, learning rate: [1.0000000000000005e-08], loss:2.258965015411377\n",
      "Epoch: 129, batch index: 2072, learning rate: [1.0000000000000005e-08], loss:2.2760117053985596\n",
      "Epoch: 129, batch index: 2073, learning rate: [1.0000000000000005e-08], loss:2.261442184448242\n",
      "Epoch: 129, batch index: 2074, learning rate: [1.0000000000000005e-08], loss:2.2745048999786377\n",
      "Epoch: 129, batch index: 2075, learning rate: [1.0000000000000005e-08], loss:2.2687575817108154\n",
      "Epoch: 129, batch index: 2076, learning rate: [1.0000000000000005e-08], loss:2.287750720977783\n",
      "Epoch: 129, batch index: 2077, learning rate: [1.0000000000000005e-08], loss:2.2436656951904297\n",
      "Epoch: 129, batch index: 2078, learning rate: [1.0000000000000005e-08], loss:2.2928807735443115\n",
      "Epoch: 129, batch index: 2079, learning rate: [1.0000000000000005e-08], loss:2.262568950653076\n",
      "Epoch 129, validation accuracy score0.318359375\n",
      "Epoch: 130, batch index: 2080, learning rate: [1.0000000000000005e-08], loss:2.2712061405181885\n",
      "Epoch: 130, batch index: 2081, learning rate: [1.0000000000000005e-08], loss:2.259028196334839\n",
      "Epoch: 130, batch index: 2082, learning rate: [1.0000000000000005e-08], loss:2.2843093872070312\n",
      "Epoch: 130, batch index: 2083, learning rate: [1.0000000000000005e-08], loss:2.2415409088134766\n",
      "Epoch: 130, batch index: 2084, learning rate: [1.0000000000000005e-08], loss:2.2699942588806152\n",
      "Epoch: 130, batch index: 2085, learning rate: [1.0000000000000005e-08], loss:2.2518045902252197\n",
      "Epoch: 130, batch index: 2086, learning rate: [1.0000000000000005e-08], loss:2.278404951095581\n",
      "Epoch: 130, batch index: 2087, learning rate: [1.0000000000000005e-08], loss:2.251613140106201\n",
      "Epoch: 130, batch index: 2088, learning rate: [1.0000000000000005e-08], loss:2.2949931621551514\n",
      "Epoch: 130, batch index: 2089, learning rate: [1.0000000000000005e-08], loss:2.2357780933380127\n",
      "Epoch: 130, batch index: 2090, learning rate: [1.0000000000000005e-08], loss:2.2410190105438232\n",
      "Epoch: 130, batch index: 2091, learning rate: [1.0000000000000005e-08], loss:2.263216257095337\n",
      "Epoch: 130, batch index: 2092, learning rate: [1.0000000000000005e-08], loss:2.292579412460327\n",
      "Epoch: 130, batch index: 2093, learning rate: [1.0000000000000005e-08], loss:2.2993342876434326\n",
      "Epoch: 130, batch index: 2094, learning rate: [1.0000000000000005e-08], loss:2.301767349243164\n",
      "Epoch: 130, batch index: 2095, learning rate: [1.0000000000000005e-08], loss:2.30298113822937\n",
      "Epoch 130, validation accuracy score0.333984375\n",
      "Epoch: 131, batch index: 2096, learning rate: [1.0000000000000005e-08], loss:2.2938990592956543\n",
      "Epoch: 131, batch index: 2097, learning rate: [1.0000000000000005e-08], loss:2.267544984817505\n",
      "Epoch: 131, batch index: 2098, learning rate: [1.0000000000000005e-08], loss:2.233579397201538\n",
      "Epoch: 131, batch index: 2099, learning rate: [1.0000000000000005e-08], loss:2.288170337677002\n",
      "Epoch: 131, batch index: 2100, learning rate: [1.0000000000000005e-08], loss:2.2571682929992676\n",
      "Epoch: 131, batch index: 2101, learning rate: [1.0000000000000005e-08], loss:2.2494957447052\n",
      "Epoch: 131, batch index: 2102, learning rate: [1.0000000000000005e-08], loss:2.2537050247192383\n",
      "Epoch: 131, batch index: 2103, learning rate: [1.0000000000000005e-08], loss:2.2845990657806396\n",
      "Epoch: 131, batch index: 2104, learning rate: [1.0000000000000005e-08], loss:2.2631185054779053\n",
      "Epoch: 131, batch index: 2105, learning rate: [1.0000000000000005e-08], loss:2.2590842247009277\n",
      "Epoch: 131, batch index: 2106, learning rate: [1.0000000000000005e-08], loss:2.290670156478882\n",
      "Epoch: 131, batch index: 2107, learning rate: [1.0000000000000005e-08], loss:2.2614264488220215\n",
      "Epoch: 131, batch index: 2108, learning rate: [1.0000000000000005e-08], loss:2.305887222290039\n",
      "Epoch: 131, batch index: 2109, learning rate: [1.0000000000000005e-08], loss:2.259737253189087\n",
      "Epoch: 131, batch index: 2110, learning rate: [1.0000000000000005e-08], loss:2.2850303649902344\n",
      "Epoch: 131, batch index: 2111, learning rate: [1.0000000000000005e-08], loss:2.263899326324463\n",
      "Epoch 131, validation accuracy score0.314453125\n",
      "Epoch: 132, batch index: 2112, learning rate: [1.0000000000000005e-08], loss:2.231182813644409\n",
      "Epoch: 132, batch index: 2113, learning rate: [1.0000000000000005e-08], loss:2.251901626586914\n",
      "Epoch: 132, batch index: 2114, learning rate: [1.0000000000000005e-08], loss:2.243565797805786\n",
      "Epoch: 132, batch index: 2115, learning rate: [1.0000000000000005e-08], loss:2.285832405090332\n",
      "Epoch: 132, batch index: 2116, learning rate: [1.0000000000000005e-08], loss:2.2863242626190186\n",
      "Epoch: 132, batch index: 2117, learning rate: [1.0000000000000005e-08], loss:2.2529797554016113\n",
      "Epoch: 132, batch index: 2118, learning rate: [1.0000000000000005e-08], loss:2.25303053855896\n",
      "Epoch: 132, batch index: 2119, learning rate: [1.0000000000000005e-08], loss:2.2857463359832764\n",
      "Epoch: 132, batch index: 2120, learning rate: [1.0000000000000005e-08], loss:2.2924163341522217\n",
      "Epoch: 132, batch index: 2121, learning rate: [1.0000000000000005e-08], loss:2.2951364517211914\n",
      "Epoch: 132, batch index: 2122, learning rate: [1.0000000000000005e-08], loss:2.279322624206543\n",
      "Epoch: 132, batch index: 2123, learning rate: [1.0000000000000005e-08], loss:2.264620780944824\n",
      "Epoch: 132, batch index: 2124, learning rate: [1.0000000000000005e-08], loss:2.2473320960998535\n",
      "Epoch: 132, batch index: 2125, learning rate: [1.0000000000000005e-08], loss:2.243589401245117\n",
      "Epoch: 132, batch index: 2126, learning rate: [1.0000000000000005e-08], loss:2.3067824840545654\n",
      "Epoch: 132, batch index: 2127, learning rate: [1.0000000000000005e-08], loss:2.278541088104248\n",
      "Epoch 132, validation accuracy score0.310546875\n",
      "Epoch: 133, batch index: 2128, learning rate: [1.0000000000000005e-08], loss:2.2737269401550293\n",
      "Epoch: 133, batch index: 2129, learning rate: [1.0000000000000005e-08], loss:2.2790839672088623\n",
      "Epoch: 133, batch index: 2130, learning rate: [1.0000000000000005e-08], loss:2.2659523487091064\n",
      "Epoch: 133, batch index: 2131, learning rate: [1.0000000000000005e-08], loss:2.274374008178711\n",
      "Epoch: 133, batch index: 2132, learning rate: [1.0000000000000005e-08], loss:2.2772629261016846\n",
      "Epoch: 133, batch index: 2133, learning rate: [1.0000000000000005e-08], loss:2.290619134902954\n",
      "Epoch: 133, batch index: 2134, learning rate: [1.0000000000000005e-08], loss:2.246533155441284\n",
      "Epoch: 133, batch index: 2135, learning rate: [1.0000000000000005e-08], loss:2.2885406017303467\n",
      "Epoch: 133, batch index: 2136, learning rate: [1.0000000000000005e-08], loss:2.243840217590332\n",
      "Epoch: 133, batch index: 2137, learning rate: [1.0000000000000005e-08], loss:2.267209768295288\n",
      "Epoch: 133, batch index: 2138, learning rate: [1.0000000000000005e-08], loss:2.266742467880249\n",
      "Epoch: 133, batch index: 2139, learning rate: [1.0000000000000005e-08], loss:2.272448778152466\n",
      "Epoch: 133, batch index: 2140, learning rate: [1.0000000000000005e-08], loss:2.2784323692321777\n",
      "Epoch: 133, batch index: 2141, learning rate: [1.0000000000000005e-08], loss:2.2732834815979004\n",
      "Epoch: 133, batch index: 2142, learning rate: [1.0000000000000005e-08], loss:2.26165509223938\n",
      "Epoch: 133, batch index: 2143, learning rate: [1.0000000000000005e-08], loss:2.2773971557617188\n",
      "Epoch 133, validation accuracy score0.3125\n",
      "Epoch: 134, batch index: 2144, learning rate: [1.0000000000000005e-08], loss:2.2368717193603516\n",
      "Epoch: 134, batch index: 2145, learning rate: [1.0000000000000005e-08], loss:2.257699489593506\n",
      "Epoch: 134, batch index: 2146, learning rate: [1.0000000000000005e-08], loss:2.2702834606170654\n",
      "Epoch: 134, batch index: 2147, learning rate: [1.0000000000000005e-08], loss:2.3188209533691406\n",
      "Epoch: 134, batch index: 2148, learning rate: [1.0000000000000005e-08], loss:2.3049561977386475\n",
      "Epoch: 134, batch index: 2149, learning rate: [1.0000000000000005e-08], loss:2.267029285430908\n",
      "Epoch: 134, batch index: 2150, learning rate: [1.0000000000000005e-08], loss:2.2791216373443604\n",
      "Epoch: 134, batch index: 2151, learning rate: [1.0000000000000005e-08], loss:2.2777786254882812\n",
      "Epoch: 134, batch index: 2152, learning rate: [1.0000000000000005e-08], loss:2.2783446311950684\n",
      "Epoch: 134, batch index: 2153, learning rate: [1.0000000000000005e-08], loss:2.266261577606201\n",
      "Epoch: 134, batch index: 2154, learning rate: [1.0000000000000005e-08], loss:2.2384374141693115\n",
      "Epoch: 134, batch index: 2155, learning rate: [1.0000000000000005e-08], loss:2.2763075828552246\n",
      "Epoch: 134, batch index: 2156, learning rate: [1.0000000000000005e-08], loss:2.240335464477539\n",
      "Epoch: 134, batch index: 2157, learning rate: [1.0000000000000005e-08], loss:2.26476788520813\n",
      "Epoch: 134, batch index: 2158, learning rate: [1.0000000000000005e-08], loss:2.3091251850128174\n",
      "Epoch: 134, batch index: 2159, learning rate: [1.0000000000000005e-08], loss:2.2528138160705566\n",
      "Epoch 134, validation accuracy score0.310546875\n",
      "Epoch: 135, batch index: 2160, learning rate: [1.0000000000000005e-08], loss:2.2446703910827637\n",
      "Epoch: 135, batch index: 2161, learning rate: [1.0000000000000005e-08], loss:2.266763925552368\n",
      "Epoch: 135, batch index: 2162, learning rate: [1.0000000000000005e-08], loss:2.2473068237304688\n",
      "Epoch: 135, batch index: 2163, learning rate: [1.0000000000000005e-08], loss:2.290818452835083\n",
      "Epoch: 135, batch index: 2164, learning rate: [1.0000000000000005e-08], loss:2.2808499336242676\n",
      "Epoch: 135, batch index: 2165, learning rate: [1.0000000000000005e-08], loss:2.2683510780334473\n",
      "Epoch: 135, batch index: 2166, learning rate: [1.0000000000000005e-08], loss:2.2645950317382812\n",
      "Epoch: 135, batch index: 2167, learning rate: [1.0000000000000005e-08], loss:2.2602782249450684\n",
      "Epoch: 135, batch index: 2168, learning rate: [1.0000000000000005e-08], loss:2.2800533771514893\n",
      "Epoch: 135, batch index: 2169, learning rate: [1.0000000000000005e-08], loss:2.2698118686676025\n",
      "Epoch: 135, batch index: 2170, learning rate: [1.0000000000000005e-08], loss:2.2564451694488525\n",
      "Epoch: 135, batch index: 2171, learning rate: [1.0000000000000005e-08], loss:2.2997400760650635\n",
      "Epoch: 135, batch index: 2172, learning rate: [1.0000000000000005e-08], loss:2.2991037368774414\n",
      "Epoch: 135, batch index: 2173, learning rate: [1.0000000000000005e-08], loss:2.308966636657715\n",
      "Epoch: 135, batch index: 2174, learning rate: [1.0000000000000005e-08], loss:2.27095890045166\n",
      "Epoch: 135, batch index: 2175, learning rate: [1.0000000000000005e-08], loss:2.293926239013672\n",
      "Epoch 135, validation accuracy score0.314453125\n",
      "Epoch: 136, batch index: 2176, learning rate: [1.0000000000000005e-08], loss:2.2730088233947754\n",
      "Epoch: 136, batch index: 2177, learning rate: [1.0000000000000005e-08], loss:2.2228612899780273\n",
      "Epoch: 136, batch index: 2178, learning rate: [1.0000000000000005e-08], loss:2.268846035003662\n",
      "Epoch: 136, batch index: 2179, learning rate: [1.0000000000000005e-08], loss:2.227545738220215\n",
      "Epoch: 136, batch index: 2180, learning rate: [1.0000000000000005e-08], loss:2.247828245162964\n",
      "Epoch: 136, batch index: 2181, learning rate: [1.0000000000000005e-08], loss:2.2467024326324463\n",
      "Epoch: 136, batch index: 2182, learning rate: [1.0000000000000005e-08], loss:2.2834842205047607\n",
      "Epoch: 136, batch index: 2183, learning rate: [1.0000000000000005e-08], loss:2.2967007160186768\n",
      "Epoch: 136, batch index: 2184, learning rate: [1.0000000000000005e-08], loss:2.29959774017334\n",
      "Epoch: 136, batch index: 2185, learning rate: [1.0000000000000005e-08], loss:2.232717752456665\n",
      "Epoch: 136, batch index: 2186, learning rate: [1.0000000000000005e-08], loss:2.2671544551849365\n",
      "Epoch: 136, batch index: 2187, learning rate: [1.0000000000000005e-08], loss:2.2760860919952393\n",
      "Epoch: 136, batch index: 2188, learning rate: [1.0000000000000005e-08], loss:2.2936949729919434\n",
      "Epoch: 136, batch index: 2189, learning rate: [1.0000000000000005e-08], loss:2.294461727142334\n",
      "Epoch: 136, batch index: 2190, learning rate: [1.0000000000000005e-08], loss:2.2853405475616455\n",
      "Epoch: 136, batch index: 2191, learning rate: [1.0000000000000005e-08], loss:2.2707886695861816\n",
      "Epoch 136, validation accuracy score0.341796875\n",
      "Epoch: 137, batch index: 2192, learning rate: [1.0000000000000005e-08], loss:2.2858738899230957\n",
      "Epoch: 137, batch index: 2193, learning rate: [1.0000000000000005e-08], loss:2.267692804336548\n",
      "Epoch: 137, batch index: 2194, learning rate: [1.0000000000000005e-08], loss:2.2783195972442627\n",
      "Epoch: 137, batch index: 2195, learning rate: [1.0000000000000005e-08], loss:2.2631967067718506\n",
      "Epoch: 137, batch index: 2196, learning rate: [1.0000000000000005e-08], loss:2.2931766510009766\n",
      "Epoch: 137, batch index: 2197, learning rate: [1.0000000000000005e-08], loss:2.250732183456421\n",
      "Epoch: 137, batch index: 2198, learning rate: [1.0000000000000005e-08], loss:2.2555437088012695\n",
      "Epoch: 137, batch index: 2199, learning rate: [1.0000000000000005e-08], loss:2.2946393489837646\n",
      "Epoch: 137, batch index: 2200, learning rate: [1.0000000000000005e-08], loss:2.25529146194458\n",
      "Epoch: 137, batch index: 2201, learning rate: [1.0000000000000005e-08], loss:2.291811943054199\n",
      "Epoch: 137, batch index: 2202, learning rate: [1.0000000000000005e-08], loss:2.273792266845703\n",
      "Epoch: 137, batch index: 2203, learning rate: [1.0000000000000005e-08], loss:2.284127712249756\n",
      "Epoch: 137, batch index: 2204, learning rate: [1.0000000000000005e-08], loss:2.2714061737060547\n",
      "Epoch: 137, batch index: 2205, learning rate: [1.0000000000000005e-08], loss:2.2948060035705566\n",
      "Epoch: 137, batch index: 2206, learning rate: [1.0000000000000005e-08], loss:2.2684450149536133\n",
      "Epoch: 137, batch index: 2207, learning rate: [1.0000000000000005e-08], loss:2.2797529697418213\n",
      "Epoch 137, validation accuracy score0.330078125\n",
      "Epoch: 138, batch index: 2208, learning rate: [1.0000000000000005e-08], loss:2.2852847576141357\n",
      "Epoch: 138, batch index: 2209, learning rate: [1.0000000000000005e-08], loss:2.282648801803589\n",
      "Epoch: 138, batch index: 2210, learning rate: [1.0000000000000005e-08], loss:2.304621458053589\n",
      "Epoch: 138, batch index: 2211, learning rate: [1.0000000000000005e-08], loss:2.273331642150879\n",
      "Epoch: 138, batch index: 2212, learning rate: [1.0000000000000005e-08], loss:2.26400089263916\n",
      "Epoch: 138, batch index: 2213, learning rate: [1.0000000000000005e-08], loss:2.2827908992767334\n",
      "Epoch: 138, batch index: 2214, learning rate: [1.0000000000000005e-08], loss:2.243560552597046\n",
      "Epoch: 138, batch index: 2215, learning rate: [1.0000000000000005e-08], loss:2.300276041030884\n",
      "Epoch: 138, batch index: 2216, learning rate: [1.0000000000000005e-08], loss:2.2725939750671387\n",
      "Epoch: 138, batch index: 2217, learning rate: [1.0000000000000005e-08], loss:2.2573025226593018\n",
      "Epoch: 138, batch index: 2218, learning rate: [1.0000000000000005e-08], loss:2.257972240447998\n",
      "Epoch: 138, batch index: 2219, learning rate: [1.0000000000000005e-08], loss:2.245185375213623\n",
      "Epoch: 138, batch index: 2220, learning rate: [1.0000000000000005e-08], loss:2.264697551727295\n",
      "Epoch: 138, batch index: 2221, learning rate: [1.0000000000000005e-08], loss:2.2684097290039062\n",
      "Epoch: 138, batch index: 2222, learning rate: [1.0000000000000005e-08], loss:2.288069725036621\n",
      "Epoch: 138, batch index: 2223, learning rate: [1.0000000000000005e-08], loss:2.309910297393799\n",
      "Epoch 138, validation accuracy score0.330078125\n",
      "Epoch: 139, batch index: 2224, learning rate: [1.0000000000000005e-08], loss:2.2884442806243896\n",
      "Epoch: 139, batch index: 2225, learning rate: [1.0000000000000005e-08], loss:2.2642970085144043\n",
      "Epoch: 139, batch index: 2226, learning rate: [1.0000000000000005e-08], loss:2.274854898452759\n",
      "Epoch: 139, batch index: 2227, learning rate: [1.0000000000000005e-08], loss:2.26615309715271\n",
      "Epoch: 139, batch index: 2228, learning rate: [1.0000000000000005e-08], loss:2.3009119033813477\n",
      "Epoch: 139, batch index: 2229, learning rate: [1.0000000000000005e-08], loss:2.2736318111419678\n",
      "Epoch: 139, batch index: 2230, learning rate: [1.0000000000000005e-08], loss:2.248776912689209\n",
      "Epoch: 139, batch index: 2231, learning rate: [1.0000000000000005e-08], loss:2.307525157928467\n",
      "Epoch: 139, batch index: 2232, learning rate: [1.0000000000000005e-08], loss:2.2602956295013428\n",
      "Epoch: 139, batch index: 2233, learning rate: [1.0000000000000005e-08], loss:2.281529664993286\n",
      "Epoch: 139, batch index: 2234, learning rate: [1.0000000000000005e-08], loss:2.2138144969940186\n",
      "Epoch: 139, batch index: 2235, learning rate: [1.0000000000000005e-08], loss:2.271023750305176\n",
      "Epoch: 139, batch index: 2236, learning rate: [1.0000000000000005e-08], loss:2.2938392162323\n",
      "Epoch: 139, batch index: 2237, learning rate: [1.0000000000000005e-08], loss:2.2639119625091553\n",
      "Epoch: 139, batch index: 2238, learning rate: [1.0000000000000005e-08], loss:2.2714438438415527\n",
      "Epoch: 139, batch index: 2239, learning rate: [1.0000000000000005e-08], loss:2.2372233867645264\n",
      "Epoch 139, validation accuracy score0.3125\n",
      "Epoch: 140, batch index: 2240, learning rate: [1.0000000000000005e-08], loss:2.3080132007598877\n",
      "Epoch: 140, batch index: 2241, learning rate: [1.0000000000000005e-08], loss:2.303147315979004\n",
      "Epoch: 140, batch index: 2242, learning rate: [1.0000000000000005e-08], loss:2.2970471382141113\n",
      "Epoch: 140, batch index: 2243, learning rate: [1.0000000000000005e-08], loss:2.278538227081299\n",
      "Epoch: 140, batch index: 2244, learning rate: [1.0000000000000005e-08], loss:2.2754247188568115\n",
      "Epoch: 140, batch index: 2245, learning rate: [1.0000000000000005e-08], loss:2.2655086517333984\n",
      "Epoch: 140, batch index: 2246, learning rate: [1.0000000000000005e-08], loss:2.2555134296417236\n",
      "Epoch: 140, batch index: 2247, learning rate: [1.0000000000000005e-08], loss:2.2993807792663574\n",
      "Epoch: 140, batch index: 2248, learning rate: [1.0000000000000005e-08], loss:2.280991554260254\n",
      "Epoch: 140, batch index: 2249, learning rate: [1.0000000000000005e-08], loss:2.2735252380371094\n",
      "Epoch: 140, batch index: 2250, learning rate: [1.0000000000000005e-08], loss:2.296804428100586\n",
      "Epoch: 140, batch index: 2251, learning rate: [1.0000000000000005e-08], loss:2.266207218170166\n",
      "Epoch: 140, batch index: 2252, learning rate: [1.0000000000000005e-08], loss:2.224593162536621\n",
      "Epoch: 140, batch index: 2253, learning rate: [1.0000000000000005e-08], loss:2.2617428302764893\n",
      "Epoch: 140, batch index: 2254, learning rate: [1.0000000000000005e-08], loss:2.2759790420532227\n",
      "Epoch: 140, batch index: 2255, learning rate: [1.0000000000000005e-08], loss:2.248664379119873\n",
      "Epoch 140, validation accuracy score0.35546875\n",
      "Epoch: 141, batch index: 2256, learning rate: [1.0000000000000005e-08], loss:2.216383695602417\n",
      "Epoch: 141, batch index: 2257, learning rate: [1.0000000000000005e-08], loss:2.2943930625915527\n",
      "Epoch: 141, batch index: 2258, learning rate: [1.0000000000000005e-08], loss:2.2680773735046387\n",
      "Epoch: 141, batch index: 2259, learning rate: [1.0000000000000005e-08], loss:2.2723212242126465\n",
      "Epoch: 141, batch index: 2260, learning rate: [1.0000000000000005e-08], loss:2.268156051635742\n",
      "Epoch: 141, batch index: 2261, learning rate: [1.0000000000000005e-08], loss:2.2540042400360107\n",
      "Epoch: 141, batch index: 2262, learning rate: [1.0000000000000005e-08], loss:2.2930381298065186\n",
      "Epoch: 141, batch index: 2263, learning rate: [1.0000000000000005e-08], loss:2.2758705615997314\n",
      "Epoch: 141, batch index: 2264, learning rate: [1.0000000000000005e-08], loss:2.257910966873169\n",
      "Epoch: 141, batch index: 2265, learning rate: [1.0000000000000005e-08], loss:2.2776882648468018\n",
      "Epoch: 141, batch index: 2266, learning rate: [1.0000000000000005e-08], loss:2.2428531646728516\n",
      "Epoch: 141, batch index: 2267, learning rate: [1.0000000000000005e-08], loss:2.307917356491089\n",
      "Epoch: 141, batch index: 2268, learning rate: [1.0000000000000005e-08], loss:2.243990421295166\n",
      "Epoch: 141, batch index: 2269, learning rate: [1.0000000000000005e-08], loss:2.3090686798095703\n",
      "Epoch: 141, batch index: 2270, learning rate: [1.0000000000000005e-08], loss:2.262265682220459\n",
      "Epoch: 141, batch index: 2271, learning rate: [1.0000000000000005e-08], loss:2.247180461883545\n",
      "Epoch 141, validation accuracy score0.314453125\n",
      "Epoch: 142, batch index: 2272, learning rate: [1.0000000000000005e-08], loss:2.2600960731506348\n",
      "Epoch: 142, batch index: 2273, learning rate: [1.0000000000000005e-08], loss:2.2191081047058105\n",
      "Epoch: 142, batch index: 2274, learning rate: [1.0000000000000005e-08], loss:2.2928569316864014\n",
      "Epoch: 142, batch index: 2275, learning rate: [1.0000000000000005e-08], loss:2.298694372177124\n",
      "Epoch: 142, batch index: 2276, learning rate: [1.0000000000000005e-08], loss:2.281128168106079\n",
      "Epoch: 142, batch index: 2277, learning rate: [1.0000000000000005e-08], loss:2.265615463256836\n",
      "Epoch: 142, batch index: 2278, learning rate: [1.0000000000000005e-08], loss:2.2725038528442383\n",
      "Epoch: 142, batch index: 2279, learning rate: [1.0000000000000005e-08], loss:2.293919324874878\n",
      "Epoch: 142, batch index: 2280, learning rate: [1.0000000000000005e-08], loss:2.2772951126098633\n",
      "Epoch: 142, batch index: 2281, learning rate: [1.0000000000000005e-08], loss:2.2519891262054443\n",
      "Epoch: 142, batch index: 2282, learning rate: [1.0000000000000005e-08], loss:2.2531559467315674\n",
      "Epoch: 142, batch index: 2283, learning rate: [1.0000000000000005e-08], loss:2.2757270336151123\n",
      "Epoch: 142, batch index: 2284, learning rate: [1.0000000000000005e-08], loss:2.2486605644226074\n",
      "Epoch: 142, batch index: 2285, learning rate: [1.0000000000000005e-08], loss:2.2652993202209473\n",
      "Epoch: 142, batch index: 2286, learning rate: [1.0000000000000005e-08], loss:2.265272617340088\n",
      "Epoch: 142, batch index: 2287, learning rate: [1.0000000000000005e-08], loss:2.285991907119751\n",
      "Epoch 142, validation accuracy score0.3046875\n",
      "Epoch: 143, batch index: 2288, learning rate: [1.0000000000000005e-08], loss:2.2862579822540283\n",
      "Epoch: 143, batch index: 2289, learning rate: [1.0000000000000005e-08], loss:2.2858641147613525\n",
      "Epoch: 143, batch index: 2290, learning rate: [1.0000000000000005e-08], loss:2.264957904815674\n",
      "Epoch: 143, batch index: 2291, learning rate: [1.0000000000000005e-08], loss:2.2711563110351562\n",
      "Epoch: 143, batch index: 2292, learning rate: [1.0000000000000005e-08], loss:2.2577710151672363\n",
      "Epoch: 143, batch index: 2293, learning rate: [1.0000000000000005e-08], loss:2.266364574432373\n",
      "Epoch: 143, batch index: 2294, learning rate: [1.0000000000000005e-08], loss:2.2664315700531006\n",
      "Epoch: 143, batch index: 2295, learning rate: [1.0000000000000005e-08], loss:2.259371042251587\n",
      "Epoch: 143, batch index: 2296, learning rate: [1.0000000000000005e-08], loss:2.2927446365356445\n",
      "Epoch: 143, batch index: 2297, learning rate: [1.0000000000000005e-08], loss:2.2729878425598145\n",
      "Epoch: 143, batch index: 2298, learning rate: [1.0000000000000005e-08], loss:2.2724547386169434\n",
      "Epoch: 143, batch index: 2299, learning rate: [1.0000000000000005e-08], loss:2.2781589031219482\n",
      "Epoch: 143, batch index: 2300, learning rate: [1.0000000000000005e-08], loss:2.279914617538452\n",
      "Epoch: 143, batch index: 2301, learning rate: [1.0000000000000005e-08], loss:2.254415512084961\n",
      "Epoch: 143, batch index: 2302, learning rate: [1.0000000000000005e-08], loss:2.250425338745117\n",
      "Epoch: 143, batch index: 2303, learning rate: [1.0000000000000005e-08], loss:2.280163288116455\n",
      "Epoch 143, validation accuracy score0.322265625\n",
      "Epoch: 144, batch index: 2304, learning rate: [1.0000000000000005e-08], loss:2.273714065551758\n",
      "Epoch: 144, batch index: 2305, learning rate: [1.0000000000000005e-08], loss:2.255415916442871\n",
      "Epoch: 144, batch index: 2306, learning rate: [1.0000000000000005e-08], loss:2.2749099731445312\n",
      "Epoch: 144, batch index: 2307, learning rate: [1.0000000000000005e-08], loss:2.272045612335205\n",
      "Epoch: 144, batch index: 2308, learning rate: [1.0000000000000005e-08], loss:2.2674477100372314\n",
      "Epoch: 144, batch index: 2309, learning rate: [1.0000000000000005e-08], loss:2.283365488052368\n",
      "Epoch: 144, batch index: 2310, learning rate: [1.0000000000000005e-08], loss:2.2882213592529297\n",
      "Epoch: 144, batch index: 2311, learning rate: [1.0000000000000005e-08], loss:2.294250249862671\n",
      "Epoch: 144, batch index: 2312, learning rate: [1.0000000000000005e-08], loss:2.263671875\n",
      "Epoch: 144, batch index: 2313, learning rate: [1.0000000000000005e-08], loss:2.2742109298706055\n",
      "Epoch: 144, batch index: 2314, learning rate: [1.0000000000000005e-08], loss:2.229160785675049\n",
      "Epoch: 144, batch index: 2315, learning rate: [1.0000000000000005e-08], loss:2.263505458831787\n",
      "Epoch: 144, batch index: 2316, learning rate: [1.0000000000000005e-08], loss:2.2968685626983643\n",
      "Epoch: 144, batch index: 2317, learning rate: [1.0000000000000005e-08], loss:2.3029162883758545\n",
      "Epoch: 144, batch index: 2318, learning rate: [1.0000000000000005e-08], loss:2.2693238258361816\n",
      "Epoch: 144, batch index: 2319, learning rate: [1.0000000000000005e-08], loss:2.297400951385498\n",
      "Epoch 144, validation accuracy score0.361328125\n",
      "Epoch: 145, batch index: 2320, learning rate: [1.0000000000000005e-08], loss:2.228041887283325\n",
      "Epoch: 145, batch index: 2321, learning rate: [1.0000000000000005e-08], loss:2.2568130493164062\n",
      "Epoch: 145, batch index: 2322, learning rate: [1.0000000000000005e-08], loss:2.264660120010376\n",
      "Epoch: 145, batch index: 2323, learning rate: [1.0000000000000005e-08], loss:2.301072597503662\n",
      "Epoch: 145, batch index: 2324, learning rate: [1.0000000000000005e-08], loss:2.2454051971435547\n",
      "Epoch: 145, batch index: 2325, learning rate: [1.0000000000000005e-08], loss:2.27510666847229\n",
      "Epoch: 145, batch index: 2326, learning rate: [1.0000000000000005e-08], loss:2.248011350631714\n",
      "Epoch: 145, batch index: 2327, learning rate: [1.0000000000000005e-08], loss:2.2637012004852295\n",
      "Epoch: 145, batch index: 2328, learning rate: [1.0000000000000005e-08], loss:2.272038459777832\n",
      "Epoch: 145, batch index: 2329, learning rate: [1.0000000000000005e-08], loss:2.2730443477630615\n",
      "Epoch: 145, batch index: 2330, learning rate: [1.0000000000000005e-08], loss:2.2696502208709717\n",
      "Epoch: 145, batch index: 2331, learning rate: [1.0000000000000005e-08], loss:2.2684991359710693\n",
      "Epoch: 145, batch index: 2332, learning rate: [1.0000000000000005e-08], loss:2.2850208282470703\n",
      "Epoch: 145, batch index: 2333, learning rate: [1.0000000000000005e-08], loss:2.2641875743865967\n",
      "Epoch: 145, batch index: 2334, learning rate: [1.0000000000000005e-08], loss:2.253342866897583\n",
      "Epoch: 145, batch index: 2335, learning rate: [1.0000000000000005e-08], loss:2.2485992908477783\n",
      "Epoch 145, validation accuracy score0.326171875\n",
      "Epoch: 146, batch index: 2336, learning rate: [1.0000000000000005e-08], loss:2.2659730911254883\n",
      "Epoch: 146, batch index: 2337, learning rate: [1.0000000000000005e-08], loss:2.273456335067749\n",
      "Epoch: 146, batch index: 2338, learning rate: [1.0000000000000005e-08], loss:2.3009188175201416\n",
      "Epoch: 146, batch index: 2339, learning rate: [1.0000000000000005e-08], loss:2.2576024532318115\n",
      "Epoch: 146, batch index: 2340, learning rate: [1.0000000000000005e-08], loss:2.257507562637329\n",
      "Epoch: 146, batch index: 2341, learning rate: [1.0000000000000005e-08], loss:2.2449138164520264\n",
      "Epoch: 146, batch index: 2342, learning rate: [1.0000000000000005e-08], loss:2.279146432876587\n",
      "Epoch: 146, batch index: 2343, learning rate: [1.0000000000000005e-08], loss:2.257195472717285\n",
      "Epoch: 146, batch index: 2344, learning rate: [1.0000000000000005e-08], loss:2.278665542602539\n",
      "Epoch: 146, batch index: 2345, learning rate: [1.0000000000000005e-08], loss:2.25042724609375\n",
      "Epoch: 146, batch index: 2346, learning rate: [1.0000000000000005e-08], loss:2.3073997497558594\n",
      "Epoch: 146, batch index: 2347, learning rate: [1.0000000000000005e-08], loss:2.265646457672119\n",
      "Epoch: 146, batch index: 2348, learning rate: [1.0000000000000005e-08], loss:2.288654327392578\n",
      "Epoch: 146, batch index: 2349, learning rate: [1.0000000000000005e-08], loss:2.2957348823547363\n",
      "Epoch: 146, batch index: 2350, learning rate: [1.0000000000000005e-08], loss:2.241953134536743\n",
      "Epoch: 146, batch index: 2351, learning rate: [1.0000000000000005e-08], loss:2.283673048019409\n",
      "Epoch 146, validation accuracy score0.322265625\n",
      "Epoch: 147, batch index: 2352, learning rate: [1.0000000000000005e-08], loss:2.3073761463165283\n",
      "Epoch: 147, batch index: 2353, learning rate: [1.0000000000000005e-08], loss:2.2638509273529053\n",
      "Epoch: 147, batch index: 2354, learning rate: [1.0000000000000005e-08], loss:2.2222182750701904\n",
      "Epoch: 147, batch index: 2355, learning rate: [1.0000000000000005e-08], loss:2.265296220779419\n",
      "Epoch: 147, batch index: 2356, learning rate: [1.0000000000000005e-08], loss:2.2954673767089844\n",
      "Epoch: 147, batch index: 2357, learning rate: [1.0000000000000005e-08], loss:2.236344575881958\n",
      "Epoch: 147, batch index: 2358, learning rate: [1.0000000000000005e-08], loss:2.293757915496826\n",
      "Epoch: 147, batch index: 2359, learning rate: [1.0000000000000005e-08], loss:2.2873375415802\n",
      "Epoch: 147, batch index: 2360, learning rate: [1.0000000000000005e-08], loss:2.2553024291992188\n",
      "Epoch: 147, batch index: 2361, learning rate: [1.0000000000000005e-08], loss:2.2759077548980713\n",
      "Epoch: 147, batch index: 2362, learning rate: [1.0000000000000005e-08], loss:2.2771732807159424\n",
      "Epoch: 147, batch index: 2363, learning rate: [1.0000000000000005e-08], loss:2.2568893432617188\n",
      "Epoch: 147, batch index: 2364, learning rate: [1.0000000000000005e-08], loss:2.296440362930298\n",
      "Epoch: 147, batch index: 2365, learning rate: [1.0000000000000005e-08], loss:2.311589241027832\n",
      "Epoch: 147, batch index: 2366, learning rate: [1.0000000000000005e-08], loss:2.2557880878448486\n",
      "Epoch: 147, batch index: 2367, learning rate: [1.0000000000000005e-08], loss:2.250946283340454\n",
      "Epoch 147, validation accuracy score0.30078125\n",
      "Epoch: 148, batch index: 2368, learning rate: [1.0000000000000005e-08], loss:2.2555172443389893\n",
      "Epoch: 148, batch index: 2369, learning rate: [1.0000000000000005e-08], loss:2.263493776321411\n",
      "Epoch: 148, batch index: 2370, learning rate: [1.0000000000000005e-08], loss:2.3021507263183594\n",
      "Epoch: 148, batch index: 2371, learning rate: [1.0000000000000005e-08], loss:2.233171224594116\n",
      "Epoch: 148, batch index: 2372, learning rate: [1.0000000000000005e-08], loss:2.249042510986328\n",
      "Epoch: 148, batch index: 2373, learning rate: [1.0000000000000005e-08], loss:2.28313946723938\n",
      "Epoch: 148, batch index: 2374, learning rate: [1.0000000000000005e-08], loss:2.268486499786377\n",
      "Epoch: 148, batch index: 2375, learning rate: [1.0000000000000005e-08], loss:2.2858166694641113\n",
      "Epoch: 148, batch index: 2376, learning rate: [1.0000000000000005e-08], loss:2.309246301651001\n",
      "Epoch: 148, batch index: 2377, learning rate: [1.0000000000000005e-08], loss:2.2780442237854004\n",
      "Epoch: 148, batch index: 2378, learning rate: [1.0000000000000005e-08], loss:2.2585504055023193\n",
      "Epoch: 148, batch index: 2379, learning rate: [1.0000000000000005e-08], loss:2.2740421295166016\n",
      "Epoch: 148, batch index: 2380, learning rate: [1.0000000000000005e-08], loss:2.2518842220306396\n",
      "Epoch: 148, batch index: 2381, learning rate: [1.0000000000000005e-08], loss:2.2598676681518555\n",
      "Epoch: 148, batch index: 2382, learning rate: [1.0000000000000005e-08], loss:2.239948272705078\n",
      "Epoch: 148, batch index: 2383, learning rate: [1.0000000000000005e-08], loss:2.2577362060546875\n",
      "Epoch 148, validation accuracy score0.275390625\n",
      "Epoch: 149, batch index: 2384, learning rate: [1.0000000000000005e-08], loss:2.2495882511138916\n",
      "Epoch: 149, batch index: 2385, learning rate: [1.0000000000000005e-08], loss:2.252143144607544\n",
      "Epoch: 149, batch index: 2386, learning rate: [1.0000000000000005e-08], loss:2.233074903488159\n",
      "Epoch: 149, batch index: 2387, learning rate: [1.0000000000000005e-08], loss:2.2552738189697266\n",
      "Epoch: 149, batch index: 2388, learning rate: [1.0000000000000005e-08], loss:2.24284029006958\n",
      "Epoch: 149, batch index: 2389, learning rate: [1.0000000000000005e-08], loss:2.2698190212249756\n",
      "Epoch: 149, batch index: 2390, learning rate: [1.0000000000000005e-08], loss:2.256709337234497\n",
      "Epoch: 149, batch index: 2391, learning rate: [1.0000000000000005e-08], loss:2.2893552780151367\n",
      "Epoch: 149, batch index: 2392, learning rate: [1.0000000000000005e-08], loss:2.2331297397613525\n",
      "Epoch: 149, batch index: 2393, learning rate: [1.0000000000000005e-08], loss:2.260683298110962\n",
      "Epoch: 149, batch index: 2394, learning rate: [1.0000000000000005e-08], loss:2.2733707427978516\n",
      "Epoch: 149, batch index: 2395, learning rate: [1.0000000000000005e-08], loss:2.269887924194336\n",
      "Epoch: 149, batch index: 2396, learning rate: [1.0000000000000005e-08], loss:2.2515194416046143\n",
      "Epoch: 149, batch index: 2397, learning rate: [1.0000000000000005e-08], loss:2.2746944427490234\n",
      "Epoch: 149, batch index: 2398, learning rate: [1.0000000000000005e-08], loss:2.2936654090881348\n",
      "Epoch: 149, batch index: 2399, learning rate: [1.0000000000000005e-08], loss:2.3158767223358154\n",
      "Epoch 149, validation accuracy score0.326171875\n",
      "Epoch: 150, batch index: 2400, learning rate: [1.0000000000000005e-08], loss:2.265831470489502\n",
      "Epoch: 150, batch index: 2401, learning rate: [1.0000000000000005e-08], loss:2.231612205505371\n",
      "Epoch: 150, batch index: 2402, learning rate: [1.0000000000000005e-08], loss:2.2293283939361572\n",
      "Epoch: 150, batch index: 2403, learning rate: [1.0000000000000005e-08], loss:2.2561700344085693\n",
      "Epoch: 150, batch index: 2404, learning rate: [1.0000000000000005e-08], loss:2.2691092491149902\n",
      "Epoch: 150, batch index: 2405, learning rate: [1.0000000000000005e-08], loss:2.292449712753296\n",
      "Epoch: 150, batch index: 2406, learning rate: [1.0000000000000005e-08], loss:2.2885074615478516\n",
      "Epoch: 150, batch index: 2407, learning rate: [1.0000000000000005e-08], loss:2.2597837448120117\n",
      "Epoch: 150, batch index: 2408, learning rate: [1.0000000000000005e-08], loss:2.290727376937866\n",
      "Epoch: 150, batch index: 2409, learning rate: [1.0000000000000005e-08], loss:2.2373745441436768\n",
      "Epoch: 150, batch index: 2410, learning rate: [1.0000000000000005e-08], loss:2.264540195465088\n",
      "Epoch: 150, batch index: 2411, learning rate: [1.0000000000000005e-08], loss:2.271695852279663\n",
      "Epoch: 150, batch index: 2412, learning rate: [1.0000000000000005e-08], loss:2.2896268367767334\n",
      "Epoch: 150, batch index: 2413, learning rate: [1.0000000000000005e-08], loss:2.2879350185394287\n",
      "Epoch: 150, batch index: 2414, learning rate: [1.0000000000000005e-08], loss:2.2434375286102295\n",
      "Epoch: 150, batch index: 2415, learning rate: [1.0000000000000005e-08], loss:2.257708787918091\n",
      "Epoch 150, validation accuracy score0.333984375\n",
      "0\n",
      "Epoch: 151, batch index: 2416, learning rate: [1.0000000000000005e-08], loss:2.2689402103424072\n",
      "Epoch: 151, batch index: 2417, learning rate: [1.0000000000000005e-08], loss:2.305638074874878\n",
      "Epoch: 151, batch index: 2418, learning rate: [1.0000000000000005e-08], loss:2.2898809909820557\n",
      "Epoch: 151, batch index: 2419, learning rate: [1.0000000000000005e-08], loss:2.256179094314575\n",
      "Epoch: 151, batch index: 2420, learning rate: [1.0000000000000005e-08], loss:2.269469738006592\n",
      "Epoch: 151, batch index: 2421, learning rate: [1.0000000000000005e-08], loss:2.3068718910217285\n",
      "Epoch: 151, batch index: 2422, learning rate: [1.0000000000000005e-08], loss:2.2381606101989746\n",
      "Epoch: 151, batch index: 2423, learning rate: [1.0000000000000005e-08], loss:2.267791748046875\n",
      "Epoch: 151, batch index: 2424, learning rate: [1.0000000000000005e-08], loss:2.235269546508789\n",
      "Epoch: 151, batch index: 2425, learning rate: [1.0000000000000005e-08], loss:2.276805877685547\n",
      "Epoch: 151, batch index: 2426, learning rate: [1.0000000000000005e-08], loss:2.2403244972229004\n",
      "Epoch: 151, batch index: 2427, learning rate: [1.0000000000000005e-08], loss:2.25773286819458\n",
      "Epoch: 151, batch index: 2428, learning rate: [1.0000000000000005e-08], loss:2.2538139820098877\n",
      "Epoch: 151, batch index: 2429, learning rate: [1.0000000000000005e-08], loss:2.2873265743255615\n",
      "Epoch: 151, batch index: 2430, learning rate: [1.0000000000000005e-08], loss:2.2889676094055176\n",
      "Epoch: 151, batch index: 2431, learning rate: [1.0000000000000005e-08], loss:2.27294659614563\n",
      "Epoch 151, validation accuracy score0.31640625\n",
      "Epoch: 152, batch index: 2432, learning rate: [1.0000000000000005e-08], loss:2.249302625656128\n",
      "Epoch: 152, batch index: 2433, learning rate: [1.0000000000000005e-08], loss:2.2951478958129883\n",
      "Epoch: 152, batch index: 2434, learning rate: [1.0000000000000005e-08], loss:2.265943765640259\n",
      "Epoch: 152, batch index: 2435, learning rate: [1.0000000000000005e-08], loss:2.22627854347229\n",
      "Epoch: 152, batch index: 2436, learning rate: [1.0000000000000005e-08], loss:2.280160427093506\n",
      "Epoch: 152, batch index: 2437, learning rate: [1.0000000000000005e-08], loss:2.270634889602661\n",
      "Epoch: 152, batch index: 2438, learning rate: [1.0000000000000005e-08], loss:2.2320120334625244\n",
      "Epoch: 152, batch index: 2439, learning rate: [1.0000000000000005e-08], loss:2.2838916778564453\n",
      "Epoch: 152, batch index: 2440, learning rate: [1.0000000000000005e-08], loss:2.283842086791992\n",
      "Epoch: 152, batch index: 2441, learning rate: [1.0000000000000005e-08], loss:2.2629411220550537\n",
      "Epoch: 152, batch index: 2442, learning rate: [1.0000000000000005e-08], loss:2.278076648712158\n",
      "Epoch: 152, batch index: 2443, learning rate: [1.0000000000000005e-08], loss:2.273336410522461\n",
      "Epoch: 152, batch index: 2444, learning rate: [1.0000000000000005e-08], loss:2.3010878562927246\n",
      "Epoch: 152, batch index: 2445, learning rate: [1.0000000000000005e-08], loss:2.2786359786987305\n",
      "Epoch: 152, batch index: 2446, learning rate: [1.0000000000000005e-08], loss:2.2898166179656982\n",
      "Epoch: 152, batch index: 2447, learning rate: [1.0000000000000005e-08], loss:2.2518868446350098\n",
      "Epoch 152, validation accuracy score0.326171875\n",
      "Epoch: 153, batch index: 2448, learning rate: [1.0000000000000005e-08], loss:2.2691586017608643\n",
      "Epoch: 153, batch index: 2449, learning rate: [1.0000000000000005e-08], loss:2.2464728355407715\n",
      "Epoch: 153, batch index: 2450, learning rate: [1.0000000000000005e-08], loss:2.2682411670684814\n",
      "Epoch: 153, batch index: 2451, learning rate: [1.0000000000000005e-08], loss:2.3089940547943115\n",
      "Epoch: 153, batch index: 2452, learning rate: [1.0000000000000005e-08], loss:2.2274491786956787\n",
      "Epoch: 153, batch index: 2453, learning rate: [1.0000000000000005e-08], loss:2.3148367404937744\n",
      "Epoch: 153, batch index: 2454, learning rate: [1.0000000000000005e-08], loss:2.261806011199951\n",
      "Epoch: 153, batch index: 2455, learning rate: [1.0000000000000005e-08], loss:2.24625825881958\n",
      "Epoch: 153, batch index: 2456, learning rate: [1.0000000000000005e-08], loss:2.2333149909973145\n",
      "Epoch: 153, batch index: 2457, learning rate: [1.0000000000000005e-08], loss:2.2712695598602295\n",
      "Epoch: 153, batch index: 2458, learning rate: [1.0000000000000005e-08], loss:2.2921254634857178\n",
      "Epoch: 153, batch index: 2459, learning rate: [1.0000000000000005e-08], loss:2.260796308517456\n",
      "Epoch: 153, batch index: 2460, learning rate: [1.0000000000000005e-08], loss:2.2755651473999023\n",
      "Epoch: 153, batch index: 2461, learning rate: [1.0000000000000005e-08], loss:2.247227907180786\n",
      "Epoch: 153, batch index: 2462, learning rate: [1.0000000000000005e-08], loss:2.256342887878418\n",
      "Epoch: 153, batch index: 2463, learning rate: [1.0000000000000005e-08], loss:2.349609613418579\n",
      "Epoch 153, validation accuracy score0.32421875\n",
      "Epoch: 154, batch index: 2464, learning rate: [1.0000000000000005e-08], loss:2.301558256149292\n",
      "Epoch: 154, batch index: 2465, learning rate: [1.0000000000000005e-08], loss:2.2871978282928467\n",
      "Epoch: 154, batch index: 2466, learning rate: [1.0000000000000005e-08], loss:2.2554564476013184\n",
      "Epoch: 154, batch index: 2467, learning rate: [1.0000000000000005e-08], loss:2.2852556705474854\n",
      "Epoch: 154, batch index: 2468, learning rate: [1.0000000000000005e-08], loss:2.2595229148864746\n",
      "Epoch: 154, batch index: 2469, learning rate: [1.0000000000000005e-08], loss:2.3176093101501465\n",
      "Epoch: 154, batch index: 2470, learning rate: [1.0000000000000005e-08], loss:2.2547500133514404\n",
      "Epoch: 154, batch index: 2471, learning rate: [1.0000000000000005e-08], loss:2.2398018836975098\n",
      "Epoch: 154, batch index: 2472, learning rate: [1.0000000000000005e-08], loss:2.2849981784820557\n",
      "Epoch: 154, batch index: 2473, learning rate: [1.0000000000000005e-08], loss:2.290478229522705\n",
      "Epoch: 154, batch index: 2474, learning rate: [1.0000000000000005e-08], loss:2.225656270980835\n",
      "Epoch: 154, batch index: 2475, learning rate: [1.0000000000000005e-08], loss:2.239337682723999\n",
      "Epoch: 154, batch index: 2476, learning rate: [1.0000000000000005e-08], loss:2.2720072269439697\n",
      "Epoch: 154, batch index: 2477, learning rate: [1.0000000000000005e-08], loss:2.2476446628570557\n",
      "Epoch: 154, batch index: 2478, learning rate: [1.0000000000000005e-08], loss:2.2912955284118652\n",
      "Epoch: 154, batch index: 2479, learning rate: [1.0000000000000005e-08], loss:2.257702589035034\n",
      "Epoch 154, validation accuracy score0.3125\n",
      "Epoch: 155, batch index: 2480, learning rate: [1.0000000000000005e-08], loss:2.2976298332214355\n",
      "Epoch: 155, batch index: 2481, learning rate: [1.0000000000000005e-08], loss:2.2732861042022705\n",
      "Epoch: 155, batch index: 2482, learning rate: [1.0000000000000005e-08], loss:2.2776994705200195\n",
      "Epoch: 155, batch index: 2483, learning rate: [1.0000000000000005e-08], loss:2.288264513015747\n",
      "Epoch: 155, batch index: 2484, learning rate: [1.0000000000000005e-08], loss:2.265207052230835\n",
      "Epoch: 155, batch index: 2485, learning rate: [1.0000000000000005e-08], loss:2.275653839111328\n",
      "Epoch: 155, batch index: 2486, learning rate: [1.0000000000000005e-08], loss:2.2702291011810303\n",
      "Epoch: 155, batch index: 2487, learning rate: [1.0000000000000005e-08], loss:2.248674154281616\n",
      "Epoch: 155, batch index: 2488, learning rate: [1.0000000000000005e-08], loss:2.277916669845581\n",
      "Epoch: 155, batch index: 2489, learning rate: [1.0000000000000005e-08], loss:2.302295207977295\n",
      "Epoch: 155, batch index: 2490, learning rate: [1.0000000000000005e-08], loss:2.262889862060547\n",
      "Epoch: 155, batch index: 2491, learning rate: [1.0000000000000005e-08], loss:2.269052028656006\n",
      "Epoch: 155, batch index: 2492, learning rate: [1.0000000000000005e-08], loss:2.23063063621521\n",
      "Epoch: 155, batch index: 2493, learning rate: [1.0000000000000005e-08], loss:2.284672737121582\n",
      "Epoch: 155, batch index: 2494, learning rate: [1.0000000000000005e-08], loss:2.254014015197754\n",
      "Epoch: 155, batch index: 2495, learning rate: [1.0000000000000005e-08], loss:2.259885787963867\n",
      "Epoch 155, validation accuracy score0.3203125\n",
      "Epoch: 156, batch index: 2496, learning rate: [1.0000000000000005e-08], loss:2.2894070148468018\n",
      "Epoch: 156, batch index: 2497, learning rate: [1.0000000000000005e-08], loss:2.3076252937316895\n",
      "Epoch: 156, batch index: 2498, learning rate: [1.0000000000000005e-08], loss:2.253291606903076\n",
      "Epoch: 156, batch index: 2499, learning rate: [1.0000000000000005e-08], loss:2.2596144676208496\n",
      "Epoch: 156, batch index: 2500, learning rate: [1.0000000000000005e-08], loss:2.239837884902954\n",
      "Epoch: 156, batch index: 2501, learning rate: [1.0000000000000005e-08], loss:2.2424070835113525\n",
      "Epoch: 156, batch index: 2502, learning rate: [1.0000000000000005e-08], loss:2.2395336627960205\n",
      "Epoch: 156, batch index: 2503, learning rate: [1.0000000000000005e-08], loss:2.3074541091918945\n",
      "Epoch: 156, batch index: 2504, learning rate: [1.0000000000000005e-08], loss:2.2486696243286133\n",
      "Epoch: 156, batch index: 2505, learning rate: [1.0000000000000005e-08], loss:2.3003764152526855\n",
      "Epoch: 156, batch index: 2506, learning rate: [1.0000000000000005e-08], loss:2.2785534858703613\n",
      "Epoch: 156, batch index: 2507, learning rate: [1.0000000000000005e-08], loss:2.2401340007781982\n",
      "Epoch: 156, batch index: 2508, learning rate: [1.0000000000000005e-08], loss:2.276031017303467\n",
      "Epoch: 156, batch index: 2509, learning rate: [1.0000000000000005e-08], loss:2.2664952278137207\n",
      "Epoch: 156, batch index: 2510, learning rate: [1.0000000000000005e-08], loss:2.3196070194244385\n",
      "Epoch: 156, batch index: 2511, learning rate: [1.0000000000000005e-08], loss:2.228929042816162\n",
      "Epoch 156, validation accuracy score0.328125\n",
      "Epoch: 157, batch index: 2512, learning rate: [1.0000000000000005e-08], loss:2.2548911571502686\n",
      "Epoch: 157, batch index: 2513, learning rate: [1.0000000000000005e-08], loss:2.2838518619537354\n",
      "Epoch: 157, batch index: 2514, learning rate: [1.0000000000000005e-08], loss:2.28277850151062\n",
      "Epoch: 157, batch index: 2515, learning rate: [1.0000000000000005e-08], loss:2.2966818809509277\n",
      "Epoch: 157, batch index: 2516, learning rate: [1.0000000000000005e-08], loss:2.292099952697754\n",
      "Epoch: 157, batch index: 2517, learning rate: [1.0000000000000005e-08], loss:2.276566505432129\n",
      "Epoch: 157, batch index: 2518, learning rate: [1.0000000000000005e-08], loss:2.29316782951355\n",
      "Epoch: 157, batch index: 2519, learning rate: [1.0000000000000005e-08], loss:2.2649827003479004\n",
      "Epoch: 157, batch index: 2520, learning rate: [1.0000000000000005e-08], loss:2.2056665420532227\n",
      "Epoch: 157, batch index: 2521, learning rate: [1.0000000000000005e-08], loss:2.2237601280212402\n",
      "Epoch: 157, batch index: 2522, learning rate: [1.0000000000000005e-08], loss:2.2565014362335205\n",
      "Epoch: 157, batch index: 2523, learning rate: [1.0000000000000005e-08], loss:2.2888412475585938\n",
      "Epoch: 157, batch index: 2524, learning rate: [1.0000000000000005e-08], loss:2.2912323474884033\n",
      "Epoch: 157, batch index: 2525, learning rate: [1.0000000000000005e-08], loss:2.3011462688446045\n",
      "Epoch: 157, batch index: 2526, learning rate: [1.0000000000000005e-08], loss:2.281283140182495\n",
      "Epoch: 157, batch index: 2527, learning rate: [1.0000000000000005e-08], loss:2.2327723503112793\n",
      "Epoch 157, validation accuracy score0.326171875\n",
      "Epoch: 158, batch index: 2528, learning rate: [1.0000000000000005e-08], loss:2.303330659866333\n",
      "Epoch: 158, batch index: 2529, learning rate: [1.0000000000000005e-08], loss:2.281038284301758\n",
      "Epoch: 158, batch index: 2530, learning rate: [1.0000000000000005e-08], loss:2.2304608821868896\n",
      "Epoch: 158, batch index: 2531, learning rate: [1.0000000000000005e-08], loss:2.286353349685669\n",
      "Epoch: 158, batch index: 2532, learning rate: [1.0000000000000005e-08], loss:2.29875111579895\n",
      "Epoch: 158, batch index: 2533, learning rate: [1.0000000000000005e-08], loss:2.254019021987915\n",
      "Epoch: 158, batch index: 2534, learning rate: [1.0000000000000005e-08], loss:2.2261104583740234\n",
      "Epoch: 158, batch index: 2535, learning rate: [1.0000000000000005e-08], loss:2.242932081222534\n",
      "Epoch: 158, batch index: 2536, learning rate: [1.0000000000000005e-08], loss:2.2178008556365967\n",
      "Epoch: 158, batch index: 2537, learning rate: [1.0000000000000005e-08], loss:2.239844799041748\n",
      "Epoch: 158, batch index: 2538, learning rate: [1.0000000000000005e-08], loss:2.2807860374450684\n",
      "Epoch: 158, batch index: 2539, learning rate: [1.0000000000000005e-08], loss:2.3108086585998535\n",
      "Epoch: 158, batch index: 2540, learning rate: [1.0000000000000005e-08], loss:2.252478837966919\n",
      "Epoch: 158, batch index: 2541, learning rate: [1.0000000000000005e-08], loss:2.255910634994507\n",
      "Epoch: 158, batch index: 2542, learning rate: [1.0000000000000005e-08], loss:2.314177989959717\n",
      "Epoch: 158, batch index: 2543, learning rate: [1.0000000000000005e-08], loss:2.2676172256469727\n",
      "Epoch 158, validation accuracy score0.333984375\n",
      "Epoch: 159, batch index: 2544, learning rate: [1.0000000000000005e-08], loss:2.2282729148864746\n",
      "Epoch: 159, batch index: 2545, learning rate: [1.0000000000000005e-08], loss:2.245008707046509\n",
      "Epoch: 159, batch index: 2546, learning rate: [1.0000000000000005e-08], loss:2.2819395065307617\n",
      "Epoch: 159, batch index: 2547, learning rate: [1.0000000000000005e-08], loss:2.293548822402954\n",
      "Epoch: 159, batch index: 2548, learning rate: [1.0000000000000005e-08], loss:2.2871925830841064\n",
      "Epoch: 159, batch index: 2549, learning rate: [1.0000000000000005e-08], loss:2.2416183948516846\n",
      "Epoch: 159, batch index: 2550, learning rate: [1.0000000000000005e-08], loss:2.2862086296081543\n",
      "Epoch: 159, batch index: 2551, learning rate: [1.0000000000000005e-08], loss:2.2416598796844482\n",
      "Epoch: 159, batch index: 2552, learning rate: [1.0000000000000005e-08], loss:2.232724666595459\n",
      "Epoch: 159, batch index: 2553, learning rate: [1.0000000000000005e-08], loss:2.257736921310425\n",
      "Epoch: 159, batch index: 2554, learning rate: [1.0000000000000005e-08], loss:2.2898812294006348\n",
      "Epoch: 159, batch index: 2555, learning rate: [1.0000000000000005e-08], loss:2.283550500869751\n",
      "Epoch: 159, batch index: 2556, learning rate: [1.0000000000000005e-08], loss:2.2835686206817627\n",
      "Epoch: 159, batch index: 2557, learning rate: [1.0000000000000005e-08], loss:2.2434868812561035\n",
      "Epoch: 159, batch index: 2558, learning rate: [1.0000000000000005e-08], loss:2.2437000274658203\n",
      "Epoch: 159, batch index: 2559, learning rate: [1.0000000000000005e-08], loss:2.308307409286499\n",
      "Epoch 159, validation accuracy score0.34765625\n",
      "Epoch: 160, batch index: 2560, learning rate: [1.0000000000000005e-08], loss:2.257561445236206\n",
      "Epoch: 160, batch index: 2561, learning rate: [1.0000000000000005e-08], loss:2.264266014099121\n",
      "Epoch: 160, batch index: 2562, learning rate: [1.0000000000000005e-08], loss:2.290271043777466\n",
      "Epoch: 160, batch index: 2563, learning rate: [1.0000000000000005e-08], loss:2.282914161682129\n",
      "Epoch: 160, batch index: 2564, learning rate: [1.0000000000000005e-08], loss:2.259838581085205\n",
      "Epoch: 160, batch index: 2565, learning rate: [1.0000000000000005e-08], loss:2.293365955352783\n",
      "Epoch: 160, batch index: 2566, learning rate: [1.0000000000000005e-08], loss:2.2653775215148926\n",
      "Epoch: 160, batch index: 2567, learning rate: [1.0000000000000005e-08], loss:2.2628514766693115\n",
      "Epoch: 160, batch index: 2568, learning rate: [1.0000000000000005e-08], loss:2.2860219478607178\n",
      "Epoch: 160, batch index: 2569, learning rate: [1.0000000000000005e-08], loss:2.2613813877105713\n",
      "Epoch: 160, batch index: 2570, learning rate: [1.0000000000000005e-08], loss:2.253997802734375\n",
      "Epoch: 160, batch index: 2571, learning rate: [1.0000000000000005e-08], loss:2.2710912227630615\n",
      "Epoch: 160, batch index: 2572, learning rate: [1.0000000000000005e-08], loss:2.296417474746704\n",
      "Epoch: 160, batch index: 2573, learning rate: [1.0000000000000005e-08], loss:2.2723593711853027\n",
      "Epoch: 160, batch index: 2574, learning rate: [1.0000000000000005e-08], loss:2.288564443588257\n",
      "Epoch: 160, batch index: 2575, learning rate: [1.0000000000000005e-08], loss:2.2398509979248047\n",
      "Epoch 160, validation accuracy score0.298828125\n",
      "Epoch: 161, batch index: 2576, learning rate: [1.0000000000000005e-08], loss:2.2549116611480713\n",
      "Epoch: 161, batch index: 2577, learning rate: [1.0000000000000005e-08], loss:2.2785732746124268\n",
      "Epoch: 161, batch index: 2578, learning rate: [1.0000000000000005e-08], loss:2.2722361087799072\n",
      "Epoch: 161, batch index: 2579, learning rate: [1.0000000000000005e-08], loss:2.266879081726074\n",
      "Epoch: 161, batch index: 2580, learning rate: [1.0000000000000005e-08], loss:2.2754578590393066\n",
      "Epoch: 161, batch index: 2581, learning rate: [1.0000000000000005e-08], loss:2.2981338500976562\n",
      "Epoch: 161, batch index: 2582, learning rate: [1.0000000000000005e-08], loss:2.233783483505249\n",
      "Epoch: 161, batch index: 2583, learning rate: [1.0000000000000005e-08], loss:2.263021230697632\n",
      "Epoch: 161, batch index: 2584, learning rate: [1.0000000000000005e-08], loss:2.2944164276123047\n",
      "Epoch: 161, batch index: 2585, learning rate: [1.0000000000000005e-08], loss:2.257375478744507\n",
      "Epoch: 161, batch index: 2586, learning rate: [1.0000000000000005e-08], loss:2.2796099185943604\n",
      "Epoch: 161, batch index: 2587, learning rate: [1.0000000000000005e-08], loss:2.239140748977661\n",
      "Epoch: 161, batch index: 2588, learning rate: [1.0000000000000005e-08], loss:2.274402379989624\n",
      "Epoch: 161, batch index: 2589, learning rate: [1.0000000000000005e-08], loss:2.252000331878662\n",
      "Epoch: 161, batch index: 2590, learning rate: [1.0000000000000005e-08], loss:2.267596960067749\n",
      "Epoch: 161, batch index: 2591, learning rate: [1.0000000000000005e-08], loss:2.2480101585388184\n",
      "Epoch 161, validation accuracy score0.31640625\n",
      "Epoch: 162, batch index: 2592, learning rate: [1.0000000000000005e-08], loss:2.278794765472412\n",
      "Epoch: 162, batch index: 2593, learning rate: [1.0000000000000005e-08], loss:2.2415144443511963\n",
      "Epoch: 162, batch index: 2594, learning rate: [1.0000000000000005e-08], loss:2.2646231651306152\n",
      "Epoch: 162, batch index: 2595, learning rate: [1.0000000000000005e-08], loss:2.2598791122436523\n",
      "Epoch: 162, batch index: 2596, learning rate: [1.0000000000000005e-08], loss:2.2431211471557617\n",
      "Epoch: 162, batch index: 2597, learning rate: [1.0000000000000005e-08], loss:2.309743881225586\n",
      "Epoch: 162, batch index: 2598, learning rate: [1.0000000000000005e-08], loss:2.3073601722717285\n",
      "Epoch: 162, batch index: 2599, learning rate: [1.0000000000000005e-08], loss:2.2600607872009277\n",
      "Epoch: 162, batch index: 2600, learning rate: [1.0000000000000005e-08], loss:2.2758140563964844\n",
      "Epoch: 162, batch index: 2601, learning rate: [1.0000000000000005e-08], loss:2.290436029434204\n",
      "Epoch: 162, batch index: 2602, learning rate: [1.0000000000000005e-08], loss:2.242734432220459\n",
      "Epoch: 162, batch index: 2603, learning rate: [1.0000000000000005e-08], loss:2.270947217941284\n",
      "Epoch: 162, batch index: 2604, learning rate: [1.0000000000000005e-08], loss:2.269016981124878\n",
      "Epoch: 162, batch index: 2605, learning rate: [1.0000000000000005e-08], loss:2.2674739360809326\n",
      "Epoch: 162, batch index: 2606, learning rate: [1.0000000000000005e-08], loss:2.2418668270111084\n",
      "Epoch: 162, batch index: 2607, learning rate: [1.0000000000000005e-08], loss:2.2694785594940186\n",
      "Epoch 162, validation accuracy score0.310546875\n",
      "Epoch: 163, batch index: 2608, learning rate: [1.0000000000000005e-08], loss:2.269530773162842\n",
      "Epoch: 163, batch index: 2609, learning rate: [1.0000000000000005e-08], loss:2.2597968578338623\n",
      "Epoch: 163, batch index: 2610, learning rate: [1.0000000000000005e-08], loss:2.2985119819641113\n",
      "Epoch: 163, batch index: 2611, learning rate: [1.0000000000000005e-08], loss:2.2399754524230957\n",
      "Epoch: 163, batch index: 2612, learning rate: [1.0000000000000005e-08], loss:2.2400503158569336\n",
      "Epoch: 163, batch index: 2613, learning rate: [1.0000000000000005e-08], loss:2.2893571853637695\n",
      "Epoch: 163, batch index: 2614, learning rate: [1.0000000000000005e-08], loss:2.2929227352142334\n",
      "Epoch: 163, batch index: 2615, learning rate: [1.0000000000000005e-08], loss:2.2612357139587402\n",
      "Epoch: 163, batch index: 2616, learning rate: [1.0000000000000005e-08], loss:2.2583072185516357\n",
      "Epoch: 163, batch index: 2617, learning rate: [1.0000000000000005e-08], loss:2.2752184867858887\n",
      "Epoch: 163, batch index: 2618, learning rate: [1.0000000000000005e-08], loss:2.2710540294647217\n",
      "Epoch: 163, batch index: 2619, learning rate: [1.0000000000000005e-08], loss:2.2716941833496094\n",
      "Epoch: 163, batch index: 2620, learning rate: [1.0000000000000005e-08], loss:2.2594146728515625\n",
      "Epoch: 163, batch index: 2621, learning rate: [1.0000000000000005e-08], loss:2.280251979827881\n",
      "Epoch: 163, batch index: 2622, learning rate: [1.0000000000000005e-08], loss:2.329219341278076\n",
      "Epoch: 163, batch index: 2623, learning rate: [1.0000000000000005e-08], loss:2.2354021072387695\n",
      "Epoch 163, validation accuracy score0.306640625\n",
      "Epoch: 164, batch index: 2624, learning rate: [1.0000000000000005e-08], loss:2.251253128051758\n",
      "Epoch: 164, batch index: 2625, learning rate: [1.0000000000000005e-08], loss:2.2948811054229736\n",
      "Epoch: 164, batch index: 2626, learning rate: [1.0000000000000005e-08], loss:2.278301477432251\n",
      "Epoch: 164, batch index: 2627, learning rate: [1.0000000000000005e-08], loss:2.287179708480835\n",
      "Epoch: 164, batch index: 2628, learning rate: [1.0000000000000005e-08], loss:2.2589523792266846\n",
      "Epoch: 164, batch index: 2629, learning rate: [1.0000000000000005e-08], loss:2.2388064861297607\n",
      "Epoch: 164, batch index: 2630, learning rate: [1.0000000000000005e-08], loss:2.2563929557800293\n",
      "Epoch: 164, batch index: 2631, learning rate: [1.0000000000000005e-08], loss:2.2936346530914307\n",
      "Epoch: 164, batch index: 2632, learning rate: [1.0000000000000005e-08], loss:2.272726058959961\n",
      "Epoch: 164, batch index: 2633, learning rate: [1.0000000000000005e-08], loss:2.3040359020233154\n",
      "Epoch: 164, batch index: 2634, learning rate: [1.0000000000000005e-08], loss:2.2270638942718506\n",
      "Epoch: 164, batch index: 2635, learning rate: [1.0000000000000005e-08], loss:2.2685658931732178\n",
      "Epoch: 164, batch index: 2636, learning rate: [1.0000000000000005e-08], loss:2.2743313312530518\n",
      "Epoch: 164, batch index: 2637, learning rate: [1.0000000000000005e-08], loss:2.2547049522399902\n",
      "Epoch: 164, batch index: 2638, learning rate: [1.0000000000000005e-08], loss:2.3089616298675537\n",
      "Epoch: 164, batch index: 2639, learning rate: [1.0000000000000005e-08], loss:2.2236478328704834\n",
      "Epoch 164, validation accuracy score0.353515625\n",
      "Epoch: 165, batch index: 2640, learning rate: [1.0000000000000005e-08], loss:2.2575438022613525\n",
      "Epoch: 165, batch index: 2641, learning rate: [1.0000000000000005e-08], loss:2.2679989337921143\n",
      "Epoch: 165, batch index: 2642, learning rate: [1.0000000000000005e-08], loss:2.232344150543213\n",
      "Epoch: 165, batch index: 2643, learning rate: [1.0000000000000005e-08], loss:2.2626309394836426\n",
      "Epoch: 165, batch index: 2644, learning rate: [1.0000000000000005e-08], loss:2.2760775089263916\n",
      "Epoch: 165, batch index: 2645, learning rate: [1.0000000000000005e-08], loss:2.2620317935943604\n",
      "Epoch: 165, batch index: 2646, learning rate: [1.0000000000000005e-08], loss:2.282640218734741\n",
      "Epoch: 165, batch index: 2647, learning rate: [1.0000000000000005e-08], loss:2.2694153785705566\n",
      "Epoch: 165, batch index: 2648, learning rate: [1.0000000000000005e-08], loss:2.2754788398742676\n",
      "Epoch: 165, batch index: 2649, learning rate: [1.0000000000000005e-08], loss:2.237412214279175\n",
      "Epoch: 165, batch index: 2650, learning rate: [1.0000000000000005e-08], loss:2.3067264556884766\n",
      "Epoch: 165, batch index: 2651, learning rate: [1.0000000000000005e-08], loss:2.254131555557251\n",
      "Epoch: 165, batch index: 2652, learning rate: [1.0000000000000005e-08], loss:2.2585718631744385\n",
      "Epoch: 165, batch index: 2653, learning rate: [1.0000000000000005e-08], loss:2.292393684387207\n",
      "Epoch: 165, batch index: 2654, learning rate: [1.0000000000000005e-08], loss:2.2926697731018066\n",
      "Epoch: 165, batch index: 2655, learning rate: [1.0000000000000005e-08], loss:2.224168062210083\n",
      "Epoch 165, validation accuracy score0.34375\n",
      "Epoch: 166, batch index: 2656, learning rate: [1.0000000000000005e-08], loss:2.2683041095733643\n",
      "Epoch: 166, batch index: 2657, learning rate: [1.0000000000000005e-08], loss:2.262887954711914\n",
      "Epoch: 166, batch index: 2658, learning rate: [1.0000000000000005e-08], loss:2.243711471557617\n",
      "Epoch: 166, batch index: 2659, learning rate: [1.0000000000000005e-08], loss:2.2852861881256104\n",
      "Epoch: 166, batch index: 2660, learning rate: [1.0000000000000005e-08], loss:2.2507429122924805\n",
      "Epoch: 166, batch index: 2661, learning rate: [1.0000000000000005e-08], loss:2.2866811752319336\n",
      "Epoch: 166, batch index: 2662, learning rate: [1.0000000000000005e-08], loss:2.2607929706573486\n",
      "Epoch: 166, batch index: 2663, learning rate: [1.0000000000000005e-08], loss:2.246044874191284\n",
      "Epoch: 166, batch index: 2664, learning rate: [1.0000000000000005e-08], loss:2.277848482131958\n",
      "Epoch: 166, batch index: 2665, learning rate: [1.0000000000000005e-08], loss:2.297109365463257\n",
      "Epoch: 166, batch index: 2666, learning rate: [1.0000000000000005e-08], loss:2.263643503189087\n",
      "Epoch: 166, batch index: 2667, learning rate: [1.0000000000000005e-08], loss:2.3083484172821045\n",
      "Epoch: 166, batch index: 2668, learning rate: [1.0000000000000005e-08], loss:2.280790090560913\n",
      "Epoch: 166, batch index: 2669, learning rate: [1.0000000000000005e-08], loss:2.2572481632232666\n",
      "Epoch: 166, batch index: 2670, learning rate: [1.0000000000000005e-08], loss:2.2492432594299316\n",
      "Epoch: 166, batch index: 2671, learning rate: [1.0000000000000005e-08], loss:2.304086685180664\n",
      "Epoch 166, validation accuracy score0.314453125\n",
      "Epoch: 167, batch index: 2672, learning rate: [1.0000000000000005e-08], loss:2.279660940170288\n",
      "Epoch: 167, batch index: 2673, learning rate: [1.0000000000000005e-08], loss:2.2668824195861816\n",
      "Epoch: 167, batch index: 2674, learning rate: [1.0000000000000005e-08], loss:2.2289860248565674\n",
      "Epoch: 167, batch index: 2675, learning rate: [1.0000000000000005e-08], loss:2.2434418201446533\n",
      "Epoch: 167, batch index: 2676, learning rate: [1.0000000000000005e-08], loss:2.2614874839782715\n",
      "Epoch: 167, batch index: 2677, learning rate: [1.0000000000000005e-08], loss:2.241856336593628\n",
      "Epoch: 167, batch index: 2678, learning rate: [1.0000000000000005e-08], loss:2.2497708797454834\n",
      "Epoch: 167, batch index: 2679, learning rate: [1.0000000000000005e-08], loss:2.2583460807800293\n",
      "Epoch: 167, batch index: 2680, learning rate: [1.0000000000000005e-08], loss:2.2621066570281982\n",
      "Epoch: 167, batch index: 2681, learning rate: [1.0000000000000005e-08], loss:2.287400960922241\n",
      "Epoch: 167, batch index: 2682, learning rate: [1.0000000000000005e-08], loss:2.2902204990386963\n",
      "Epoch: 167, batch index: 2683, learning rate: [1.0000000000000005e-08], loss:2.2933123111724854\n",
      "Epoch: 167, batch index: 2684, learning rate: [1.0000000000000005e-08], loss:2.2798404693603516\n",
      "Epoch: 167, batch index: 2685, learning rate: [1.0000000000000005e-08], loss:2.289071559906006\n",
      "Epoch: 167, batch index: 2686, learning rate: [1.0000000000000005e-08], loss:2.2604176998138428\n",
      "Epoch: 167, batch index: 2687, learning rate: [1.0000000000000005e-08], loss:2.2660868167877197\n",
      "Epoch 167, validation accuracy score0.302734375\n",
      "Epoch: 168, batch index: 2688, learning rate: [1.0000000000000005e-08], loss:2.262951374053955\n",
      "Epoch: 168, batch index: 2689, learning rate: [1.0000000000000005e-08], loss:2.2980470657348633\n",
      "Epoch: 168, batch index: 2690, learning rate: [1.0000000000000005e-08], loss:2.2485382556915283\n",
      "Epoch: 168, batch index: 2691, learning rate: [1.0000000000000005e-08], loss:2.287557363510132\n",
      "Epoch: 168, batch index: 2692, learning rate: [1.0000000000000005e-08], loss:2.2469944953918457\n",
      "Epoch: 168, batch index: 2693, learning rate: [1.0000000000000005e-08], loss:2.27075457572937\n",
      "Epoch: 168, batch index: 2694, learning rate: [1.0000000000000005e-08], loss:2.2833826541900635\n",
      "Epoch: 168, batch index: 2695, learning rate: [1.0000000000000005e-08], loss:2.2619941234588623\n",
      "Epoch: 168, batch index: 2696, learning rate: [1.0000000000000005e-08], loss:2.2693722248077393\n",
      "Epoch: 168, batch index: 2697, learning rate: [1.0000000000000005e-08], loss:2.2730727195739746\n",
      "Epoch: 168, batch index: 2698, learning rate: [1.0000000000000005e-08], loss:2.294046401977539\n",
      "Epoch: 168, batch index: 2699, learning rate: [1.0000000000000005e-08], loss:2.2947194576263428\n",
      "Epoch: 168, batch index: 2700, learning rate: [1.0000000000000005e-08], loss:2.2904343605041504\n",
      "Epoch: 168, batch index: 2701, learning rate: [1.0000000000000005e-08], loss:2.269439697265625\n",
      "Epoch: 168, batch index: 2702, learning rate: [1.0000000000000005e-08], loss:2.2714109420776367\n",
      "Epoch: 168, batch index: 2703, learning rate: [1.0000000000000005e-08], loss:2.278564929962158\n",
      "Epoch 168, validation accuracy score0.318359375\n",
      "Epoch: 169, batch index: 2704, learning rate: [1.0000000000000005e-08], loss:2.310149908065796\n",
      "Epoch: 169, batch index: 2705, learning rate: [1.0000000000000005e-08], loss:2.2582972049713135\n",
      "Epoch: 169, batch index: 2706, learning rate: [1.0000000000000005e-08], loss:2.2498862743377686\n",
      "Epoch: 169, batch index: 2707, learning rate: [1.0000000000000005e-08], loss:2.2552175521850586\n",
      "Epoch: 169, batch index: 2708, learning rate: [1.0000000000000005e-08], loss:2.307040214538574\n",
      "Epoch: 169, batch index: 2709, learning rate: [1.0000000000000005e-08], loss:2.293506383895874\n",
      "Epoch: 169, batch index: 2710, learning rate: [1.0000000000000005e-08], loss:2.2961528301239014\n",
      "Epoch: 169, batch index: 2711, learning rate: [1.0000000000000005e-08], loss:2.262378215789795\n",
      "Epoch: 169, batch index: 2712, learning rate: [1.0000000000000005e-08], loss:2.2743608951568604\n",
      "Epoch: 169, batch index: 2713, learning rate: [1.0000000000000005e-08], loss:2.240046739578247\n",
      "Epoch: 169, batch index: 2714, learning rate: [1.0000000000000005e-08], loss:2.2862226963043213\n",
      "Epoch: 169, batch index: 2715, learning rate: [1.0000000000000005e-08], loss:2.311505079269409\n",
      "Epoch: 169, batch index: 2716, learning rate: [1.0000000000000005e-08], loss:2.270902633666992\n",
      "Epoch: 169, batch index: 2717, learning rate: [1.0000000000000005e-08], loss:2.262641429901123\n",
      "Epoch: 169, batch index: 2718, learning rate: [1.0000000000000005e-08], loss:2.2718327045440674\n",
      "Epoch: 169, batch index: 2719, learning rate: [1.0000000000000005e-08], loss:2.2261807918548584\n",
      "Epoch 169, validation accuracy score0.310546875\n",
      "Epoch: 170, batch index: 2720, learning rate: [1.0000000000000005e-08], loss:2.3084607124328613\n",
      "Epoch: 170, batch index: 2721, learning rate: [1.0000000000000005e-08], loss:2.286491870880127\n",
      "Epoch: 170, batch index: 2722, learning rate: [1.0000000000000005e-08], loss:2.2338457107543945\n",
      "Epoch: 170, batch index: 2723, learning rate: [1.0000000000000005e-08], loss:2.26371169090271\n",
      "Epoch: 170, batch index: 2724, learning rate: [1.0000000000000005e-08], loss:2.2705259323120117\n",
      "Epoch: 170, batch index: 2725, learning rate: [1.0000000000000005e-08], loss:2.2693586349487305\n",
      "Epoch: 170, batch index: 2726, learning rate: [1.0000000000000005e-08], loss:2.3035030364990234\n",
      "Epoch: 170, batch index: 2727, learning rate: [1.0000000000000005e-08], loss:2.27339506149292\n",
      "Epoch: 170, batch index: 2728, learning rate: [1.0000000000000005e-08], loss:2.307504892349243\n",
      "Epoch: 170, batch index: 2729, learning rate: [1.0000000000000005e-08], loss:2.242628574371338\n",
      "Epoch: 170, batch index: 2730, learning rate: [1.0000000000000005e-08], loss:2.29691219329834\n",
      "Epoch: 170, batch index: 2731, learning rate: [1.0000000000000005e-08], loss:2.2287182807922363\n",
      "Epoch: 170, batch index: 2732, learning rate: [1.0000000000000005e-08], loss:2.2533481121063232\n",
      "Epoch: 170, batch index: 2733, learning rate: [1.0000000000000005e-08], loss:2.2601733207702637\n",
      "Epoch: 170, batch index: 2734, learning rate: [1.0000000000000005e-08], loss:2.2597239017486572\n",
      "Epoch: 170, batch index: 2735, learning rate: [1.0000000000000005e-08], loss:2.2437942028045654\n",
      "Epoch 170, validation accuracy score0.318359375\n",
      "Epoch: 171, batch index: 2736, learning rate: [1.0000000000000005e-08], loss:2.270042896270752\n",
      "Epoch: 171, batch index: 2737, learning rate: [1.0000000000000005e-08], loss:2.2681360244750977\n",
      "Epoch: 171, batch index: 2738, learning rate: [1.0000000000000005e-08], loss:2.243204355239868\n",
      "Epoch: 171, batch index: 2739, learning rate: [1.0000000000000005e-08], loss:2.2785463333129883\n",
      "Epoch: 171, batch index: 2740, learning rate: [1.0000000000000005e-08], loss:2.271951198577881\n",
      "Epoch: 171, batch index: 2741, learning rate: [1.0000000000000005e-08], loss:2.2582626342773438\n",
      "Epoch: 171, batch index: 2742, learning rate: [1.0000000000000005e-08], loss:2.243572235107422\n",
      "Epoch: 171, batch index: 2743, learning rate: [1.0000000000000005e-08], loss:2.2637951374053955\n",
      "Epoch: 171, batch index: 2744, learning rate: [1.0000000000000005e-08], loss:2.2684569358825684\n",
      "Epoch: 171, batch index: 2745, learning rate: [1.0000000000000005e-08], loss:2.280292510986328\n",
      "Epoch: 171, batch index: 2746, learning rate: [1.0000000000000005e-08], loss:2.2871999740600586\n",
      "Epoch: 171, batch index: 2747, learning rate: [1.0000000000000005e-08], loss:2.2553038597106934\n",
      "Epoch: 171, batch index: 2748, learning rate: [1.0000000000000005e-08], loss:2.2502996921539307\n",
      "Epoch: 171, batch index: 2749, learning rate: [1.0000000000000005e-08], loss:2.2584214210510254\n",
      "Epoch: 171, batch index: 2750, learning rate: [1.0000000000000005e-08], loss:2.257434844970703\n",
      "Epoch: 171, batch index: 2751, learning rate: [1.0000000000000005e-08], loss:2.3186166286468506\n",
      "Epoch 171, validation accuracy score0.271484375\n",
      "Epoch: 172, batch index: 2752, learning rate: [1.0000000000000005e-08], loss:2.2918362617492676\n",
      "Epoch: 172, batch index: 2753, learning rate: [1.0000000000000005e-08], loss:2.318167209625244\n",
      "Epoch: 172, batch index: 2754, learning rate: [1.0000000000000005e-08], loss:2.248004674911499\n",
      "Epoch: 172, batch index: 2755, learning rate: [1.0000000000000005e-08], loss:2.2312347888946533\n",
      "Epoch: 172, batch index: 2756, learning rate: [1.0000000000000005e-08], loss:2.250457763671875\n",
      "Epoch: 172, batch index: 2757, learning rate: [1.0000000000000005e-08], loss:2.2779173851013184\n",
      "Epoch: 172, batch index: 2758, learning rate: [1.0000000000000005e-08], loss:2.2877726554870605\n",
      "Epoch: 172, batch index: 2759, learning rate: [1.0000000000000005e-08], loss:2.3000714778900146\n",
      "Epoch: 172, batch index: 2760, learning rate: [1.0000000000000005e-08], loss:2.2437450885772705\n",
      "Epoch: 172, batch index: 2761, learning rate: [1.0000000000000005e-08], loss:2.253916025161743\n",
      "Epoch: 172, batch index: 2762, learning rate: [1.0000000000000005e-08], loss:2.2654807567596436\n",
      "Epoch: 172, batch index: 2763, learning rate: [1.0000000000000005e-08], loss:2.2716541290283203\n",
      "Epoch: 172, batch index: 2764, learning rate: [1.0000000000000005e-08], loss:2.268125057220459\n",
      "Epoch: 172, batch index: 2765, learning rate: [1.0000000000000005e-08], loss:2.2713663578033447\n",
      "Epoch: 172, batch index: 2766, learning rate: [1.0000000000000005e-08], loss:2.2583773136138916\n",
      "Epoch: 172, batch index: 2767, learning rate: [1.0000000000000005e-08], loss:2.280296564102173\n",
      "Epoch 172, validation accuracy score0.322265625\n",
      "Epoch: 173, batch index: 2768, learning rate: [1.0000000000000005e-08], loss:2.251434087753296\n",
      "Epoch: 173, batch index: 2769, learning rate: [1.0000000000000005e-08], loss:2.232645034790039\n",
      "Epoch: 173, batch index: 2770, learning rate: [1.0000000000000005e-08], loss:2.2431628704071045\n",
      "Epoch: 173, batch index: 2771, learning rate: [1.0000000000000005e-08], loss:2.2625458240509033\n",
      "Epoch: 173, batch index: 2772, learning rate: [1.0000000000000005e-08], loss:2.2734196186065674\n",
      "Epoch: 173, batch index: 2773, learning rate: [1.0000000000000005e-08], loss:2.2745962142944336\n",
      "Epoch: 173, batch index: 2774, learning rate: [1.0000000000000005e-08], loss:2.2686283588409424\n",
      "Epoch: 173, batch index: 2775, learning rate: [1.0000000000000005e-08], loss:2.248600721359253\n",
      "Epoch: 173, batch index: 2776, learning rate: [1.0000000000000005e-08], loss:2.2650883197784424\n",
      "Epoch: 173, batch index: 2777, learning rate: [1.0000000000000005e-08], loss:2.3096771240234375\n",
      "Epoch: 173, batch index: 2778, learning rate: [1.0000000000000005e-08], loss:2.263601064682007\n",
      "Epoch: 173, batch index: 2779, learning rate: [1.0000000000000005e-08], loss:2.2940449714660645\n",
      "Epoch: 173, batch index: 2780, learning rate: [1.0000000000000005e-08], loss:2.2917819023132324\n",
      "Epoch: 173, batch index: 2781, learning rate: [1.0000000000000005e-08], loss:2.2258903980255127\n",
      "Epoch: 173, batch index: 2782, learning rate: [1.0000000000000005e-08], loss:2.300567150115967\n",
      "Epoch: 173, batch index: 2783, learning rate: [1.0000000000000005e-08], loss:2.2925913333892822\n",
      "Epoch 173, validation accuracy score0.291015625\n",
      "Epoch: 174, batch index: 2784, learning rate: [1.0000000000000005e-08], loss:2.2696340084075928\n",
      "Epoch: 174, batch index: 2785, learning rate: [1.0000000000000005e-08], loss:2.26269268989563\n",
      "Epoch: 174, batch index: 2786, learning rate: [1.0000000000000005e-08], loss:2.2682580947875977\n",
      "Epoch: 174, batch index: 2787, learning rate: [1.0000000000000005e-08], loss:2.286445140838623\n",
      "Epoch: 174, batch index: 2788, learning rate: [1.0000000000000005e-08], loss:2.2515811920166016\n",
      "Epoch: 174, batch index: 2789, learning rate: [1.0000000000000005e-08], loss:2.2938590049743652\n",
      "Epoch: 174, batch index: 2790, learning rate: [1.0000000000000005e-08], loss:2.2583868503570557\n",
      "Epoch: 174, batch index: 2791, learning rate: [1.0000000000000005e-08], loss:2.262558937072754\n",
      "Epoch: 174, batch index: 2792, learning rate: [1.0000000000000005e-08], loss:2.259152412414551\n",
      "Epoch: 174, batch index: 2793, learning rate: [1.0000000000000005e-08], loss:2.290998935699463\n",
      "Epoch: 174, batch index: 2794, learning rate: [1.0000000000000005e-08], loss:2.2837953567504883\n",
      "Epoch: 174, batch index: 2795, learning rate: [1.0000000000000005e-08], loss:2.2667508125305176\n",
      "Epoch: 174, batch index: 2796, learning rate: [1.0000000000000005e-08], loss:2.264054775238037\n",
      "Epoch: 174, batch index: 2797, learning rate: [1.0000000000000005e-08], loss:2.2466135025024414\n",
      "Epoch: 174, batch index: 2798, learning rate: [1.0000000000000005e-08], loss:2.2566967010498047\n",
      "Epoch: 174, batch index: 2799, learning rate: [1.0000000000000005e-08], loss:2.233017683029175\n",
      "Epoch 174, validation accuracy score0.3203125\n",
      "Epoch: 175, batch index: 2800, learning rate: [1.0000000000000005e-08], loss:2.275804042816162\n",
      "Epoch: 175, batch index: 2801, learning rate: [1.0000000000000005e-08], loss:2.25756573677063\n",
      "Epoch: 175, batch index: 2802, learning rate: [1.0000000000000005e-08], loss:2.2476871013641357\n",
      "Epoch: 175, batch index: 2803, learning rate: [1.0000000000000005e-08], loss:2.2609643936157227\n",
      "Epoch: 175, batch index: 2804, learning rate: [1.0000000000000005e-08], loss:2.263922929763794\n",
      "Epoch: 175, batch index: 2805, learning rate: [1.0000000000000005e-08], loss:2.290741443634033\n",
      "Epoch: 175, batch index: 2806, learning rate: [1.0000000000000005e-08], loss:2.2460556030273438\n",
      "Epoch: 175, batch index: 2807, learning rate: [1.0000000000000005e-08], loss:2.2592852115631104\n",
      "Epoch: 175, batch index: 2808, learning rate: [1.0000000000000005e-08], loss:2.2649238109588623\n",
      "Epoch: 175, batch index: 2809, learning rate: [1.0000000000000005e-08], loss:2.2952687740325928\n",
      "Epoch: 175, batch index: 2810, learning rate: [1.0000000000000005e-08], loss:2.2829556465148926\n",
      "Epoch: 175, batch index: 2811, learning rate: [1.0000000000000005e-08], loss:2.252760648727417\n",
      "Epoch: 175, batch index: 2812, learning rate: [1.0000000000000005e-08], loss:2.2350597381591797\n",
      "Epoch: 175, batch index: 2813, learning rate: [1.0000000000000005e-08], loss:2.295337438583374\n",
      "Epoch: 175, batch index: 2814, learning rate: [1.0000000000000005e-08], loss:2.2666804790496826\n",
      "Epoch: 175, batch index: 2815, learning rate: [1.0000000000000005e-08], loss:2.2899978160858154\n",
      "Epoch 175, validation accuracy score0.330078125\n",
      "Epoch: 176, batch index: 2816, learning rate: [1.0000000000000005e-08], loss:2.270211935043335\n",
      "Epoch: 176, batch index: 2817, learning rate: [1.0000000000000005e-08], loss:2.2913451194763184\n",
      "Epoch: 176, batch index: 2818, learning rate: [1.0000000000000005e-08], loss:2.2101681232452393\n",
      "Epoch: 176, batch index: 2819, learning rate: [1.0000000000000005e-08], loss:2.2747483253479004\n",
      "Epoch: 176, batch index: 2820, learning rate: [1.0000000000000005e-08], loss:2.2715208530426025\n",
      "Epoch: 176, batch index: 2821, learning rate: [1.0000000000000005e-08], loss:2.2648568153381348\n",
      "Epoch: 176, batch index: 2822, learning rate: [1.0000000000000005e-08], loss:2.253659963607788\n",
      "Epoch: 176, batch index: 2823, learning rate: [1.0000000000000005e-08], loss:2.2627878189086914\n",
      "Epoch: 176, batch index: 2824, learning rate: [1.0000000000000005e-08], loss:2.272587776184082\n",
      "Epoch: 176, batch index: 2825, learning rate: [1.0000000000000005e-08], loss:2.3145430088043213\n",
      "Epoch: 176, batch index: 2826, learning rate: [1.0000000000000005e-08], loss:2.2267143726348877\n",
      "Epoch: 176, batch index: 2827, learning rate: [1.0000000000000005e-08], loss:2.2789688110351562\n",
      "Epoch: 176, batch index: 2828, learning rate: [1.0000000000000005e-08], loss:2.269575357437134\n",
      "Epoch: 176, batch index: 2829, learning rate: [1.0000000000000005e-08], loss:2.2856690883636475\n",
      "Epoch: 176, batch index: 2830, learning rate: [1.0000000000000005e-08], loss:2.3019766807556152\n",
      "Epoch: 176, batch index: 2831, learning rate: [1.0000000000000005e-08], loss:2.3191680908203125\n",
      "Epoch 176, validation accuracy score0.291015625\n",
      "Epoch: 177, batch index: 2832, learning rate: [1.0000000000000005e-08], loss:2.246629238128662\n",
      "Epoch: 177, batch index: 2833, learning rate: [1.0000000000000005e-08], loss:2.256391763687134\n",
      "Epoch: 177, batch index: 2834, learning rate: [1.0000000000000005e-08], loss:2.2967820167541504\n",
      "Epoch: 177, batch index: 2835, learning rate: [1.0000000000000005e-08], loss:2.2511351108551025\n",
      "Epoch: 177, batch index: 2836, learning rate: [1.0000000000000005e-08], loss:2.2376856803894043\n",
      "Epoch: 177, batch index: 2837, learning rate: [1.0000000000000005e-08], loss:2.2388319969177246\n",
      "Epoch: 177, batch index: 2838, learning rate: [1.0000000000000005e-08], loss:2.2460334300994873\n",
      "Epoch: 177, batch index: 2839, learning rate: [1.0000000000000005e-08], loss:2.303673267364502\n",
      "Epoch: 177, batch index: 2840, learning rate: [1.0000000000000005e-08], loss:2.297616958618164\n",
      "Epoch: 177, batch index: 2841, learning rate: [1.0000000000000005e-08], loss:2.2674734592437744\n",
      "Epoch: 177, batch index: 2842, learning rate: [1.0000000000000005e-08], loss:2.2552077770233154\n",
      "Epoch: 177, batch index: 2843, learning rate: [1.0000000000000005e-08], loss:2.270806074142456\n",
      "Epoch: 177, batch index: 2844, learning rate: [1.0000000000000005e-08], loss:2.263420820236206\n",
      "Epoch: 177, batch index: 2845, learning rate: [1.0000000000000005e-08], loss:2.261767625808716\n",
      "Epoch: 177, batch index: 2846, learning rate: [1.0000000000000005e-08], loss:2.2862637042999268\n",
      "Epoch: 177, batch index: 2847, learning rate: [1.0000000000000005e-08], loss:2.289713144302368\n",
      "Epoch 177, validation accuracy score0.306640625\n",
      "Epoch: 178, batch index: 2848, learning rate: [1.0000000000000005e-08], loss:2.2720608711242676\n",
      "Epoch: 178, batch index: 2849, learning rate: [1.0000000000000005e-08], loss:2.2401974201202393\n",
      "Epoch: 178, batch index: 2850, learning rate: [1.0000000000000005e-08], loss:2.2818305492401123\n",
      "Epoch: 178, batch index: 2851, learning rate: [1.0000000000000005e-08], loss:2.2448530197143555\n",
      "Epoch: 178, batch index: 2852, learning rate: [1.0000000000000005e-08], loss:2.254765510559082\n",
      "Epoch: 178, batch index: 2853, learning rate: [1.0000000000000005e-08], loss:2.2684502601623535\n",
      "Epoch: 178, batch index: 2854, learning rate: [1.0000000000000005e-08], loss:2.2604923248291016\n",
      "Epoch: 178, batch index: 2855, learning rate: [1.0000000000000005e-08], loss:2.289674758911133\n",
      "Epoch: 178, batch index: 2856, learning rate: [1.0000000000000005e-08], loss:2.275141716003418\n",
      "Epoch: 178, batch index: 2857, learning rate: [1.0000000000000005e-08], loss:2.3104465007781982\n",
      "Epoch: 178, batch index: 2858, learning rate: [1.0000000000000005e-08], loss:2.2863099575042725\n",
      "Epoch: 178, batch index: 2859, learning rate: [1.0000000000000005e-08], loss:2.254943370819092\n",
      "Epoch: 178, batch index: 2860, learning rate: [1.0000000000000005e-08], loss:2.276416540145874\n",
      "Epoch: 178, batch index: 2861, learning rate: [1.0000000000000005e-08], loss:2.2377817630767822\n",
      "Epoch: 178, batch index: 2862, learning rate: [1.0000000000000005e-08], loss:2.2599146366119385\n",
      "Epoch: 178, batch index: 2863, learning rate: [1.0000000000000005e-08], loss:2.275283098220825\n",
      "Epoch 178, validation accuracy score0.3359375\n",
      "Epoch: 179, batch index: 2864, learning rate: [1.0000000000000005e-08], loss:2.2705276012420654\n",
      "Epoch: 179, batch index: 2865, learning rate: [1.0000000000000005e-08], loss:2.2941956520080566\n",
      "Epoch: 179, batch index: 2866, learning rate: [1.0000000000000005e-08], loss:2.288667678833008\n",
      "Epoch: 179, batch index: 2867, learning rate: [1.0000000000000005e-08], loss:2.250481605529785\n",
      "Epoch: 179, batch index: 2868, learning rate: [1.0000000000000005e-08], loss:2.239158868789673\n",
      "Epoch: 179, batch index: 2869, learning rate: [1.0000000000000005e-08], loss:2.283385753631592\n",
      "Epoch: 179, batch index: 2870, learning rate: [1.0000000000000005e-08], loss:2.27205228805542\n",
      "Epoch: 179, batch index: 2871, learning rate: [1.0000000000000005e-08], loss:2.2702138423919678\n",
      "Epoch: 179, batch index: 2872, learning rate: [1.0000000000000005e-08], loss:2.2834997177124023\n",
      "Epoch: 179, batch index: 2873, learning rate: [1.0000000000000005e-08], loss:2.253734827041626\n",
      "Epoch: 179, batch index: 2874, learning rate: [1.0000000000000005e-08], loss:2.291135549545288\n",
      "Epoch: 179, batch index: 2875, learning rate: [1.0000000000000005e-08], loss:2.2761433124542236\n",
      "Epoch: 179, batch index: 2876, learning rate: [1.0000000000000005e-08], loss:2.266551971435547\n",
      "Epoch: 179, batch index: 2877, learning rate: [1.0000000000000005e-08], loss:2.248194932937622\n",
      "Epoch: 179, batch index: 2878, learning rate: [1.0000000000000005e-08], loss:2.2873666286468506\n",
      "Epoch: 179, batch index: 2879, learning rate: [1.0000000000000005e-08], loss:2.2309298515319824\n",
      "Epoch 179, validation accuracy score0.33203125\n",
      "Epoch: 180, batch index: 2880, learning rate: [1.0000000000000005e-08], loss:2.233996629714966\n",
      "Epoch: 180, batch index: 2881, learning rate: [1.0000000000000005e-08], loss:2.2865445613861084\n",
      "Epoch: 180, batch index: 2882, learning rate: [1.0000000000000005e-08], loss:2.259918212890625\n",
      "Epoch: 180, batch index: 2883, learning rate: [1.0000000000000005e-08], loss:2.2572031021118164\n",
      "Epoch: 180, batch index: 2884, learning rate: [1.0000000000000005e-08], loss:2.27750301361084\n",
      "Epoch: 180, batch index: 2885, learning rate: [1.0000000000000005e-08], loss:2.2776854038238525\n",
      "Epoch: 180, batch index: 2886, learning rate: [1.0000000000000005e-08], loss:2.277282476425171\n",
      "Epoch: 180, batch index: 2887, learning rate: [1.0000000000000005e-08], loss:2.289445161819458\n",
      "Epoch: 180, batch index: 2888, learning rate: [1.0000000000000005e-08], loss:2.2402138710021973\n",
      "Epoch: 180, batch index: 2889, learning rate: [1.0000000000000005e-08], loss:2.272427558898926\n",
      "Epoch: 180, batch index: 2890, learning rate: [1.0000000000000005e-08], loss:2.2619693279266357\n",
      "Epoch: 180, batch index: 2891, learning rate: [1.0000000000000005e-08], loss:2.259183883666992\n",
      "Epoch: 180, batch index: 2892, learning rate: [1.0000000000000005e-08], loss:2.269562244415283\n",
      "Epoch: 180, batch index: 2893, learning rate: [1.0000000000000005e-08], loss:2.252153158187866\n",
      "Epoch: 180, batch index: 2894, learning rate: [1.0000000000000005e-08], loss:2.2825348377227783\n",
      "Epoch: 180, batch index: 2895, learning rate: [1.0000000000000005e-08], loss:2.253876209259033\n",
      "Epoch 180, validation accuracy score0.3125\n",
      "Epoch: 181, batch index: 2896, learning rate: [1.0000000000000005e-08], loss:2.2581639289855957\n",
      "Epoch: 181, batch index: 2897, learning rate: [1.0000000000000005e-08], loss:2.280116558074951\n",
      "Epoch: 181, batch index: 2898, learning rate: [1.0000000000000005e-08], loss:2.256406784057617\n",
      "Epoch: 181, batch index: 2899, learning rate: [1.0000000000000005e-08], loss:2.282695770263672\n",
      "Epoch: 181, batch index: 2900, learning rate: [1.0000000000000005e-08], loss:2.2686314582824707\n",
      "Epoch: 181, batch index: 2901, learning rate: [1.0000000000000005e-08], loss:2.2314536571502686\n",
      "Epoch: 181, batch index: 2902, learning rate: [1.0000000000000005e-08], loss:2.2389817237854004\n",
      "Epoch: 181, batch index: 2903, learning rate: [1.0000000000000005e-08], loss:2.2702243328094482\n",
      "Epoch: 181, batch index: 2904, learning rate: [1.0000000000000005e-08], loss:2.2500197887420654\n",
      "Epoch: 181, batch index: 2905, learning rate: [1.0000000000000005e-08], loss:2.284126043319702\n",
      "Epoch: 181, batch index: 2906, learning rate: [1.0000000000000005e-08], loss:2.290140151977539\n",
      "Epoch: 181, batch index: 2907, learning rate: [1.0000000000000005e-08], loss:2.3140459060668945\n",
      "Epoch: 181, batch index: 2908, learning rate: [1.0000000000000005e-08], loss:2.2338051795959473\n",
      "Epoch: 181, batch index: 2909, learning rate: [1.0000000000000005e-08], loss:2.2681586742401123\n",
      "Epoch: 181, batch index: 2910, learning rate: [1.0000000000000005e-08], loss:2.2776970863342285\n",
      "Epoch: 181, batch index: 2911, learning rate: [1.0000000000000005e-08], loss:2.2977657318115234\n",
      "Epoch 181, validation accuracy score0.310546875\n",
      "Epoch: 182, batch index: 2912, learning rate: [1.0000000000000005e-08], loss:2.2895710468292236\n",
      "Epoch: 182, batch index: 2913, learning rate: [1.0000000000000005e-08], loss:2.3166255950927734\n",
      "Epoch: 182, batch index: 2914, learning rate: [1.0000000000000005e-08], loss:2.2650630474090576\n",
      "Epoch: 182, batch index: 2915, learning rate: [1.0000000000000005e-08], loss:2.22991681098938\n",
      "Epoch: 182, batch index: 2916, learning rate: [1.0000000000000005e-08], loss:2.2816050052642822\n",
      "Epoch: 182, batch index: 2917, learning rate: [1.0000000000000005e-08], loss:2.2662112712860107\n",
      "Epoch: 182, batch index: 2918, learning rate: [1.0000000000000005e-08], loss:2.2405200004577637\n",
      "Epoch: 182, batch index: 2919, learning rate: [1.0000000000000005e-08], loss:2.3113341331481934\n",
      "Epoch: 182, batch index: 2920, learning rate: [1.0000000000000005e-08], loss:2.2438721656799316\n",
      "Epoch: 182, batch index: 2921, learning rate: [1.0000000000000005e-08], loss:2.2507998943328857\n",
      "Epoch: 182, batch index: 2922, learning rate: [1.0000000000000005e-08], loss:2.3068935871124268\n",
      "Epoch: 182, batch index: 2923, learning rate: [1.0000000000000005e-08], loss:2.2804882526397705\n",
      "Epoch: 182, batch index: 2924, learning rate: [1.0000000000000005e-08], loss:2.2859888076782227\n",
      "Epoch: 182, batch index: 2925, learning rate: [1.0000000000000005e-08], loss:2.26424241065979\n",
      "Epoch: 182, batch index: 2926, learning rate: [1.0000000000000005e-08], loss:2.2354350090026855\n",
      "Epoch: 182, batch index: 2927, learning rate: [1.0000000000000005e-08], loss:2.271780490875244\n",
      "Epoch 182, validation accuracy score0.326171875\n",
      "Epoch: 183, batch index: 2928, learning rate: [1.0000000000000005e-08], loss:2.2633564472198486\n",
      "Epoch: 183, batch index: 2929, learning rate: [1.0000000000000005e-08], loss:2.25666880607605\n",
      "Epoch: 183, batch index: 2930, learning rate: [1.0000000000000005e-08], loss:2.2749719619750977\n",
      "Epoch: 183, batch index: 2931, learning rate: [1.0000000000000005e-08], loss:2.2921276092529297\n",
      "Epoch: 183, batch index: 2932, learning rate: [1.0000000000000005e-08], loss:2.258650064468384\n",
      "Epoch: 183, batch index: 2933, learning rate: [1.0000000000000005e-08], loss:2.2766788005828857\n",
      "Epoch: 183, batch index: 2934, learning rate: [1.0000000000000005e-08], loss:2.26596736907959\n",
      "Epoch: 183, batch index: 2935, learning rate: [1.0000000000000005e-08], loss:2.267284393310547\n",
      "Epoch: 183, batch index: 2936, learning rate: [1.0000000000000005e-08], loss:2.278878688812256\n",
      "Epoch: 183, batch index: 2937, learning rate: [1.0000000000000005e-08], loss:2.2457334995269775\n",
      "Epoch: 183, batch index: 2938, learning rate: [1.0000000000000005e-08], loss:2.2677760124206543\n",
      "Epoch: 183, batch index: 2939, learning rate: [1.0000000000000005e-08], loss:2.255455255508423\n",
      "Epoch: 183, batch index: 2940, learning rate: [1.0000000000000005e-08], loss:2.3027093410491943\n",
      "Epoch: 183, batch index: 2941, learning rate: [1.0000000000000005e-08], loss:2.252861261367798\n",
      "Epoch: 183, batch index: 2942, learning rate: [1.0000000000000005e-08], loss:2.2611641883850098\n",
      "Epoch: 183, batch index: 2943, learning rate: [1.0000000000000005e-08], loss:2.3084659576416016\n",
      "Epoch 183, validation accuracy score0.3203125\n",
      "Epoch: 184, batch index: 2944, learning rate: [1.0000000000000005e-08], loss:2.2505571842193604\n",
      "Epoch: 184, batch index: 2945, learning rate: [1.0000000000000005e-08], loss:2.2381412982940674\n",
      "Epoch: 184, batch index: 2946, learning rate: [1.0000000000000005e-08], loss:2.3254830837249756\n",
      "Epoch: 184, batch index: 2947, learning rate: [1.0000000000000005e-08], loss:2.277974843978882\n",
      "Epoch: 184, batch index: 2948, learning rate: [1.0000000000000005e-08], loss:2.2939870357513428\n",
      "Epoch: 184, batch index: 2949, learning rate: [1.0000000000000005e-08], loss:2.2727932929992676\n",
      "Epoch: 184, batch index: 2950, learning rate: [1.0000000000000005e-08], loss:2.2754907608032227\n",
      "Epoch: 184, batch index: 2951, learning rate: [1.0000000000000005e-08], loss:2.2597620487213135\n",
      "Epoch: 184, batch index: 2952, learning rate: [1.0000000000000005e-08], loss:2.2566921710968018\n",
      "Epoch: 184, batch index: 2953, learning rate: [1.0000000000000005e-08], loss:2.2623140811920166\n",
      "Epoch: 184, batch index: 2954, learning rate: [1.0000000000000005e-08], loss:2.2694485187530518\n",
      "Epoch: 184, batch index: 2955, learning rate: [1.0000000000000005e-08], loss:2.290972948074341\n",
      "Epoch: 184, batch index: 2956, learning rate: [1.0000000000000005e-08], loss:2.2778732776641846\n",
      "Epoch: 184, batch index: 2957, learning rate: [1.0000000000000005e-08], loss:2.268782377243042\n",
      "Epoch: 184, batch index: 2958, learning rate: [1.0000000000000005e-08], loss:2.3125438690185547\n",
      "Epoch: 184, batch index: 2959, learning rate: [1.0000000000000005e-08], loss:2.2718892097473145\n",
      "Epoch 184, validation accuracy score0.33203125\n",
      "Epoch: 185, batch index: 2960, learning rate: [1.0000000000000005e-08], loss:2.273998737335205\n",
      "Epoch: 185, batch index: 2961, learning rate: [1.0000000000000005e-08], loss:2.2602999210357666\n",
      "Epoch: 185, batch index: 2962, learning rate: [1.0000000000000005e-08], loss:2.2548675537109375\n",
      "Epoch: 185, batch index: 2963, learning rate: [1.0000000000000005e-08], loss:2.253329038619995\n",
      "Epoch: 185, batch index: 2964, learning rate: [1.0000000000000005e-08], loss:2.2976224422454834\n",
      "Epoch: 185, batch index: 2965, learning rate: [1.0000000000000005e-08], loss:2.260862350463867\n",
      "Epoch: 185, batch index: 2966, learning rate: [1.0000000000000005e-08], loss:2.310096025466919\n",
      "Epoch: 185, batch index: 2967, learning rate: [1.0000000000000005e-08], loss:2.2683069705963135\n",
      "Epoch: 185, batch index: 2968, learning rate: [1.0000000000000005e-08], loss:2.276848316192627\n",
      "Epoch: 185, batch index: 2969, learning rate: [1.0000000000000005e-08], loss:2.2738168239593506\n",
      "Epoch: 185, batch index: 2970, learning rate: [1.0000000000000005e-08], loss:2.250511407852173\n",
      "Epoch: 185, batch index: 2971, learning rate: [1.0000000000000005e-08], loss:2.2864959239959717\n",
      "Epoch: 185, batch index: 2972, learning rate: [1.0000000000000005e-08], loss:2.2834670543670654\n",
      "Epoch: 185, batch index: 2973, learning rate: [1.0000000000000005e-08], loss:2.254169464111328\n",
      "Epoch: 185, batch index: 2974, learning rate: [1.0000000000000005e-08], loss:2.2585389614105225\n",
      "Epoch: 185, batch index: 2975, learning rate: [1.0000000000000005e-08], loss:2.25101900100708\n",
      "Epoch 185, validation accuracy score0.330078125\n",
      "Epoch: 186, batch index: 2976, learning rate: [1.0000000000000005e-08], loss:2.2519447803497314\n",
      "Epoch: 186, batch index: 2977, learning rate: [1.0000000000000005e-08], loss:2.27457594871521\n",
      "Epoch: 186, batch index: 2978, learning rate: [1.0000000000000005e-08], loss:2.2501134872436523\n",
      "Epoch: 186, batch index: 2979, learning rate: [1.0000000000000005e-08], loss:2.2514967918395996\n",
      "Epoch: 186, batch index: 2980, learning rate: [1.0000000000000005e-08], loss:2.282681465148926\n",
      "Epoch: 186, batch index: 2981, learning rate: [1.0000000000000005e-08], loss:2.2879252433776855\n",
      "Epoch: 186, batch index: 2982, learning rate: [1.0000000000000005e-08], loss:2.26928448677063\n",
      "Epoch: 186, batch index: 2983, learning rate: [1.0000000000000005e-08], loss:2.2663564682006836\n",
      "Epoch: 186, batch index: 2984, learning rate: [1.0000000000000005e-08], loss:2.288618326187134\n",
      "Epoch: 186, batch index: 2985, learning rate: [1.0000000000000005e-08], loss:2.247413396835327\n",
      "Epoch: 186, batch index: 2986, learning rate: [1.0000000000000005e-08], loss:2.2905497550964355\n",
      "Epoch: 186, batch index: 2987, learning rate: [1.0000000000000005e-08], loss:2.3060874938964844\n",
      "Epoch: 186, batch index: 2988, learning rate: [1.0000000000000005e-08], loss:2.281060218811035\n",
      "Epoch: 186, batch index: 2989, learning rate: [1.0000000000000005e-08], loss:2.2492620944976807\n",
      "Epoch: 186, batch index: 2990, learning rate: [1.0000000000000005e-08], loss:2.2489397525787354\n",
      "Epoch: 186, batch index: 2991, learning rate: [1.0000000000000005e-08], loss:2.2526280879974365\n",
      "Epoch 186, validation accuracy score0.333984375\n",
      "Epoch: 187, batch index: 2992, learning rate: [1.0000000000000005e-08], loss:2.279881477355957\n",
      "Epoch: 187, batch index: 2993, learning rate: [1.0000000000000005e-08], loss:2.2806434631347656\n",
      "Epoch: 187, batch index: 2994, learning rate: [1.0000000000000005e-08], loss:2.2800135612487793\n",
      "Epoch: 187, batch index: 2995, learning rate: [1.0000000000000005e-08], loss:2.2737035751342773\n",
      "Epoch: 187, batch index: 2996, learning rate: [1.0000000000000005e-08], loss:2.243577003479004\n",
      "Epoch: 187, batch index: 2997, learning rate: [1.0000000000000005e-08], loss:2.2888290882110596\n",
      "Epoch: 187, batch index: 2998, learning rate: [1.0000000000000005e-08], loss:2.259209632873535\n",
      "Epoch: 187, batch index: 2999, learning rate: [1.0000000000000005e-08], loss:2.2778446674346924\n",
      "Epoch: 187, batch index: 3000, learning rate: [1.0000000000000005e-08], loss:2.261558771133423\n",
      "Epoch: 187, batch index: 3001, learning rate: [1.0000000000000005e-08], loss:2.279764413833618\n",
      "Epoch: 187, batch index: 3002, learning rate: [1.0000000000000005e-08], loss:2.231696605682373\n",
      "Epoch: 187, batch index: 3003, learning rate: [1.0000000000000005e-08], loss:2.2601470947265625\n",
      "Epoch: 187, batch index: 3004, learning rate: [1.0000000000000005e-08], loss:2.291152000427246\n",
      "Epoch: 187, batch index: 3005, learning rate: [1.0000000000000005e-08], loss:2.295588254928589\n",
      "Epoch: 187, batch index: 3006, learning rate: [1.0000000000000005e-08], loss:2.2686305046081543\n",
      "Epoch: 187, batch index: 3007, learning rate: [1.0000000000000005e-08], loss:2.256824493408203\n",
      "Epoch 187, validation accuracy score0.3359375\n",
      "Epoch: 188, batch index: 3008, learning rate: [1.0000000000000005e-08], loss:2.2504405975341797\n",
      "Epoch: 188, batch index: 3009, learning rate: [1.0000000000000005e-08], loss:2.25900936126709\n",
      "Epoch: 188, batch index: 3010, learning rate: [1.0000000000000005e-08], loss:2.2602386474609375\n",
      "Epoch: 188, batch index: 3011, learning rate: [1.0000000000000005e-08], loss:2.255100965499878\n",
      "Epoch: 188, batch index: 3012, learning rate: [1.0000000000000005e-08], loss:2.2725086212158203\n",
      "Epoch: 188, batch index: 3013, learning rate: [1.0000000000000005e-08], loss:2.278031587600708\n",
      "Epoch: 188, batch index: 3014, learning rate: [1.0000000000000005e-08], loss:2.245206356048584\n",
      "Epoch: 188, batch index: 3015, learning rate: [1.0000000000000005e-08], loss:2.2736828327178955\n",
      "Epoch: 188, batch index: 3016, learning rate: [1.0000000000000005e-08], loss:2.2269506454467773\n",
      "Epoch: 188, batch index: 3017, learning rate: [1.0000000000000005e-08], loss:2.2980875968933105\n",
      "Epoch: 188, batch index: 3018, learning rate: [1.0000000000000005e-08], loss:2.2759697437286377\n",
      "Epoch: 188, batch index: 3019, learning rate: [1.0000000000000005e-08], loss:2.2549033164978027\n",
      "Epoch: 188, batch index: 3020, learning rate: [1.0000000000000005e-08], loss:2.2892954349517822\n",
      "Epoch: 188, batch index: 3021, learning rate: [1.0000000000000005e-08], loss:2.3357431888580322\n",
      "Epoch: 188, batch index: 3022, learning rate: [1.0000000000000005e-08], loss:2.281233072280884\n",
      "Epoch: 188, batch index: 3023, learning rate: [1.0000000000000005e-08], loss:2.2597036361694336\n",
      "Epoch 188, validation accuracy score0.373046875\n",
      "Epoch: 189, batch index: 3024, learning rate: [1.0000000000000005e-08], loss:2.3116984367370605\n",
      "Epoch: 189, batch index: 3025, learning rate: [1.0000000000000005e-08], loss:2.2527410984039307\n",
      "Epoch: 189, batch index: 3026, learning rate: [1.0000000000000005e-08], loss:2.272280693054199\n",
      "Epoch: 189, batch index: 3027, learning rate: [1.0000000000000005e-08], loss:2.261416435241699\n",
      "Epoch: 189, batch index: 3028, learning rate: [1.0000000000000005e-08], loss:2.314962863922119\n",
      "Epoch: 189, batch index: 3029, learning rate: [1.0000000000000005e-08], loss:2.2683117389678955\n",
      "Epoch: 189, batch index: 3030, learning rate: [1.0000000000000005e-08], loss:2.2309906482696533\n",
      "Epoch: 189, batch index: 3031, learning rate: [1.0000000000000005e-08], loss:2.2648775577545166\n",
      "Epoch: 189, batch index: 3032, learning rate: [1.0000000000000005e-08], loss:2.282219171524048\n",
      "Epoch: 189, batch index: 3033, learning rate: [1.0000000000000005e-08], loss:2.274935245513916\n",
      "Epoch: 189, batch index: 3034, learning rate: [1.0000000000000005e-08], loss:2.297858476638794\n",
      "Epoch: 189, batch index: 3035, learning rate: [1.0000000000000005e-08], loss:2.2874042987823486\n",
      "Epoch: 189, batch index: 3036, learning rate: [1.0000000000000005e-08], loss:2.2703137397766113\n",
      "Epoch: 189, batch index: 3037, learning rate: [1.0000000000000005e-08], loss:2.238701820373535\n",
      "Epoch: 189, batch index: 3038, learning rate: [1.0000000000000005e-08], loss:2.3063242435455322\n",
      "Epoch: 189, batch index: 3039, learning rate: [1.0000000000000005e-08], loss:2.2715773582458496\n",
      "Epoch 189, validation accuracy score0.302734375\n",
      "Epoch: 190, batch index: 3040, learning rate: [1.0000000000000005e-08], loss:2.292301654815674\n",
      "Epoch: 190, batch index: 3041, learning rate: [1.0000000000000005e-08], loss:2.2992241382598877\n",
      "Epoch: 190, batch index: 3042, learning rate: [1.0000000000000005e-08], loss:2.302314519882202\n",
      "Epoch: 190, batch index: 3043, learning rate: [1.0000000000000005e-08], loss:2.2397844791412354\n",
      "Epoch: 190, batch index: 3044, learning rate: [1.0000000000000005e-08], loss:2.2918572425842285\n",
      "Epoch: 190, batch index: 3045, learning rate: [1.0000000000000005e-08], loss:2.27168345451355\n",
      "Epoch: 190, batch index: 3046, learning rate: [1.0000000000000005e-08], loss:2.28608775138855\n",
      "Epoch: 190, batch index: 3047, learning rate: [1.0000000000000005e-08], loss:2.287017583847046\n",
      "Epoch: 190, batch index: 3048, learning rate: [1.0000000000000005e-08], loss:2.246127128601074\n",
      "Epoch: 190, batch index: 3049, learning rate: [1.0000000000000005e-08], loss:2.283067226409912\n",
      "Epoch: 190, batch index: 3050, learning rate: [1.0000000000000005e-08], loss:2.239438533782959\n",
      "Epoch: 190, batch index: 3051, learning rate: [1.0000000000000005e-08], loss:2.293666362762451\n",
      "Epoch: 190, batch index: 3052, learning rate: [1.0000000000000005e-08], loss:2.281851053237915\n",
      "Epoch: 190, batch index: 3053, learning rate: [1.0000000000000005e-08], loss:2.2663960456848145\n",
      "Epoch: 190, batch index: 3054, learning rate: [1.0000000000000005e-08], loss:2.2321226596832275\n",
      "Epoch: 190, batch index: 3055, learning rate: [1.0000000000000005e-08], loss:2.258910894393921\n",
      "Epoch 190, validation accuracy score0.337890625\n",
      "Epoch: 191, batch index: 3056, learning rate: [1.0000000000000005e-08], loss:2.259308338165283\n",
      "Epoch: 191, batch index: 3057, learning rate: [1.0000000000000005e-08], loss:2.2758846282958984\n",
      "Epoch: 191, batch index: 3058, learning rate: [1.0000000000000005e-08], loss:2.230804681777954\n",
      "Epoch: 191, batch index: 3059, learning rate: [1.0000000000000005e-08], loss:2.2564635276794434\n",
      "Epoch: 191, batch index: 3060, learning rate: [1.0000000000000005e-08], loss:2.256859302520752\n",
      "Epoch: 191, batch index: 3061, learning rate: [1.0000000000000005e-08], loss:2.2679240703582764\n",
      "Epoch: 191, batch index: 3062, learning rate: [1.0000000000000005e-08], loss:2.2658984661102295\n",
      "Epoch: 191, batch index: 3063, learning rate: [1.0000000000000005e-08], loss:2.270674705505371\n",
      "Epoch: 191, batch index: 3064, learning rate: [1.0000000000000005e-08], loss:2.2654318809509277\n",
      "Epoch: 191, batch index: 3065, learning rate: [1.0000000000000005e-08], loss:2.259951114654541\n",
      "Epoch: 191, batch index: 3066, learning rate: [1.0000000000000005e-08], loss:2.250406265258789\n",
      "Epoch: 191, batch index: 3067, learning rate: [1.0000000000000005e-08], loss:2.292447805404663\n",
      "Epoch: 191, batch index: 3068, learning rate: [1.0000000000000005e-08], loss:2.273775815963745\n",
      "Epoch: 191, batch index: 3069, learning rate: [1.0000000000000005e-08], loss:2.2702391147613525\n",
      "Epoch: 191, batch index: 3070, learning rate: [1.0000000000000005e-08], loss:2.2875466346740723\n",
      "Epoch: 191, batch index: 3071, learning rate: [1.0000000000000005e-08], loss:2.2582552433013916\n",
      "Epoch 191, validation accuracy score0.3203125\n",
      "Epoch: 192, batch index: 3072, learning rate: [1.0000000000000005e-08], loss:2.3123509883880615\n",
      "Epoch: 192, batch index: 3073, learning rate: [1.0000000000000005e-08], loss:2.2928178310394287\n",
      "Epoch: 192, batch index: 3074, learning rate: [1.0000000000000005e-08], loss:2.2371578216552734\n",
      "Epoch: 192, batch index: 3075, learning rate: [1.0000000000000005e-08], loss:2.2286674976348877\n",
      "Epoch: 192, batch index: 3076, learning rate: [1.0000000000000005e-08], loss:2.2434327602386475\n",
      "Epoch: 192, batch index: 3077, learning rate: [1.0000000000000005e-08], loss:2.2709686756134033\n",
      "Epoch: 192, batch index: 3078, learning rate: [1.0000000000000005e-08], loss:2.274660348892212\n",
      "Epoch: 192, batch index: 3079, learning rate: [1.0000000000000005e-08], loss:2.2770960330963135\n",
      "Epoch: 192, batch index: 3080, learning rate: [1.0000000000000005e-08], loss:2.261716842651367\n",
      "Epoch: 192, batch index: 3081, learning rate: [1.0000000000000005e-08], loss:2.2905642986297607\n",
      "Epoch: 192, batch index: 3082, learning rate: [1.0000000000000005e-08], loss:2.292743444442749\n",
      "Epoch: 192, batch index: 3083, learning rate: [1.0000000000000005e-08], loss:2.2656898498535156\n",
      "Epoch: 192, batch index: 3084, learning rate: [1.0000000000000005e-08], loss:2.2931768894195557\n",
      "Epoch: 192, batch index: 3085, learning rate: [1.0000000000000005e-08], loss:2.299267053604126\n",
      "Epoch: 192, batch index: 3086, learning rate: [1.0000000000000005e-08], loss:2.268143892288208\n",
      "Epoch: 192, batch index: 3087, learning rate: [1.0000000000000005e-08], loss:2.270397663116455\n",
      "Epoch 192, validation accuracy score0.3046875\n",
      "Epoch: 193, batch index: 3088, learning rate: [1.0000000000000005e-08], loss:2.2965087890625\n",
      "Epoch: 193, batch index: 3089, learning rate: [1.0000000000000005e-08], loss:2.2589941024780273\n",
      "Epoch: 193, batch index: 3090, learning rate: [1.0000000000000005e-08], loss:2.2630882263183594\n",
      "Epoch: 193, batch index: 3091, learning rate: [1.0000000000000005e-08], loss:2.316389322280884\n",
      "Epoch: 193, batch index: 3092, learning rate: [1.0000000000000005e-08], loss:2.2722291946411133\n",
      "Epoch: 193, batch index: 3093, learning rate: [1.0000000000000005e-08], loss:2.2626726627349854\n",
      "Epoch: 193, batch index: 3094, learning rate: [1.0000000000000005e-08], loss:2.2907748222351074\n",
      "Epoch: 193, batch index: 3095, learning rate: [1.0000000000000005e-08], loss:2.25783634185791\n",
      "Epoch: 193, batch index: 3096, learning rate: [1.0000000000000005e-08], loss:2.2512261867523193\n",
      "Epoch: 193, batch index: 3097, learning rate: [1.0000000000000005e-08], loss:2.2269277572631836\n",
      "Epoch: 193, batch index: 3098, learning rate: [1.0000000000000005e-08], loss:2.275240421295166\n",
      "Epoch: 193, batch index: 3099, learning rate: [1.0000000000000005e-08], loss:2.2674834728240967\n",
      "Epoch: 193, batch index: 3100, learning rate: [1.0000000000000005e-08], loss:2.29657244682312\n",
      "Epoch: 193, batch index: 3101, learning rate: [1.0000000000000005e-08], loss:2.265017509460449\n",
      "Epoch: 193, batch index: 3102, learning rate: [1.0000000000000005e-08], loss:2.2595393657684326\n",
      "Epoch: 193, batch index: 3103, learning rate: [1.0000000000000005e-08], loss:2.265718936920166\n",
      "Epoch 193, validation accuracy score0.326171875\n",
      "Epoch: 194, batch index: 3104, learning rate: [1.0000000000000005e-08], loss:2.2642345428466797\n",
      "Epoch: 194, batch index: 3105, learning rate: [1.0000000000000005e-08], loss:2.2675116062164307\n",
      "Epoch: 194, batch index: 3106, learning rate: [1.0000000000000005e-08], loss:2.322448968887329\n",
      "Epoch: 194, batch index: 3107, learning rate: [1.0000000000000005e-08], loss:2.2698686122894287\n",
      "Epoch: 194, batch index: 3108, learning rate: [1.0000000000000005e-08], loss:2.2577717304229736\n",
      "Epoch: 194, batch index: 3109, learning rate: [1.0000000000000005e-08], loss:2.243213176727295\n",
      "Epoch: 194, batch index: 3110, learning rate: [1.0000000000000005e-08], loss:2.2712998390197754\n",
      "Epoch: 194, batch index: 3111, learning rate: [1.0000000000000005e-08], loss:2.255566120147705\n",
      "Epoch: 194, batch index: 3112, learning rate: [1.0000000000000005e-08], loss:2.2864954471588135\n",
      "Epoch: 194, batch index: 3113, learning rate: [1.0000000000000005e-08], loss:2.2908759117126465\n",
      "Epoch: 194, batch index: 3114, learning rate: [1.0000000000000005e-08], loss:2.263263702392578\n",
      "Epoch: 194, batch index: 3115, learning rate: [1.0000000000000005e-08], loss:2.284924268722534\n",
      "Epoch: 194, batch index: 3116, learning rate: [1.0000000000000005e-08], loss:2.2599916458129883\n",
      "Epoch: 194, batch index: 3117, learning rate: [1.0000000000000005e-08], loss:2.2401599884033203\n",
      "Epoch: 194, batch index: 3118, learning rate: [1.0000000000000005e-08], loss:2.262282133102417\n",
      "Epoch: 194, batch index: 3119, learning rate: [1.0000000000000005e-08], loss:2.2506728172302246\n",
      "Epoch 194, validation accuracy score0.345703125\n",
      "Epoch: 195, batch index: 3120, learning rate: [1.0000000000000005e-08], loss:2.2636289596557617\n",
      "Epoch: 195, batch index: 3121, learning rate: [1.0000000000000005e-08], loss:2.2581899166107178\n",
      "Epoch: 195, batch index: 3122, learning rate: [1.0000000000000005e-08], loss:2.3036844730377197\n",
      "Epoch: 195, batch index: 3123, learning rate: [1.0000000000000005e-08], loss:2.275160074234009\n",
      "Epoch: 195, batch index: 3124, learning rate: [1.0000000000000005e-08], loss:2.3123979568481445\n",
      "Epoch: 195, batch index: 3125, learning rate: [1.0000000000000005e-08], loss:2.2285122871398926\n",
      "Epoch: 195, batch index: 3126, learning rate: [1.0000000000000005e-08], loss:2.280139446258545\n",
      "Epoch: 195, batch index: 3127, learning rate: [1.0000000000000005e-08], loss:2.3046185970306396\n",
      "Epoch: 195, batch index: 3128, learning rate: [1.0000000000000005e-08], loss:2.281593084335327\n",
      "Epoch: 195, batch index: 3129, learning rate: [1.0000000000000005e-08], loss:2.2554519176483154\n",
      "Epoch: 195, batch index: 3130, learning rate: [1.0000000000000005e-08], loss:2.2664687633514404\n",
      "Epoch: 195, batch index: 3131, learning rate: [1.0000000000000005e-08], loss:2.2528207302093506\n",
      "Epoch: 195, batch index: 3132, learning rate: [1.0000000000000005e-08], loss:2.251276969909668\n",
      "Epoch: 195, batch index: 3133, learning rate: [1.0000000000000005e-08], loss:2.2433691024780273\n",
      "Epoch: 195, batch index: 3134, learning rate: [1.0000000000000005e-08], loss:2.264798879623413\n",
      "Epoch: 195, batch index: 3135, learning rate: [1.0000000000000005e-08], loss:2.270751476287842\n",
      "Epoch 195, validation accuracy score0.333984375\n",
      "Epoch: 196, batch index: 3136, learning rate: [1.0000000000000005e-08], loss:2.2665209770202637\n",
      "Epoch: 196, batch index: 3137, learning rate: [1.0000000000000005e-08], loss:2.273535966873169\n",
      "Epoch: 196, batch index: 3138, learning rate: [1.0000000000000005e-08], loss:2.267784833908081\n",
      "Epoch: 196, batch index: 3139, learning rate: [1.0000000000000005e-08], loss:2.2681405544281006\n",
      "Epoch: 196, batch index: 3140, learning rate: [1.0000000000000005e-08], loss:2.2491252422332764\n",
      "Epoch: 196, batch index: 3141, learning rate: [1.0000000000000005e-08], loss:2.2562003135681152\n",
      "Epoch: 196, batch index: 3142, learning rate: [1.0000000000000005e-08], loss:2.2427215576171875\n",
      "Epoch: 196, batch index: 3143, learning rate: [1.0000000000000005e-08], loss:2.2423508167266846\n",
      "Epoch: 196, batch index: 3144, learning rate: [1.0000000000000005e-08], loss:2.277334690093994\n",
      "Epoch: 196, batch index: 3145, learning rate: [1.0000000000000005e-08], loss:2.2900989055633545\n",
      "Epoch: 196, batch index: 3146, learning rate: [1.0000000000000005e-08], loss:2.2757303714752197\n",
      "Epoch: 196, batch index: 3147, learning rate: [1.0000000000000005e-08], loss:2.2762482166290283\n",
      "Epoch: 196, batch index: 3148, learning rate: [1.0000000000000005e-08], loss:2.296854019165039\n",
      "Epoch: 196, batch index: 3149, learning rate: [1.0000000000000005e-08], loss:2.286914825439453\n",
      "Epoch: 196, batch index: 3150, learning rate: [1.0000000000000005e-08], loss:2.2946295738220215\n",
      "Epoch: 196, batch index: 3151, learning rate: [1.0000000000000005e-08], loss:2.2554612159729004\n",
      "Epoch 196, validation accuracy score0.32421875\n",
      "Epoch: 197, batch index: 3152, learning rate: [1.0000000000000005e-08], loss:2.273906946182251\n",
      "Epoch: 197, batch index: 3153, learning rate: [1.0000000000000005e-08], loss:2.240901231765747\n",
      "Epoch: 197, batch index: 3154, learning rate: [1.0000000000000005e-08], loss:2.2715094089508057\n",
      "Epoch: 197, batch index: 3155, learning rate: [1.0000000000000005e-08], loss:2.2563865184783936\n",
      "Epoch: 197, batch index: 3156, learning rate: [1.0000000000000005e-08], loss:2.2705953121185303\n",
      "Epoch: 197, batch index: 3157, learning rate: [1.0000000000000005e-08], loss:2.2777206897735596\n",
      "Epoch: 197, batch index: 3158, learning rate: [1.0000000000000005e-08], loss:2.2788193225860596\n",
      "Epoch: 197, batch index: 3159, learning rate: [1.0000000000000005e-08], loss:2.2948083877563477\n",
      "Epoch: 197, batch index: 3160, learning rate: [1.0000000000000005e-08], loss:2.3022453784942627\n",
      "Epoch: 197, batch index: 3161, learning rate: [1.0000000000000005e-08], loss:2.260652780532837\n",
      "Epoch: 197, batch index: 3162, learning rate: [1.0000000000000005e-08], loss:2.2992255687713623\n",
      "Epoch: 197, batch index: 3163, learning rate: [1.0000000000000005e-08], loss:2.2882237434387207\n",
      "Epoch: 197, batch index: 3164, learning rate: [1.0000000000000005e-08], loss:2.267404794692993\n",
      "Epoch: 197, batch index: 3165, learning rate: [1.0000000000000005e-08], loss:2.2983920574188232\n",
      "Epoch: 197, batch index: 3166, learning rate: [1.0000000000000005e-08], loss:2.2469403743743896\n",
      "Epoch: 197, batch index: 3167, learning rate: [1.0000000000000005e-08], loss:2.262111186981201\n",
      "Epoch 197, validation accuracy score0.2890625\n",
      "Epoch: 198, batch index: 3168, learning rate: [1.0000000000000005e-08], loss:2.246537685394287\n",
      "Epoch: 198, batch index: 3169, learning rate: [1.0000000000000005e-08], loss:2.2885525226593018\n",
      "Epoch: 198, batch index: 3170, learning rate: [1.0000000000000005e-08], loss:2.2704732418060303\n",
      "Epoch: 198, batch index: 3171, learning rate: [1.0000000000000005e-08], loss:2.255643844604492\n",
      "Epoch: 198, batch index: 3172, learning rate: [1.0000000000000005e-08], loss:2.260425329208374\n",
      "Epoch: 198, batch index: 3173, learning rate: [1.0000000000000005e-08], loss:2.2550556659698486\n",
      "Epoch: 198, batch index: 3174, learning rate: [1.0000000000000005e-08], loss:2.2804620265960693\n",
      "Epoch: 198, batch index: 3175, learning rate: [1.0000000000000005e-08], loss:2.270123243331909\n",
      "Epoch: 198, batch index: 3176, learning rate: [1.0000000000000005e-08], loss:2.2577505111694336\n",
      "Epoch: 198, batch index: 3177, learning rate: [1.0000000000000005e-08], loss:2.2704646587371826\n",
      "Epoch: 198, batch index: 3178, learning rate: [1.0000000000000005e-08], loss:2.2457680702209473\n",
      "Epoch: 198, batch index: 3179, learning rate: [1.0000000000000005e-08], loss:2.269469738006592\n",
      "Epoch: 198, batch index: 3180, learning rate: [1.0000000000000005e-08], loss:2.263176202774048\n",
      "Epoch: 198, batch index: 3181, learning rate: [1.0000000000000005e-08], loss:2.2688474655151367\n",
      "Epoch: 198, batch index: 3182, learning rate: [1.0000000000000005e-08], loss:2.276034355163574\n",
      "Epoch: 198, batch index: 3183, learning rate: [1.0000000000000005e-08], loss:2.277336597442627\n",
      "Epoch 198, validation accuracy score0.298828125\n",
      "Epoch: 199, batch index: 3184, learning rate: [1.0000000000000005e-08], loss:2.2225258350372314\n",
      "Epoch: 199, batch index: 3185, learning rate: [1.0000000000000005e-08], loss:2.254396438598633\n",
      "Epoch: 199, batch index: 3186, learning rate: [1.0000000000000005e-08], loss:2.276214838027954\n",
      "Epoch: 199, batch index: 3187, learning rate: [1.0000000000000005e-08], loss:2.2713327407836914\n",
      "Epoch: 199, batch index: 3188, learning rate: [1.0000000000000005e-08], loss:2.2873048782348633\n",
      "Epoch: 199, batch index: 3189, learning rate: [1.0000000000000005e-08], loss:2.2940561771392822\n",
      "Epoch: 199, batch index: 3190, learning rate: [1.0000000000000005e-08], loss:2.282963514328003\n",
      "Epoch: 199, batch index: 3191, learning rate: [1.0000000000000005e-08], loss:2.2701292037963867\n",
      "Epoch: 199, batch index: 3192, learning rate: [1.0000000000000005e-08], loss:2.290010452270508\n",
      "Epoch: 199, batch index: 3193, learning rate: [1.0000000000000005e-08], loss:2.2872722148895264\n",
      "Epoch: 199, batch index: 3194, learning rate: [1.0000000000000005e-08], loss:2.2407894134521484\n",
      "Epoch: 199, batch index: 3195, learning rate: [1.0000000000000005e-08], loss:2.2598721981048584\n",
      "Epoch: 199, batch index: 3196, learning rate: [1.0000000000000005e-08], loss:2.2545619010925293\n",
      "Epoch: 199, batch index: 3197, learning rate: [1.0000000000000005e-08], loss:2.267596960067749\n",
      "Epoch: 199, batch index: 3198, learning rate: [1.0000000000000005e-08], loss:2.2857189178466797\n",
      "Epoch: 199, batch index: 3199, learning rate: [1.0000000000000005e-08], loss:2.281951427459717\n",
      "Epoch 199, validation accuracy score0.30078125\n",
      "Epoch: 200, batch index: 3200, learning rate: [1.0000000000000005e-08], loss:2.283113718032837\n",
      "Epoch: 200, batch index: 3201, learning rate: [1.0000000000000005e-08], loss:2.2835519313812256\n",
      "Epoch: 200, batch index: 3202, learning rate: [1.0000000000000005e-08], loss:2.2803468704223633\n",
      "Epoch: 200, batch index: 3203, learning rate: [1.0000000000000005e-08], loss:2.2686824798583984\n",
      "Epoch: 200, batch index: 3204, learning rate: [1.0000000000000005e-08], loss:2.3067893981933594\n",
      "Epoch: 200, batch index: 3205, learning rate: [1.0000000000000005e-08], loss:2.2702670097351074\n",
      "Epoch: 200, batch index: 3206, learning rate: [1.0000000000000005e-08], loss:2.2888944149017334\n",
      "Epoch: 200, batch index: 3207, learning rate: [1.0000000000000005e-08], loss:2.239563465118408\n",
      "Epoch: 200, batch index: 3208, learning rate: [1.0000000000000005e-08], loss:2.2478740215301514\n",
      "Epoch: 200, batch index: 3209, learning rate: [1.0000000000000005e-08], loss:2.247360944747925\n",
      "Epoch: 200, batch index: 3210, learning rate: [1.0000000000000005e-08], loss:2.265423059463501\n",
      "Epoch: 200, batch index: 3211, learning rate: [1.0000000000000005e-08], loss:2.2575204372406006\n",
      "Epoch: 200, batch index: 3212, learning rate: [1.0000000000000005e-08], loss:2.2525131702423096\n",
      "Epoch: 200, batch index: 3213, learning rate: [1.0000000000000005e-08], loss:2.2619283199310303\n",
      "Epoch: 200, batch index: 3214, learning rate: [1.0000000000000005e-08], loss:2.2631759643554688\n",
      "Epoch: 200, batch index: 3215, learning rate: [1.0000000000000005e-08], loss:2.2749533653259277\n",
      "Epoch 200, validation accuracy score0.30859375\n",
      "0\n",
      "Epoch: 201, batch index: 3216, learning rate: [1.0000000000000005e-08], loss:2.274792432785034\n",
      "Epoch: 201, batch index: 3217, learning rate: [1.0000000000000005e-08], loss:2.2335288524627686\n",
      "Epoch: 201, batch index: 3218, learning rate: [1.0000000000000005e-08], loss:2.2500667572021484\n",
      "Epoch: 201, batch index: 3219, learning rate: [1.0000000000000005e-08], loss:2.263376235961914\n",
      "Epoch: 201, batch index: 3220, learning rate: [1.0000000000000005e-08], loss:2.2843849658966064\n",
      "Epoch: 201, batch index: 3221, learning rate: [1.0000000000000005e-08], loss:2.2693493366241455\n",
      "Epoch: 201, batch index: 3222, learning rate: [1.0000000000000005e-08], loss:2.270707130432129\n",
      "Epoch: 201, batch index: 3223, learning rate: [1.0000000000000005e-08], loss:2.2728047370910645\n",
      "Epoch: 201, batch index: 3224, learning rate: [1.0000000000000005e-08], loss:2.3115429878234863\n",
      "Epoch: 201, batch index: 3225, learning rate: [1.0000000000000005e-08], loss:2.2557461261749268\n",
      "Epoch: 201, batch index: 3226, learning rate: [1.0000000000000005e-08], loss:2.247856616973877\n",
      "Epoch: 201, batch index: 3227, learning rate: [1.0000000000000005e-08], loss:2.225804090499878\n",
      "Epoch: 201, batch index: 3228, learning rate: [1.0000000000000005e-08], loss:2.267836332321167\n",
      "Epoch: 201, batch index: 3229, learning rate: [1.0000000000000005e-08], loss:2.3333957195281982\n",
      "Epoch: 201, batch index: 3230, learning rate: [1.0000000000000005e-08], loss:2.2478132247924805\n",
      "Epoch: 201, batch index: 3231, learning rate: [1.0000000000000005e-08], loss:2.289072275161743\n",
      "Epoch 201, validation accuracy score0.341796875\n",
      "Epoch: 202, batch index: 3232, learning rate: [1.0000000000000005e-08], loss:2.2657248973846436\n",
      "Epoch: 202, batch index: 3233, learning rate: [1.0000000000000005e-08], loss:2.325301170349121\n",
      "Epoch: 202, batch index: 3234, learning rate: [1.0000000000000005e-08], loss:2.258589029312134\n",
      "Epoch: 202, batch index: 3235, learning rate: [1.0000000000000005e-08], loss:2.2821357250213623\n",
      "Epoch: 202, batch index: 3236, learning rate: [1.0000000000000005e-08], loss:2.2566769123077393\n",
      "Epoch: 202, batch index: 3237, learning rate: [1.0000000000000005e-08], loss:2.246785879135132\n",
      "Epoch: 202, batch index: 3238, learning rate: [1.0000000000000005e-08], loss:2.243739128112793\n",
      "Epoch: 202, batch index: 3239, learning rate: [1.0000000000000005e-08], loss:2.2294867038726807\n",
      "Epoch: 202, batch index: 3240, learning rate: [1.0000000000000005e-08], loss:2.275298833847046\n",
      "Epoch: 202, batch index: 3241, learning rate: [1.0000000000000005e-08], loss:2.295593738555908\n",
      "Epoch: 202, batch index: 3242, learning rate: [1.0000000000000005e-08], loss:2.276792526245117\n",
      "Epoch: 202, batch index: 3243, learning rate: [1.0000000000000005e-08], loss:2.3055219650268555\n",
      "Epoch: 202, batch index: 3244, learning rate: [1.0000000000000005e-08], loss:2.3065571784973145\n",
      "Epoch: 202, batch index: 3245, learning rate: [1.0000000000000005e-08], loss:2.3005664348602295\n",
      "Epoch: 202, batch index: 3246, learning rate: [1.0000000000000005e-08], loss:2.24965238571167\n",
      "Epoch: 202, batch index: 3247, learning rate: [1.0000000000000005e-08], loss:2.2759652137756348\n",
      "Epoch 202, validation accuracy score0.31640625\n",
      "Epoch: 203, batch index: 3248, learning rate: [1.0000000000000005e-08], loss:2.307528495788574\n",
      "Epoch: 203, batch index: 3249, learning rate: [1.0000000000000005e-08], loss:2.3017382621765137\n",
      "Epoch: 203, batch index: 3250, learning rate: [1.0000000000000005e-08], loss:2.288057565689087\n",
      "Epoch: 203, batch index: 3251, learning rate: [1.0000000000000005e-08], loss:2.2524149417877197\n",
      "Epoch: 203, batch index: 3252, learning rate: [1.0000000000000005e-08], loss:2.2806053161621094\n",
      "Epoch: 203, batch index: 3253, learning rate: [1.0000000000000005e-08], loss:2.295395612716675\n",
      "Epoch: 203, batch index: 3254, learning rate: [1.0000000000000005e-08], loss:2.2874817848205566\n",
      "Epoch: 203, batch index: 3255, learning rate: [1.0000000000000005e-08], loss:2.277158260345459\n",
      "Epoch: 203, batch index: 3256, learning rate: [1.0000000000000005e-08], loss:2.251288652420044\n",
      "Epoch: 203, batch index: 3257, learning rate: [1.0000000000000005e-08], loss:2.2769083976745605\n",
      "Epoch: 203, batch index: 3258, learning rate: [1.0000000000000005e-08], loss:2.257716417312622\n",
      "Epoch: 203, batch index: 3259, learning rate: [1.0000000000000005e-08], loss:2.271911382675171\n",
      "Epoch: 203, batch index: 3260, learning rate: [1.0000000000000005e-08], loss:2.2727982997894287\n",
      "Epoch: 203, batch index: 3261, learning rate: [1.0000000000000005e-08], loss:2.24817156791687\n",
      "Epoch: 203, batch index: 3262, learning rate: [1.0000000000000005e-08], loss:2.25898814201355\n",
      "Epoch: 203, batch index: 3263, learning rate: [1.0000000000000005e-08], loss:2.283259868621826\n",
      "Epoch 203, validation accuracy score0.341796875\n",
      "Epoch: 204, batch index: 3264, learning rate: [1.0000000000000005e-08], loss:2.2572946548461914\n",
      "Epoch: 204, batch index: 3265, learning rate: [1.0000000000000005e-08], loss:2.267711639404297\n",
      "Epoch: 204, batch index: 3266, learning rate: [1.0000000000000005e-08], loss:2.26784610748291\n",
      "Epoch: 204, batch index: 3267, learning rate: [1.0000000000000005e-08], loss:2.2948451042175293\n",
      "Epoch: 204, batch index: 3268, learning rate: [1.0000000000000005e-08], loss:2.2810585498809814\n",
      "Epoch: 204, batch index: 3269, learning rate: [1.0000000000000005e-08], loss:2.272254467010498\n",
      "Epoch: 204, batch index: 3270, learning rate: [1.0000000000000005e-08], loss:2.275804281234741\n",
      "Epoch: 204, batch index: 3271, learning rate: [1.0000000000000005e-08], loss:2.268827438354492\n",
      "Epoch: 204, batch index: 3272, learning rate: [1.0000000000000005e-08], loss:2.2714412212371826\n",
      "Epoch: 204, batch index: 3273, learning rate: [1.0000000000000005e-08], loss:2.2982351779937744\n",
      "Epoch: 204, batch index: 3274, learning rate: [1.0000000000000005e-08], loss:2.2494657039642334\n",
      "Epoch: 204, batch index: 3275, learning rate: [1.0000000000000005e-08], loss:2.279951333999634\n",
      "Epoch: 204, batch index: 3276, learning rate: [1.0000000000000005e-08], loss:2.2570900917053223\n",
      "Epoch: 204, batch index: 3277, learning rate: [1.0000000000000005e-08], loss:2.2712759971618652\n",
      "Epoch: 204, batch index: 3278, learning rate: [1.0000000000000005e-08], loss:2.2438955307006836\n",
      "Epoch: 204, batch index: 3279, learning rate: [1.0000000000000005e-08], loss:2.3080458641052246\n",
      "Epoch 204, validation accuracy score0.2890625\n",
      "Epoch: 205, batch index: 3280, learning rate: [1.0000000000000005e-08], loss:2.2800509929656982\n",
      "Epoch: 205, batch index: 3281, learning rate: [1.0000000000000005e-08], loss:2.274014472961426\n",
      "Epoch: 205, batch index: 3282, learning rate: [1.0000000000000005e-08], loss:2.2720539569854736\n",
      "Epoch: 205, batch index: 3283, learning rate: [1.0000000000000005e-08], loss:2.28048038482666\n",
      "Epoch: 205, batch index: 3284, learning rate: [1.0000000000000005e-08], loss:2.263005495071411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, validation_loader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print the performance\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Perform one step of stochastic gradient descent\u001b[39;00m\n\u001b[1;32m     47\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, validation_loader, epochs=500, learning_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept, with just two labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50.layer4.0.conv1.weight True\n",
      "resnet50.layer4.0.bn1.weight True\n",
      "resnet50.layer4.0.bn1.bias True\n",
      "resnet50.layer4.0.conv2.weight True\n",
      "resnet50.layer4.0.bn2.weight True\n",
      "resnet50.layer4.0.bn2.bias True\n",
      "resnet50.layer4.0.conv3.weight True\n",
      "resnet50.layer4.0.bn3.weight True\n",
      "resnet50.layer4.0.bn3.bias True\n",
      "resnet50.layer4.0.downsample.0.weight True\n",
      "resnet50.layer4.0.downsample.1.weight True\n",
      "resnet50.layer4.0.downsample.1.bias True\n",
      "resnet50.layer4.1.conv1.weight True\n",
      "resnet50.layer4.1.bn1.weight True\n",
      "resnet50.layer4.1.bn1.bias True\n",
      "resnet50.layer4.1.conv2.weight True\n",
      "resnet50.layer4.1.bn2.weight True\n",
      "resnet50.layer4.1.bn2.bias True\n",
      "resnet50.layer4.1.conv3.weight True\n",
      "resnet50.layer4.1.bn3.weight True\n",
      "resnet50.layer4.1.bn3.bias True\n",
      "resnet50.layer4.2.conv1.weight True\n",
      "resnet50.layer4.2.bn1.weight True\n",
      "resnet50.layer4.2.bn1.bias True\n",
      "resnet50.layer4.2.conv2.weight True\n",
      "resnet50.layer4.2.bn2.weight True\n",
      "resnet50.layer4.2.bn2.bias True\n",
      "resnet50.layer4.2.conv3.weight True\n",
      "resnet50.layer4.2.bn3.weight True\n",
      "resnet50.layer4.2.bn3.bias True\n",
      "resnet50.fc.weight True\n",
      "resnet50.fc.bias True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>root_category</th>\n",
       "      <th>root_category_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>47c3e7cf-9996-4e73-a0b4-8b7fb4ca5d0e</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>e07140ff-9bb9-4fc4-b156-13335159bbfb</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>646c85b0-8ea8-4904-b234-8fbf55d8e1ed</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>c65bf600-a038-4ce2-be43-a1867a4be5ec</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>29f4c193-47ba-41b0-89e6-bdfba5938d86</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1995</td>\n",
       "      <td>be6eb99e-e9ee-4974-9d95-4e369c59e665</td>\n",
       "      <td>Baby &amp; Kids Stuff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1996</td>\n",
       "      <td>ab439d5f-529c-4ac3-a0b9-4e74ee2b65e7</td>\n",
       "      <td>Baby &amp; Kids Stuff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1997</td>\n",
       "      <td>00ae22cf-1298-4a11-b416-15c2be484816</td>\n",
       "      <td>Baby &amp; Kids Stuff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1998</td>\n",
       "      <td>108dd408-b1ba-42b1-b90d-599bc61e2e00</td>\n",
       "      <td>Baby &amp; Kids Stuff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1999</td>\n",
       "      <td>8d348c17-5bbc-4603-aa81-551047278ade</td>\n",
       "      <td>Baby &amp; Kids Stuff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                    id      root_category  \\\n",
       "0     1000  47c3e7cf-9996-4e73-a0b4-8b7fb4ca5d0e      Home & Garden   \n",
       "1     1001  e07140ff-9bb9-4fc4-b156-13335159bbfb      Home & Garden   \n",
       "2     1002  646c85b0-8ea8-4904-b234-8fbf55d8e1ed      Home & Garden   \n",
       "3     1003  c65bf600-a038-4ce2-be43-a1867a4be5ec      Home & Garden   \n",
       "4     1004  29f4c193-47ba-41b0-89e6-bdfba5938d86      Home & Garden   \n",
       "..     ...                                   ...                ...   \n",
       "995   1995  be6eb99e-e9ee-4974-9d95-4e369c59e665  Baby & Kids Stuff   \n",
       "996   1996  ab439d5f-529c-4ac3-a0b9-4e74ee2b65e7  Baby & Kids Stuff   \n",
       "997   1997  00ae22cf-1298-4a11-b416-15c2be484816  Baby & Kids Stuff   \n",
       "998   1998  108dd408-b1ba-42b1-b90d-599bc61e2e00  Baby & Kids Stuff   \n",
       "999   1999  8d348c17-5bbc-4603-aa81-551047278ade  Baby & Kids Stuff   \n",
       "\n",
       "     root_category_index  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "..                   ...  \n",
       "995                    1  \n",
       "996                    1  \n",
       "997                    1  \n",
       "998                    1  \n",
       "999                    1  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = merged_df[1000:2000]\n",
    "\n",
    "small_df['root_category']\n",
    "\n",
    "small_df.reset_index(inplace=True)\n",
    "\n",
    "small_dataset = ImageDataset(small_df, \"Datasets/cleaned_images_224/\")\n",
    "\n",
    "small_train, small_valid = torch.utils.data.random_split(\n",
    "    small_dataset, [800, 200]\n",
    ")\n",
    "\n",
    "small_train_loader = DataLoader(small_train, batch_size=150, shuffle=True, num_workers=4)\n",
    "small_validation_loader = DataLoader(small_valid, batch_size=50, shuffle=True)\n",
    "\n",
    "small_model = ImageClassifier(2)\n",
    "\n",
    "small_model.cuda()\n",
    "\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch index: 0, learning rate: [0.01], loss:0.7139397859573364\n",
      "Epoch: 0, batch index: 1, learning rate: [0.01], loss:0.721604585647583\n",
      "Epoch: 0, batch index: 2, learning rate: [0.01], loss:0.7015841007232666\n",
      "Epoch: 0, batch index: 3, learning rate: [0.01], loss:0.7155447006225586\n",
      "Epoch: 0, batch index: 4, learning rate: [0.01], loss:0.7009787559509277\n",
      "Epoch: 0, batch index: 5, learning rate: [0.01], loss:0.6873529553413391\n",
      "Epoch 0, validation accuracy score0.5\n",
      "0\n",
      "Epoch: 1, batch index: 6, learning rate: [0.01], loss:0.6977428793907166\n",
      "Epoch: 1, batch index: 7, learning rate: [0.01], loss:0.6752026677131653\n",
      "Epoch: 1, batch index: 8, learning rate: [0.01], loss:0.7035495638847351\n",
      "Epoch: 1, batch index: 9, learning rate: [0.01], loss:0.68355792760849\n",
      "Epoch: 1, batch index: 10, learning rate: [0.01], loss:0.7017321586608887\n",
      "Epoch: 1, batch index: 11, learning rate: [0.01], loss:0.7194548845291138\n",
      "Epoch 1, validation accuracy score0.62\n",
      "Epoch: 2, batch index: 12, learning rate: [0.01], loss:0.6565775275230408\n",
      "Epoch: 2, batch index: 13, learning rate: [0.01], loss:0.6863741874694824\n",
      "Epoch: 2, batch index: 14, learning rate: [0.01], loss:0.6590659022331238\n",
      "Epoch: 2, batch index: 15, learning rate: [0.01], loss:0.6661736369132996\n",
      "Epoch: 2, batch index: 16, learning rate: [0.01], loss:0.6744915246963501\n",
      "Epoch: 2, batch index: 17, learning rate: [0.01], loss:0.6749553680419922\n",
      "Epoch 2, validation accuracy score0.62\n",
      "Epoch: 3, batch index: 18, learning rate: [0.01], loss:0.6593137979507446\n",
      "Epoch: 3, batch index: 19, learning rate: [0.01], loss:0.640881359577179\n",
      "Epoch: 3, batch index: 20, learning rate: [0.01], loss:0.6650545001029968\n",
      "Epoch: 3, batch index: 21, learning rate: [0.01], loss:0.6632366180419922\n",
      "Epoch: 3, batch index: 22, learning rate: [0.01], loss:0.6401305198669434\n",
      "Epoch: 3, batch index: 23, learning rate: [0.01], loss:0.6430630683898926\n",
      "Epoch 3, validation accuracy score0.64\n",
      "Epoch: 4, batch index: 24, learning rate: [0.01], loss:0.6629364490509033\n",
      "Epoch: 4, batch index: 25, learning rate: [0.01], loss:0.6444830298423767\n",
      "Epoch: 4, batch index: 26, learning rate: [0.01], loss:0.6635818481445312\n",
      "Epoch: 4, batch index: 27, learning rate: [0.01], loss:0.638132631778717\n",
      "Epoch: 4, batch index: 28, learning rate: [0.01], loss:0.6627590656280518\n",
      "Epoch: 4, batch index: 29, learning rate: [0.01], loss:0.6703020334243774\n",
      "Epoch 4, validation accuracy score0.56\n",
      "Epoch: 5, batch index: 30, learning rate: [0.01], loss:0.6387649774551392\n",
      "Epoch: 5, batch index: 31, learning rate: [0.01], loss:0.6301238536834717\n",
      "Epoch: 5, batch index: 32, learning rate: [0.01], loss:0.6315459609031677\n",
      "Epoch: 5, batch index: 33, learning rate: [0.01], loss:0.6578671932220459\n",
      "Epoch: 5, batch index: 34, learning rate: [0.01], loss:0.6192444562911987\n",
      "Epoch: 5, batch index: 35, learning rate: [0.01], loss:0.638373851776123\n",
      "Epoch 5, validation accuracy score0.76\n",
      "Epoch: 6, batch index: 36, learning rate: [0.01], loss:0.6192139983177185\n",
      "Epoch: 6, batch index: 37, learning rate: [0.01], loss:0.6484629511833191\n",
      "Epoch: 6, batch index: 38, learning rate: [0.01], loss:0.6236534118652344\n",
      "Epoch: 6, batch index: 39, learning rate: [0.01], loss:0.5901869535446167\n",
      "Epoch: 6, batch index: 40, learning rate: [0.01], loss:0.6260061860084534\n",
      "Epoch: 6, batch index: 41, learning rate: [0.01], loss:0.592500627040863\n",
      "Epoch 6, validation accuracy score0.82\n",
      "Epoch: 7, batch index: 42, learning rate: [0.01], loss:0.5884606838226318\n",
      "Epoch: 7, batch index: 43, learning rate: [0.01], loss:0.5980209112167358\n",
      "Epoch: 7, batch index: 44, learning rate: [0.01], loss:0.6180152297019958\n",
      "Epoch: 7, batch index: 45, learning rate: [0.01], loss:0.5751959085464478\n",
      "Epoch: 7, batch index: 46, learning rate: [0.01], loss:0.6012956500053406\n",
      "Epoch: 7, batch index: 47, learning rate: [0.01], loss:0.6072295308113098\n",
      "Epoch 7, validation accuracy score0.76\n",
      "Epoch: 8, batch index: 48, learning rate: [0.01], loss:0.6094545722007751\n",
      "Epoch: 8, batch index: 49, learning rate: [0.01], loss:0.6119109392166138\n",
      "Epoch: 8, batch index: 50, learning rate: [0.01], loss:0.5908535122871399\n",
      "Epoch: 8, batch index: 51, learning rate: [0.01], loss:0.5797547698020935\n",
      "Epoch: 8, batch index: 52, learning rate: [0.01], loss:0.5699859261512756\n",
      "Epoch: 8, batch index: 53, learning rate: [0.01], loss:0.5749608874320984\n",
      "Epoch 8, validation accuracy score0.74\n",
      "Epoch: 9, batch index: 54, learning rate: [0.01], loss:0.5789570212364197\n",
      "Epoch: 9, batch index: 55, learning rate: [0.01], loss:0.5751025080680847\n",
      "Epoch: 9, batch index: 56, learning rate: [0.01], loss:0.5934373736381531\n",
      "Epoch: 9, batch index: 57, learning rate: [0.01], loss:0.5759795904159546\n",
      "Epoch: 9, batch index: 58, learning rate: [0.01], loss:0.5775598883628845\n",
      "Epoch: 9, batch index: 59, learning rate: [0.01], loss:0.5360351800918579\n",
      "Epoch 9, validation accuracy score0.76\n",
      "Epoch: 10, batch index: 60, learning rate: [0.01], loss:0.5711401104927063\n",
      "Epoch: 10, batch index: 61, learning rate: [0.01], loss:0.5742433071136475\n",
      "Epoch: 10, batch index: 62, learning rate: [0.01], loss:0.5883126854896545\n",
      "Epoch: 10, batch index: 63, learning rate: [0.01], loss:0.5743992924690247\n",
      "Epoch: 10, batch index: 64, learning rate: [0.01], loss:0.604522705078125\n",
      "Epoch: 10, batch index: 65, learning rate: [0.01], loss:0.5553011894226074\n",
      "Epoch 10, validation accuracy score0.82\n",
      "Epoch: 11, batch index: 66, learning rate: [0.01], loss:0.5633941888809204\n",
      "Epoch: 11, batch index: 67, learning rate: [0.01], loss:0.5589637756347656\n",
      "Epoch: 11, batch index: 68, learning rate: [0.01], loss:0.5663684606552124\n",
      "Epoch: 11, batch index: 69, learning rate: [0.01], loss:0.539581835269928\n",
      "Epoch: 11, batch index: 70, learning rate: [0.01], loss:0.5467451810836792\n",
      "Epoch: 11, batch index: 71, learning rate: [0.01], loss:0.5632759928703308\n",
      "Epoch 11, validation accuracy score0.66\n",
      "Epoch: 12, batch index: 72, learning rate: [0.01], loss:0.5491401553153992\n",
      "Epoch: 12, batch index: 73, learning rate: [0.01], loss:0.5568441152572632\n",
      "Epoch: 12, batch index: 74, learning rate: [0.01], loss:0.5127610564231873\n",
      "Epoch: 12, batch index: 75, learning rate: [0.01], loss:0.577016294002533\n",
      "Epoch: 12, batch index: 76, learning rate: [0.01], loss:0.5459046363830566\n",
      "Epoch: 12, batch index: 77, learning rate: [0.01], loss:0.5947198867797852\n",
      "Epoch 12, validation accuracy score0.76\n",
      "Epoch: 13, batch index: 78, learning rate: [0.01], loss:0.5865567922592163\n",
      "Epoch: 13, batch index: 79, learning rate: [0.01], loss:0.5237665772438049\n",
      "Epoch: 13, batch index: 80, learning rate: [0.01], loss:0.5687219500541687\n",
      "Epoch: 13, batch index: 81, learning rate: [0.01], loss:0.5191420912742615\n",
      "Epoch: 13, batch index: 82, learning rate: [0.01], loss:0.5560848712921143\n",
      "Epoch: 13, batch index: 83, learning rate: [0.01], loss:0.4982631206512451\n",
      "Epoch 13, validation accuracy score0.82\n",
      "Epoch: 14, batch index: 84, learning rate: [0.01], loss:0.5583246350288391\n",
      "Epoch: 14, batch index: 85, learning rate: [0.01], loss:0.5210341811180115\n",
      "Epoch: 14, batch index: 86, learning rate: [0.01], loss:0.5696598291397095\n",
      "Epoch: 14, batch index: 87, learning rate: [0.01], loss:0.536970317363739\n",
      "Epoch: 14, batch index: 88, learning rate: [0.01], loss:0.5290417671203613\n",
      "Epoch: 14, batch index: 89, learning rate: [0.01], loss:0.5630074143409729\n",
      "Epoch 14, validation accuracy score0.78\n",
      "Epoch: 15, batch index: 90, learning rate: [0.01], loss:0.534548819065094\n",
      "Epoch: 15, batch index: 91, learning rate: [0.01], loss:0.5313037633895874\n",
      "Epoch: 15, batch index: 92, learning rate: [0.01], loss:0.5214247107505798\n",
      "Epoch: 15, batch index: 93, learning rate: [0.01], loss:0.49016043543815613\n",
      "Epoch: 15, batch index: 94, learning rate: [0.01], loss:0.49817657470703125\n",
      "Epoch: 15, batch index: 95, learning rate: [0.01], loss:0.5570457577705383\n",
      "Epoch 15, validation accuracy score0.84\n",
      "Epoch: 16, batch index: 96, learning rate: [0.01], loss:0.5072727203369141\n",
      "Epoch: 16, batch index: 97, learning rate: [0.01], loss:0.5280932188034058\n",
      "Epoch: 16, batch index: 98, learning rate: [0.01], loss:0.541884183883667\n",
      "Epoch: 16, batch index: 99, learning rate: [0.01], loss:0.5525268912315369\n",
      "Epoch: 16, batch index: 100, learning rate: [0.01], loss:0.5159991979598999\n",
      "Epoch: 16, batch index: 101, learning rate: [0.01], loss:0.49125367403030396\n",
      "Epoch 16, validation accuracy score0.84\n",
      "Epoch: 17, batch index: 102, learning rate: [0.01], loss:0.5035400986671448\n",
      "Epoch: 17, batch index: 103, learning rate: [0.01], loss:0.5306759476661682\n",
      "Epoch: 17, batch index: 104, learning rate: [0.01], loss:0.5118661522865295\n",
      "Epoch: 17, batch index: 105, learning rate: [0.01], loss:0.5153716206550598\n",
      "Epoch: 17, batch index: 106, learning rate: [0.01], loss:0.5297414064407349\n",
      "Epoch: 17, batch index: 107, learning rate: [0.01], loss:0.4689444303512573\n",
      "Epoch 17, validation accuracy score0.74\n",
      "Epoch: 18, batch index: 108, learning rate: [0.01], loss:0.48721492290496826\n",
      "Epoch: 18, batch index: 109, learning rate: [0.01], loss:0.48552465438842773\n",
      "Epoch: 18, batch index: 110, learning rate: [0.01], loss:0.49702271819114685\n",
      "Epoch: 18, batch index: 111, learning rate: [0.01], loss:0.5201334357261658\n",
      "Epoch: 18, batch index: 112, learning rate: [0.01], loss:0.5243019461631775\n",
      "Epoch: 18, batch index: 113, learning rate: [0.01], loss:0.46011266112327576\n",
      "Epoch 18, validation accuracy score0.9\n",
      "Epoch: 19, batch index: 114, learning rate: [0.01], loss:0.476310133934021\n",
      "Epoch: 19, batch index: 115, learning rate: [0.01], loss:0.47983983159065247\n",
      "Epoch: 19, batch index: 116, learning rate: [0.01], loss:0.49673858284950256\n",
      "Epoch: 19, batch index: 117, learning rate: [0.01], loss:0.5187782049179077\n",
      "Epoch: 19, batch index: 118, learning rate: [0.01], loss:0.5004624128341675\n",
      "Epoch: 19, batch index: 119, learning rate: [0.01], loss:0.532835066318512\n",
      "Epoch 19, validation accuracy score0.8\n",
      "Epoch: 20, batch index: 120, learning rate: [0.01], loss:0.49482107162475586\n",
      "Epoch: 20, batch index: 121, learning rate: [0.01], loss:0.499963641166687\n",
      "Epoch: 20, batch index: 122, learning rate: [0.01], loss:0.4818018078804016\n",
      "Epoch: 20, batch index: 123, learning rate: [0.01], loss:0.48286136984825134\n",
      "Epoch: 20, batch index: 124, learning rate: [0.01], loss:0.5101480484008789\n",
      "Epoch: 20, batch index: 125, learning rate: [0.01], loss:0.46991726756095886\n",
      "Epoch 20, validation accuracy score0.78\n",
      "Epoch: 21, batch index: 126, learning rate: [0.01], loss:0.48643285036087036\n",
      "Epoch: 21, batch index: 127, learning rate: [0.01], loss:0.485116571187973\n",
      "Epoch: 21, batch index: 128, learning rate: [0.01], loss:0.5097895264625549\n",
      "Epoch: 21, batch index: 129, learning rate: [0.01], loss:0.5069319605827332\n",
      "Epoch: 21, batch index: 130, learning rate: [0.01], loss:0.48644229769706726\n",
      "Epoch: 21, batch index: 131, learning rate: [0.01], loss:0.48017260432243347\n",
      "Epoch 21, validation accuracy score0.82\n",
      "Epoch: 22, batch index: 132, learning rate: [0.01], loss:0.46702778339385986\n",
      "Epoch: 22, batch index: 133, learning rate: [0.01], loss:0.48256149888038635\n",
      "Epoch: 22, batch index: 134, learning rate: [0.01], loss:0.49579039216041565\n",
      "Epoch: 22, batch index: 135, learning rate: [0.01], loss:0.4953414499759674\n",
      "Epoch: 22, batch index: 136, learning rate: [0.01], loss:0.5151641368865967\n",
      "Epoch: 22, batch index: 137, learning rate: [0.01], loss:0.4604838192462921\n",
      "Epoch 22, validation accuracy score0.84\n",
      "Epoch: 23, batch index: 138, learning rate: [0.01], loss:0.47193822264671326\n",
      "Epoch: 23, batch index: 139, learning rate: [0.01], loss:0.4945479929447174\n",
      "Epoch: 23, batch index: 140, learning rate: [0.01], loss:0.48878616094589233\n",
      "Epoch: 23, batch index: 141, learning rate: [0.01], loss:0.4903053939342499\n",
      "Epoch: 23, batch index: 142, learning rate: [0.01], loss:0.4894966185092926\n",
      "Epoch: 23, batch index: 143, learning rate: [0.01], loss:0.48390474915504456\n",
      "Epoch 23, validation accuracy score0.82\n",
      "Epoch: 24, batch index: 144, learning rate: [0.01], loss:0.4874930679798126\n",
      "Epoch: 24, batch index: 145, learning rate: [0.01], loss:0.465069979429245\n",
      "Epoch: 24, batch index: 146, learning rate: [0.01], loss:0.48812296986579895\n",
      "Epoch: 24, batch index: 147, learning rate: [0.01], loss:0.504132866859436\n",
      "Epoch: 24, batch index: 148, learning rate: [0.01], loss:0.48550164699554443\n",
      "Epoch: 24, batch index: 149, learning rate: [0.01], loss:0.48770275712013245\n",
      "Epoch 24, validation accuracy score0.76\n",
      "Epoch: 25, batch index: 150, learning rate: [0.01], loss:0.47770562767982483\n",
      "Epoch: 25, batch index: 151, learning rate: [0.01], loss:0.45123282074928284\n",
      "Epoch: 25, batch index: 152, learning rate: [0.01], loss:0.49253973364830017\n",
      "Epoch: 25, batch index: 153, learning rate: [0.01], loss:0.4631655812263489\n",
      "Epoch: 25, batch index: 154, learning rate: [0.01], loss:0.4646487832069397\n",
      "Epoch: 25, batch index: 155, learning rate: [0.01], loss:0.4989674687385559\n",
      "Epoch 25, validation accuracy score0.8\n",
      "Epoch: 26, batch index: 156, learning rate: [0.01], loss:0.4496346414089203\n",
      "Epoch: 26, batch index: 157, learning rate: [0.01], loss:0.47855785489082336\n",
      "Epoch: 26, batch index: 158, learning rate: [0.01], loss:0.45796361565589905\n",
      "Epoch: 26, batch index: 159, learning rate: [0.01], loss:0.4760853946208954\n",
      "Epoch: 26, batch index: 160, learning rate: [0.01], loss:0.5300564169883728\n",
      "Epoch: 26, batch index: 161, learning rate: [0.01], loss:0.4584144949913025\n",
      "Epoch 26, validation accuracy score0.74\n",
      "Epoch: 27, batch index: 162, learning rate: [0.01], loss:0.454539030790329\n",
      "Epoch: 27, batch index: 163, learning rate: [0.01], loss:0.4734764099121094\n",
      "Epoch: 27, batch index: 164, learning rate: [0.01], loss:0.5081430077552795\n",
      "Epoch: 27, batch index: 165, learning rate: [0.01], loss:0.4657279849052429\n",
      "Epoch: 27, batch index: 166, learning rate: [0.01], loss:0.44723862409591675\n",
      "Epoch: 27, batch index: 167, learning rate: [0.01], loss:0.48624664545059204\n",
      "Epoch 27, validation accuracy score0.82\n",
      "Epoch: 28, batch index: 168, learning rate: [0.01], loss:0.4788379371166229\n",
      "Epoch: 28, batch index: 169, learning rate: [0.01], loss:0.47448277473449707\n",
      "Epoch: 28, batch index: 170, learning rate: [0.01], loss:0.46307459473609924\n",
      "Epoch: 28, batch index: 171, learning rate: [0.01], loss:0.47777530550956726\n",
      "Epoch: 28, batch index: 172, learning rate: [0.01], loss:0.46545034646987915\n",
      "Epoch: 28, batch index: 173, learning rate: [0.01], loss:0.45238104462623596\n",
      "Epoch 28, validation accuracy score0.74\n",
      "Epoch: 29, batch index: 174, learning rate: [0.01], loss:0.45450714230537415\n",
      "Epoch: 29, batch index: 175, learning rate: [0.01], loss:0.4477560818195343\n",
      "Epoch: 29, batch index: 176, learning rate: [0.01], loss:0.4870113730430603\n",
      "Epoch: 29, batch index: 177, learning rate: [0.01], loss:0.4802521765232086\n",
      "Epoch: 29, batch index: 178, learning rate: [0.01], loss:0.4858666956424713\n",
      "Epoch: 29, batch index: 179, learning rate: [0.01], loss:0.49702271819114685\n",
      "Epoch 29, validation accuracy score0.76\n"
     ]
    }
   ],
   "source": [
    "train(small_model, small_train_loader, small_validation_loader, epochs=30, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_model = deepcopy(model)\n",
    "\n",
    "feature_extraction_model\n",
    "\n",
    "feature_extraction_model.resnet50.fc = torch.nn.Identity()\n",
    "\n",
    "torch.save(feature_extraction_model.state_dict(), 'model_evaluation/final_model/image_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the image processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29739/1248834949.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.resnet50(X.float()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0802, 0.0650, 0.0856, 0.0719, 0.0739, 0.1049, 0.0786, 0.0784, 0.0581,\n",
       "         0.0708, 0.0627, 0.0850, 0.0851]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from image_processor import process_image\n",
    "\n",
    "image_tensor = process_image(\"Datasets/images/ffff23f1-59fc-47bd-b0cd-186933803287.jpg\").to(device)\n",
    "\n",
    "model(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data_loader = DataLoader(train_dataset,batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(json_file_name:str, folder_of_images:str, df_of_keys):\n",
    "    transformer =  transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    dict_of_features = {}\n",
    "    list_of_image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "    for index in range(0, 100):#len(merged_df)):\n",
    "        image_path = list_of_image_paths[index]\n",
    "        with Image.open(image_path) as img:\n",
    "            features = transformer(img).unsqueeze(0)\n",
    "        features = features.to(device)\n",
    "        image_embedding = feature_extraction_model(features).tolist()\n",
    "        dict_of_features[df_of_keys['id'][index]] = image_embedding\n",
    "        print(dict_of_features, index)\n",
    "    with open('my_dict.json', '+w') as file:\n",
    "        json.dump(dict_of_features, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction('123', 'Datasets/cleaned_images_224/', merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'name1':'Superman'}\n",
    "\n",
    "with open('my_dict.json', '+w') as file:\n",
    "        json.dump(my_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
