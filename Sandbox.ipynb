{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import json \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Datasets/Products.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0             7156\n",
       " id                     7156\n",
       " product_name           7156\n",
       " category               7156\n",
       " product_description    7156\n",
       " price                  7156\n",
       " location               7156\n",
       " dtype: int64,\n",
       " Unnamed: 0             7156\n",
       " id                     7156\n",
       " product_name           7156\n",
       " category               7156\n",
       " product_description    7156\n",
       " price                  7156\n",
       " location               7156\n",
       " dtype: int64,\n",
       " array(['£'], dtype=object),\n",
       " array(['.00', '.99', '.78', '.01', '.97', '.25', '.50', '.20', '.90',\n",
       "        '.80', '.60', '.23', '.05', '.75', '.56', '.40', '.44', '.95',\n",
       "        '.66', '.35', '.85', '.30', '.45', '.16', '.69', '.49', '.55',\n",
       "        '.09', '.11'], dtype=object))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count(), dataset.dropna().count(), dataset[\"price\"].map(lambda x: x[0]).unique(), dataset[\"price\"].map(lambda x: x[-3:]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7156 entries, 0 to 7155\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Unnamed: 0           7156 non-null   int64 \n",
      " 1   id                   7156 non-null   object\n",
      " 2   product_name         7156 non-null   object\n",
      " 3   category             7156 non-null   object\n",
      " 4   product_description  7156 non-null   object\n",
      " 5   price                7156 non-null   object\n",
      " 6   location             7156 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 391.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the dataset seems to already be clean\n",
    "\n",
    "Now we convert the price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset\n",
    "\n",
    "def remove_pound_sign(string_to_replace) -> str:\n",
    "    return \n",
    "\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x : x.replace('£', '')) #Removes the pound signs\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x: x.replace(',', '')) #Removes commas\n",
    "\n",
    "dataset_cleaned['price'] = pd.to_numeric(dataset_cleaned['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we extract the root category from each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['category'] = dataset_cleaned['category'].map(lambda x: x.split(' /' )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home & Garden', 'Baby & Kids Stuff', 'DIY Tools & Materials',\n",
       "       'Music, Films, Books & Games', 'Phones, Mobile Phones & Telecoms',\n",
       "       'Clothes, Footwear & Accessories', 'Other Goods',\n",
       "       'Health & Beauty', 'Sports, Leisure & Travel', 'Appliances',\n",
       "       'Computers & Software', 'Office Furniture & Equipment',\n",
       "       'Video Games & Consoles'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an encoder to go to and from the categories and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_categories = list(dataset_cleaned['category'].unique())\n",
    "\n",
    "encoder = {x: list_of_categories.index(x) for x in list_of_categories}\n",
    "\n",
    "decoder = {list_of_categories.index(x):x for x in list_of_categories}\n",
    "\n",
    "encoder, decoder\n",
    "\n",
    "#Let's save these to pickle files:\n",
    "\n",
    "with open(\"encoder_pickle\", 'wb') as encody:\n",
    "    pickle.dump(encoder, encody)\n",
    "\n",
    "with open(\"decoder_pickle\", 'wb') as decody:\n",
    "    pickle.dump(decoder, decody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we merge this with the original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['root_category'] = dataset_cleaned['category']\n",
    "dataset_cleaned['root_category_index'] = dataset_cleaned['category'].map(lambda x:encoder[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we merge this with the images table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0               int64\n",
       " id                      object\n",
       " product_name            object\n",
       " category                object\n",
       " product_description     object\n",
       " price                  float64\n",
       " location                object\n",
       " root_category           object\n",
       " root_category_index      int64\n",
       " merge_column            object\n",
       " dtype: object,\n",
       " Unnamed: 0       int64\n",
       " id              object\n",
       " product_id      object\n",
       " merge_column    object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dataset = pd.read_csv('Datasets/Images.csv')\n",
    "\n",
    "dataset_cleaned[\"merge_column\"] = dataset_cleaned['id']\n",
    "images_dataset['merge_column'] = images_dataset['product_id']\n",
    "\n",
    "dataset_cleaned.dtypes, images_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = images_dataset.merge(dataset_cleaned, on='merge_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     object\n",
       "root_category          object\n",
       "root_category_index     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df[[\"id_x\", \"product_id\", \"root_category\", \"root_category_index\"]]\n",
    "merged_df = merged_df.rename(columns={'id_x':'id'})\n",
    "merged_df = merged_df.drop(columns=['product_id'])\n",
    "\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Datasets/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Image Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(final_size, im):\n",
    "    size = im.size\n",
    "    ratio = float(final_size) / max(size)\n",
    "    new_image_size = tuple([int(x*ratio) for x in size])\n",
    "    im = im.resize(new_image_size)\n",
    "    new_im = Image.new(\"RGB\", (final_size, final_size))\n",
    "    new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n",
    "    return new_im\n",
    "\n",
    "def clean_images(path_to_extract = \"Datasets/images/\", path_to_save = \"Datasets/cleaned_images/\", image_size = 64):\n",
    "    dirs = os.listdir(path_to_extract)\n",
    "    final_size = image_size\n",
    "    for n, item in enumerate(dirs, 1):\n",
    "        #print(n, item)\n",
    "        im = Image.open(path_to_extract + item)\n",
    "        #print(im.width, im.height)\n",
    "        new_im = resize_image(final_size, im)\n",
    "        #print(new_im.width, new_im.height)\n",
    "        new_im.save(path_to_save + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_images(path_to_save=\"Datasets/cleaned_images_64/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting into a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df_of_keys:pd.DataFrame, folder_of_images:str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the labels to be the column 'root_cotegory_index' of df_of_keys\n",
    "        self.labels = df_of_keys['root_category_index']\n",
    "\n",
    "        # Assings image_paths to the file name from the column 'id' of df_of_keys and maps it to it's path relative to the project root folder\n",
    "        self.image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "\n",
    "        # The Resent50 documentation said these transforms must be applied to the images before the model processes them\n",
    "        self.image_transformer = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Opens the image at index with using PIL.Image\n",
    "        with Image.open(self.image_paths[index]) as img:\n",
    "\n",
    "            # Sets the feature to be a tensor obtained by applying PILToTensor to the relevant image\n",
    "            X = self.image_transformer(img)\n",
    "        \n",
    "        # y is the label from self.labels\n",
    "        y = self.labels[index]\n",
    "\n",
    "        #print(X.shape, y.shape)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " (),\n",
       " 12604)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = ImageDataset(merged_df, \"Datasets/cleaned_images_224/\")\n",
    "\n",
    "# A quick sanity check of the dataset:\n",
    "my_dataset[1][0], my_dataset[1][1].shape, len(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split merged_df to a train_df and validation_df\n",
    "train_df, validation_df = train_test_split(merged_df, train_size=10000, test_size= len(merged_df) - 10000)\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "validation_df.reset_index(inplace=True)\n",
    "\n",
    "# Create instances of the ImageDataset class\n",
    "train_dataset = ImageDataset(train_df, \"Datasets/cleaned_images_224/\")\n",
    "validation_dataset = ImageDataset(validation_df, \"Datasets/cleaned_images_224/\")\n",
    "\n",
    "# Allow random flips and rotations when loading an image in the train dataset\n",
    "train_dataset.image_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.PILToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Pass both to dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pillow_to_image = transforms.ToPILImage()\n",
    "\n",
    "pillow_to_image(validation_dataset[712][0].squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ibs/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets the device for PyTorch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Loads the resnet50 model\n",
    "#resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', model = 'nvidia_resnet50', pretrained = True )\n",
    "resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "# Don't know what this does\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "# Again don't know what this does\n",
    "resnet50.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the model\n",
    "\n",
    "Let's print out the layers of resnet50, and if they're differentiable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_layers(model):\n",
    "    for name, module in model.named_parameters():\n",
    "        if module.requires_grad:\n",
    "            print('Here!', name, module.requires_grad)\n",
    "        else:\n",
    "            print(name, module.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here! conv1.weight True\n",
      "Here! bn1.weight True\n",
      "Here! bn1.bias True\n",
      "Here! layer1.0.conv1.weight True\n",
      "Here! layer1.0.bn1.weight True\n",
      "Here! layer1.0.bn1.bias True\n",
      "Here! layer1.0.conv2.weight True\n",
      "Here! layer1.0.bn2.weight True\n",
      "Here! layer1.0.bn2.bias True\n",
      "Here! layer1.0.conv3.weight True\n",
      "Here! layer1.0.bn3.weight True\n",
      "Here! layer1.0.bn3.bias True\n",
      "Here! layer1.0.downsample.0.weight True\n",
      "Here! layer1.0.downsample.1.weight True\n",
      "Here! layer1.0.downsample.1.bias True\n",
      "Here! layer1.1.conv1.weight True\n",
      "Here! layer1.1.bn1.weight True\n",
      "Here! layer1.1.bn1.bias True\n",
      "Here! layer1.1.conv2.weight True\n",
      "Here! layer1.1.bn2.weight True\n",
      "Here! layer1.1.bn2.bias True\n",
      "Here! layer1.1.conv3.weight True\n",
      "Here! layer1.1.bn3.weight True\n",
      "Here! layer1.1.bn3.bias True\n",
      "Here! layer1.2.conv1.weight True\n",
      "Here! layer1.2.bn1.weight True\n",
      "Here! layer1.2.bn1.bias True\n",
      "Here! layer1.2.conv2.weight True\n",
      "Here! layer1.2.bn2.weight True\n",
      "Here! layer1.2.bn2.bias True\n",
      "Here! layer1.2.conv3.weight True\n",
      "Here! layer1.2.bn3.weight True\n",
      "Here! layer1.2.bn3.bias True\n",
      "Here! layer2.0.conv1.weight True\n",
      "Here! layer2.0.bn1.weight True\n",
      "Here! layer2.0.bn1.bias True\n",
      "Here! layer2.0.conv2.weight True\n",
      "Here! layer2.0.bn2.weight True\n",
      "Here! layer2.0.bn2.bias True\n",
      "Here! layer2.0.conv3.weight True\n",
      "Here! layer2.0.bn3.weight True\n",
      "Here! layer2.0.bn3.bias True\n",
      "Here! layer2.0.downsample.0.weight True\n",
      "Here! layer2.0.downsample.1.weight True\n",
      "Here! layer2.0.downsample.1.bias True\n",
      "Here! layer2.1.conv1.weight True\n",
      "Here! layer2.1.bn1.weight True\n",
      "Here! layer2.1.bn1.bias True\n",
      "Here! layer2.1.conv2.weight True\n",
      "Here! layer2.1.bn2.weight True\n",
      "Here! layer2.1.bn2.bias True\n",
      "Here! layer2.1.conv3.weight True\n",
      "Here! layer2.1.bn3.weight True\n",
      "Here! layer2.1.bn3.bias True\n",
      "Here! layer2.2.conv1.weight True\n",
      "Here! layer2.2.bn1.weight True\n",
      "Here! layer2.2.bn1.bias True\n",
      "Here! layer2.2.conv2.weight True\n",
      "Here! layer2.2.bn2.weight True\n",
      "Here! layer2.2.bn2.bias True\n",
      "Here! layer2.2.conv3.weight True\n",
      "Here! layer2.2.bn3.weight True\n",
      "Here! layer2.2.bn3.bias True\n",
      "Here! layer2.3.conv1.weight True\n",
      "Here! layer2.3.bn1.weight True\n",
      "Here! layer2.3.bn1.bias True\n",
      "Here! layer2.3.conv2.weight True\n",
      "Here! layer2.3.bn2.weight True\n",
      "Here! layer2.3.bn2.bias True\n",
      "Here! layer2.3.conv3.weight True\n",
      "Here! layer2.3.bn3.weight True\n",
      "Here! layer2.3.bn3.bias True\n",
      "Here! layer3.0.conv1.weight True\n",
      "Here! layer3.0.bn1.weight True\n",
      "Here! layer3.0.bn1.bias True\n",
      "Here! layer3.0.conv2.weight True\n",
      "Here! layer3.0.bn2.weight True\n",
      "Here! layer3.0.bn2.bias True\n",
      "Here! layer3.0.conv3.weight True\n",
      "Here! layer3.0.bn3.weight True\n",
      "Here! layer3.0.bn3.bias True\n",
      "Here! layer3.0.downsample.0.weight True\n",
      "Here! layer3.0.downsample.1.weight True\n",
      "Here! layer3.0.downsample.1.bias True\n",
      "Here! layer3.1.conv1.weight True\n",
      "Here! layer3.1.bn1.weight True\n",
      "Here! layer3.1.bn1.bias True\n",
      "Here! layer3.1.conv2.weight True\n",
      "Here! layer3.1.bn2.weight True\n",
      "Here! layer3.1.bn2.bias True\n",
      "Here! layer3.1.conv3.weight True\n",
      "Here! layer3.1.bn3.weight True\n",
      "Here! layer3.1.bn3.bias True\n",
      "Here! layer3.2.conv1.weight True\n",
      "Here! layer3.2.bn1.weight True\n",
      "Here! layer3.2.bn1.bias True\n",
      "Here! layer3.2.conv2.weight True\n",
      "Here! layer3.2.bn2.weight True\n",
      "Here! layer3.2.bn2.bias True\n",
      "Here! layer3.2.conv3.weight True\n",
      "Here! layer3.2.bn3.weight True\n",
      "Here! layer3.2.bn3.bias True\n",
      "Here! layer3.3.conv1.weight True\n",
      "Here! layer3.3.bn1.weight True\n",
      "Here! layer3.3.bn1.bias True\n",
      "Here! layer3.3.conv2.weight True\n",
      "Here! layer3.3.bn2.weight True\n",
      "Here! layer3.3.bn2.bias True\n",
      "Here! layer3.3.conv3.weight True\n",
      "Here! layer3.3.bn3.weight True\n",
      "Here! layer3.3.bn3.bias True\n",
      "Here! layer3.4.conv1.weight True\n",
      "Here! layer3.4.bn1.weight True\n",
      "Here! layer3.4.bn1.bias True\n",
      "Here! layer3.4.conv2.weight True\n",
      "Here! layer3.4.bn2.weight True\n",
      "Here! layer3.4.bn2.bias True\n",
      "Here! layer3.4.conv3.weight True\n",
      "Here! layer3.4.bn3.weight True\n",
      "Here! layer3.4.bn3.bias True\n",
      "Here! layer3.5.conv1.weight True\n",
      "Here! layer3.5.bn1.weight True\n",
      "Here! layer3.5.bn1.bias True\n",
      "Here! layer3.5.conv2.weight True\n",
      "Here! layer3.5.bn2.weight True\n",
      "Here! layer3.5.bn2.bias True\n",
      "Here! layer3.5.conv3.weight True\n",
      "Here! layer3.5.bn3.weight True\n",
      "Here! layer3.5.bn3.bias True\n",
      "Here! layer4.0.conv1.weight True\n",
      "Here! layer4.0.bn1.weight True\n",
      "Here! layer4.0.bn1.bias True\n",
      "Here! layer4.0.conv2.weight True\n",
      "Here! layer4.0.bn2.weight True\n",
      "Here! layer4.0.bn2.bias True\n",
      "Here! layer4.0.conv3.weight True\n",
      "Here! layer4.0.bn3.weight True\n",
      "Here! layer4.0.bn3.bias True\n",
      "Here! layer4.0.downsample.0.weight True\n",
      "Here! layer4.0.downsample.1.weight True\n",
      "Here! layer4.0.downsample.1.bias True\n",
      "Here! layer4.1.conv1.weight True\n",
      "Here! layer4.1.bn1.weight True\n",
      "Here! layer4.1.bn1.bias True\n",
      "Here! layer4.1.conv2.weight True\n",
      "Here! layer4.1.bn2.weight True\n",
      "Here! layer4.1.bn2.bias True\n",
      "Here! layer4.1.conv3.weight True\n",
      "Here! layer4.1.bn3.weight True\n",
      "Here! layer4.1.bn3.bias True\n",
      "Here! layer4.2.conv1.weight True\n",
      "Here! layer4.2.bn1.weight True\n",
      "Here! layer4.2.bn1.bias True\n",
      "Here! layer4.2.conv2.weight True\n",
      "Here! layer4.2.bn2.weight True\n",
      "Here! layer4.2.bn2.bias True\n",
      "Here! layer4.2.conv3.weight True\n",
      "Here! layer4.2.bn3.weight True\n",
      "Here! layer4.2.bn3.bias True\n",
      "Here! fc.weight True\n",
      "Here! fc.bias True\n"
     ]
    }
   ],
   "source": [
    "show_model_layers(resnet50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final layer, `fc` takes in 2048 features and outputs 1000 features. The number of labels we need to classify, i.e. the number of distinct values in `merged_df['root_categories']` is 13.\n",
    "\n",
    "- So we need to modify the `fc` layer so that it has 13 outputs. We do this by changing `fc` to `torch.nn.Linear(2048, 13)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_labels:int) -> None:\n",
    "        super().__init__()\n",
    "        self.resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze all the layers, by setting 'requires_grad = False'\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the final layer of renset50:\n",
    "\n",
    "        for param in self.resnet50.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Add a linear layer reducing the 1000 label output to num_labels\n",
    "        \n",
    "        self.final = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, num_labels),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.float()\n",
    "        X = self.resnet50(X)\n",
    "        X = self.final(X)\n",
    "        return X\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward_no_grad(self, X):\n",
    "        X = X.float()\n",
    "        X = self.resnet50(X)\n",
    "        X = self.final(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=1000, out_features=13, bias=True)\n",
       "    (2): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImageClassifier(13)\n",
    "model.cuda()\n",
    "\n",
    "#show_model_layers(model)\n",
    "\n",
    "#accuracy_score_from_valiadation(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1):\n",
    "    torch.cuda.memory_summary(device=device, abbreviated=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    # Only want the parameters in fc to be updated\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=7, eps=1e-20)\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "    \n",
    "    # Prints an validation score\n",
    "    print(f\"Initial validation accuracy score{accuracy_score_from_valiadation(model, validation_loader)}\")\n",
    "\n",
    "    #Create a dictionary to store the best model parameters\n",
    "    best_model_parameters = {'Epoch':-1, 'Accuracy':0, 'Parameters':model.state_dict()}\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = F.cross_entropy(model(features), labels)\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "        \n",
    "        # Print the validation loss\n",
    "        print('Calculating validation accuracy')\n",
    "        accuracy = accuracy_score_from_valiadation(model, validation_loader)\n",
    "        print(f\"Epoch {epoch}, validation accuracy score{accuracy}\")\n",
    "\n",
    "        # Check if the model has the best perfomrance and save the parameters to 'best_model.pt'\n",
    "        if accuracy > best_model_parameters['Accuracy']:\n",
    "            best_model_parameters['Epoch'] = epoch\n",
    "            best_model_parameters['Accuracy'] = loss.item()\n",
    "            best_model_parameters['Parameters'] = model.state_dict()\n",
    "            torch.save(model.state_dict(), 'model_evaluation/weights/best_model.pt')\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch % 50)\n",
    "            # Create an instance of the datetime class\n",
    "            dt = datetime.now()\n",
    "            date_stamp = str(dt).replace(':', '_').replace('.', '_').replace(' ', '_')\n",
    "\n",
    "            # Save the model parameters to the folder 'model_evaluation/weights', along with the time and epoch they were generated\n",
    "            torch.save(model.state_dict(), f'model_evaluation/weights/model_{date_stamp}_epoch_{epoch}_accuracy_{accuracy}.pt') # Is there a better way to do this?\n",
    "    \n",
    "    print('Loading best model')\n",
    "    \n",
    "    #Update model parameters with the best model parameters:\n",
    "    model.load_state_dict(best_model_parameters['Parameters'])\n",
    "    print(f'The best model has validation accuracy {accuracy_score_from_valiadation(model, validation_loader)}')\n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    \n",
    "    list_of_scores = []\n",
    "\n",
    "    for batch in validation_loader:\n",
    "        features, labels = batch\n",
    "\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predictions = torch.tensor([torch.argmax(prediction) for prediction in model(features)]).to(device)\n",
    "        num_correct =  torch.sum(predictions == labels)\n",
    "\n",
    "        list_of_scores.append(torch.div(num_correct, len(labels)).item())\n",
    "    \n",
    "    return sum(list_of_scores) / len(list_of_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score0.0671875\n",
      "Epoch: 0, batch index: 0, learning rate: [1], loss:2.5718088150024414\n",
      "Epoch: 0, batch index: 1, learning rate: [1], loss:2.524702310562134\n",
      "Epoch: 0, batch index: 2, learning rate: [1], loss:2.6260454654693604\n",
      "Epoch: 0, batch index: 3, learning rate: [1], loss:2.6024794578552246\n",
      "Epoch: 0, batch index: 4, learning rate: [1], loss:2.610278606414795\n",
      "Epoch: 0, batch index: 5, learning rate: [1], loss:2.5862069129943848\n",
      "Epoch: 0, batch index: 6, learning rate: [1], loss:2.5980868339538574\n",
      "Epoch: 0, batch index: 7, learning rate: [1], loss:2.520627975463867\n",
      "Epoch: 0, batch index: 8, learning rate: [1], loss:2.5608558654785156\n",
      "Epoch: 0, batch index: 9, learning rate: [1], loss:2.580871343612671\n",
      "Epoch: 0, batch index: 10, learning rate: [1], loss:2.595214366912842\n",
      "Epoch: 0, batch index: 11, learning rate: [1], loss:2.5403835773468018\n",
      "Epoch: 0, batch index: 12, learning rate: [1], loss:2.5337564945220947\n",
      "Epoch: 0, batch index: 13, learning rate: [1], loss:2.520751714706421\n",
      "Epoch: 0, batch index: 14, learning rate: [1], loss:2.5288894176483154\n",
      "Epoch: 0, batch index: 15, learning rate: [1], loss:2.48891019821167\n",
      "Epoch: 0, batch index: 16, learning rate: [1], loss:2.4627466201782227\n",
      "Epoch: 0, batch index: 17, learning rate: [1], loss:2.551082134246826\n",
      "Epoch: 0, batch index: 18, learning rate: [1], loss:2.475996732711792\n",
      "Epoch: 0, batch index: 19, learning rate: [1], loss:2.4808831214904785\n",
      "Epoch: 0, batch index: 20, learning rate: [1], loss:2.517869234085083\n",
      "Epoch: 0, batch index: 21, learning rate: [1], loss:2.4572064876556396\n",
      "Epoch: 0, batch index: 22, learning rate: [1], loss:2.3361387252807617\n",
      "Epoch: 0, batch index: 23, learning rate: [1], loss:2.452176570892334\n",
      "Epoch: 0, batch index: 24, learning rate: [1], loss:2.474357843399048\n",
      "Epoch: 0, batch index: 25, learning rate: [1], loss:2.469874382019043\n",
      "Epoch: 0, batch index: 26, learning rate: [1], loss:2.4208083152770996\n",
      "Epoch: 0, batch index: 27, learning rate: [1], loss:2.4351439476013184\n",
      "Epoch: 0, batch index: 28, learning rate: [1], loss:2.375274419784546\n",
      "Epoch: 0, batch index: 29, learning rate: [1], loss:2.519000291824341\n",
      "Epoch: 0, batch index: 30, learning rate: [1], loss:2.424156665802002\n",
      "Epoch: 0, batch index: 31, learning rate: [1], loss:2.427860975265503\n",
      "Epoch: 0, batch index: 32, learning rate: [1], loss:2.3758187294006348\n",
      "Epoch: 0, batch index: 33, learning rate: [1], loss:2.4018301963806152\n",
      "Epoch: 0, batch index: 34, learning rate: [1], loss:2.378206253051758\n",
      "Epoch: 0, batch index: 35, learning rate: [1], loss:2.5072436332702637\n",
      "Epoch: 0, batch index: 36, learning rate: [1], loss:2.424121379852295\n",
      "Epoch: 0, batch index: 37, learning rate: [1], loss:2.3441214561462402\n",
      "Epoch: 0, batch index: 38, learning rate: [1], loss:2.3942298889160156\n",
      "Epoch: 0, batch index: 39, learning rate: [1], loss:2.3315844535827637\n",
      "Epoch: 0, batch index: 40, learning rate: [1], loss:2.399181604385376\n",
      "Epoch: 0, batch index: 41, learning rate: [1], loss:2.409261703491211\n",
      "Epoch: 0, batch index: 42, learning rate: [1], loss:2.400649070739746\n",
      "Epoch: 0, batch index: 43, learning rate: [1], loss:2.374964475631714\n",
      "Epoch: 0, batch index: 44, learning rate: [1], loss:2.4140114784240723\n",
      "Epoch: 0, batch index: 45, learning rate: [1], loss:2.3038711547851562\n",
      "Epoch: 0, batch index: 46, learning rate: [1], loss:2.38905668258667\n",
      "Epoch: 0, batch index: 47, learning rate: [1], loss:2.350741386413574\n",
      "Epoch: 0, batch index: 48, learning rate: [1], loss:2.3329508304595947\n",
      "Epoch: 0, batch index: 49, learning rate: [1], loss:2.4570295810699463\n",
      "Epoch: 0, batch index: 50, learning rate: [1], loss:2.341183662414551\n",
      "Epoch: 0, batch index: 51, learning rate: [1], loss:2.3367648124694824\n",
      "Epoch: 0, batch index: 52, learning rate: [1], loss:2.364567518234253\n",
      "Epoch: 0, batch index: 53, learning rate: [1], loss:2.378178358078003\n",
      "Epoch: 0, batch index: 54, learning rate: [1], loss:2.392751455307007\n",
      "Epoch: 0, batch index: 55, learning rate: [1], loss:2.3782429695129395\n",
      "Epoch: 0, batch index: 56, learning rate: [1], loss:2.3754723072052\n",
      "Epoch: 0, batch index: 57, learning rate: [1], loss:2.3805766105651855\n",
      "Epoch: 0, batch index: 58, learning rate: [1], loss:2.3389732837677\n",
      "Epoch: 0, batch index: 59, learning rate: [1], loss:2.3487768173217773\n",
      "Epoch: 0, batch index: 60, learning rate: [1], loss:2.3190982341766357\n",
      "Epoch: 0, batch index: 61, learning rate: [1], loss:2.3306515216827393\n",
      "Epoch: 0, batch index: 62, learning rate: [1], loss:2.3369014263153076\n",
      "Epoch: 0, batch index: 63, learning rate: [1], loss:2.3014073371887207\n",
      "Epoch: 0, batch index: 64, learning rate: [1], loss:2.351630449295044\n",
      "Epoch: 0, batch index: 65, learning rate: [1], loss:2.3008694648742676\n",
      "Epoch: 0, batch index: 66, learning rate: [1], loss:2.32340407371521\n",
      "Epoch: 0, batch index: 67, learning rate: [1], loss:2.345470428466797\n",
      "Epoch: 0, batch index: 68, learning rate: [1], loss:2.4125218391418457\n",
      "Epoch: 0, batch index: 69, learning rate: [1], loss:2.377546787261963\n",
      "Epoch: 0, batch index: 70, learning rate: [1], loss:2.3729615211486816\n",
      "Epoch: 0, batch index: 71, learning rate: [1], loss:2.392484188079834\n",
      "Epoch: 0, batch index: 72, learning rate: [1], loss:2.3255131244659424\n",
      "Epoch: 0, batch index: 73, learning rate: [1], loss:2.3486900329589844\n",
      "Epoch: 0, batch index: 74, learning rate: [1], loss:2.2778186798095703\n",
      "Epoch: 0, batch index: 75, learning rate: [1], loss:2.334796905517578\n",
      "Epoch: 0, batch index: 76, learning rate: [1], loss:2.2781014442443848\n",
      "Epoch: 0, batch index: 77, learning rate: [1], loss:2.3431880474090576\n",
      "Epoch: 0, batch index: 78, learning rate: [1], loss:2.2902920246124268\n",
      "Calculating validation accuracy\n",
      "Epoch 0, validation accuracy score0.35703125\n",
      "0\n",
      "Epoch: 1, batch index: 79, learning rate: [1], loss:2.3641357421875\n",
      "Epoch: 1, batch index: 80, learning rate: [1], loss:2.3433854579925537\n",
      "Epoch: 1, batch index: 81, learning rate: [1], loss:2.319257974624634\n",
      "Epoch: 1, batch index: 82, learning rate: [1], loss:2.4412806034088135\n",
      "Epoch: 1, batch index: 83, learning rate: [1], loss:2.4110214710235596\n",
      "Epoch: 1, batch index: 84, learning rate: [1], loss:2.3751611709594727\n",
      "Epoch: 1, batch index: 85, learning rate: [1], loss:2.299863338470459\n",
      "Epoch: 1, batch index: 86, learning rate: [1], loss:2.375663995742798\n",
      "Epoch: 1, batch index: 87, learning rate: [1], loss:2.4069066047668457\n",
      "Epoch: 1, batch index: 88, learning rate: [1], loss:2.371354579925537\n",
      "Epoch: 1, batch index: 89, learning rate: [1], loss:2.2821333408355713\n",
      "Epoch: 1, batch index: 90, learning rate: [1], loss:2.3067233562469482\n",
      "Epoch: 1, batch index: 91, learning rate: [1], loss:2.351837635040283\n",
      "Epoch: 1, batch index: 92, learning rate: [1], loss:2.3762872219085693\n",
      "Epoch: 1, batch index: 93, learning rate: [1], loss:2.3213727474212646\n",
      "Epoch: 1, batch index: 94, learning rate: [1], loss:2.354823589324951\n",
      "Epoch: 1, batch index: 95, learning rate: [1], loss:2.272865056991577\n",
      "Epoch: 1, batch index: 96, learning rate: [1], loss:2.369499444961548\n",
      "Epoch: 1, batch index: 97, learning rate: [1], loss:2.3432819843292236\n",
      "Epoch: 1, batch index: 98, learning rate: [1], loss:2.3312432765960693\n",
      "Epoch: 1, batch index: 99, learning rate: [1], loss:2.3394787311553955\n",
      "Epoch: 1, batch index: 100, learning rate: [1], loss:2.298664093017578\n",
      "Epoch: 1, batch index: 101, learning rate: [1], loss:2.3703832626342773\n",
      "Epoch: 1, batch index: 102, learning rate: [1], loss:2.3669869899749756\n",
      "Epoch: 1, batch index: 103, learning rate: [1], loss:2.3437819480895996\n",
      "Epoch: 1, batch index: 104, learning rate: [1], loss:2.3803489208221436\n",
      "Epoch: 1, batch index: 105, learning rate: [1], loss:2.3558692932128906\n",
      "Epoch: 1, batch index: 106, learning rate: [1], loss:2.3893442153930664\n",
      "Epoch: 1, batch index: 107, learning rate: [1], loss:2.311340093612671\n",
      "Epoch: 1, batch index: 108, learning rate: [1], loss:2.2948641777038574\n",
      "Epoch: 1, batch index: 109, learning rate: [1], loss:2.3641281127929688\n",
      "Epoch: 1, batch index: 110, learning rate: [1], loss:2.345252275466919\n",
      "Epoch: 1, batch index: 111, learning rate: [1], loss:2.3369340896606445\n",
      "Epoch: 1, batch index: 112, learning rate: [1], loss:2.3378000259399414\n",
      "Epoch: 1, batch index: 113, learning rate: [1], loss:2.3088464736938477\n",
      "Epoch: 1, batch index: 114, learning rate: [1], loss:2.331716299057007\n",
      "Epoch: 1, batch index: 115, learning rate: [1], loss:2.301729917526245\n",
      "Epoch: 1, batch index: 116, learning rate: [1], loss:2.3123722076416016\n",
      "Epoch: 1, batch index: 117, learning rate: [1], loss:2.3586983680725098\n",
      "Epoch: 1, batch index: 118, learning rate: [1], loss:2.299717426300049\n",
      "Epoch: 1, batch index: 119, learning rate: [1], loss:2.2231557369232178\n",
      "Epoch: 1, batch index: 120, learning rate: [1], loss:2.2426164150238037\n",
      "Epoch: 1, batch index: 121, learning rate: [1], loss:2.316532611846924\n",
      "Epoch: 1, batch index: 122, learning rate: [1], loss:2.224246025085449\n",
      "Epoch: 1, batch index: 123, learning rate: [1], loss:2.3323020935058594\n",
      "Epoch: 1, batch index: 124, learning rate: [1], loss:2.3123130798339844\n",
      "Epoch: 1, batch index: 125, learning rate: [1], loss:2.2890169620513916\n",
      "Epoch: 1, batch index: 126, learning rate: [1], loss:2.2875542640686035\n",
      "Epoch: 1, batch index: 127, learning rate: [1], loss:2.3104097843170166\n",
      "Epoch: 1, batch index: 128, learning rate: [1], loss:2.2403268814086914\n",
      "Epoch: 1, batch index: 129, learning rate: [1], loss:2.265960216522217\n",
      "Epoch: 1, batch index: 130, learning rate: [1], loss:2.2743277549743652\n",
      "Epoch: 1, batch index: 131, learning rate: [1], loss:2.3044090270996094\n",
      "Epoch: 1, batch index: 132, learning rate: [1], loss:2.338120937347412\n",
      "Epoch: 1, batch index: 133, learning rate: [1], loss:2.439493417739868\n",
      "Epoch: 1, batch index: 134, learning rate: [1], loss:2.389058828353882\n",
      "Epoch: 1, batch index: 135, learning rate: [1], loss:2.2950668334960938\n",
      "Epoch: 1, batch index: 136, learning rate: [1], loss:2.322442054748535\n",
      "Epoch: 1, batch index: 137, learning rate: [1], loss:2.350121259689331\n",
      "Epoch: 1, batch index: 138, learning rate: [1], loss:2.4076337814331055\n",
      "Epoch: 1, batch index: 139, learning rate: [1], loss:2.324937105178833\n",
      "Epoch: 1, batch index: 140, learning rate: [1], loss:2.295661449432373\n",
      "Epoch: 1, batch index: 141, learning rate: [1], loss:2.316929817199707\n",
      "Epoch: 1, batch index: 142, learning rate: [1], loss:2.2734827995300293\n",
      "Epoch: 1, batch index: 143, learning rate: [1], loss:2.3228931427001953\n",
      "Epoch: 1, batch index: 144, learning rate: [1], loss:2.3354241847991943\n",
      "Epoch: 1, batch index: 145, learning rate: [1], loss:2.2793898582458496\n",
      "Epoch: 1, batch index: 146, learning rate: [1], loss:2.364401340484619\n",
      "Epoch: 1, batch index: 147, learning rate: [1], loss:2.3072571754455566\n",
      "Epoch: 1, batch index: 148, learning rate: [1], loss:2.3620619773864746\n",
      "Epoch: 1, batch index: 149, learning rate: [1], loss:2.366954803466797\n",
      "Epoch: 1, batch index: 150, learning rate: [1], loss:2.297495126724243\n",
      "Epoch: 1, batch index: 151, learning rate: [1], loss:2.351924180984497\n",
      "Epoch: 1, batch index: 152, learning rate: [1], loss:2.2546122074127197\n",
      "Epoch: 1, batch index: 153, learning rate: [1], loss:2.303621292114258\n",
      "Epoch: 1, batch index: 154, learning rate: [1], loss:2.3418595790863037\n",
      "Epoch: 1, batch index: 155, learning rate: [1], loss:2.304025650024414\n",
      "Epoch: 1, batch index: 156, learning rate: [1], loss:2.2646756172180176\n",
      "Epoch: 1, batch index: 157, learning rate: [1], loss:2.5305657386779785\n",
      "Calculating validation accuracy\n",
      "Epoch 1, validation accuracy score0.3140625\n",
      "Epoch: 2, batch index: 158, learning rate: [1], loss:2.412468433380127\n",
      "Epoch: 2, batch index: 159, learning rate: [1], loss:2.2963006496429443\n",
      "Epoch: 2, batch index: 160, learning rate: [1], loss:2.3462376594543457\n",
      "Epoch: 2, batch index: 161, learning rate: [1], loss:2.391280174255371\n",
      "Epoch: 2, batch index: 162, learning rate: [1], loss:2.3333981037139893\n",
      "Epoch: 2, batch index: 163, learning rate: [1], loss:2.250509023666382\n",
      "Epoch: 2, batch index: 164, learning rate: [1], loss:2.2588984966278076\n",
      "Epoch: 2, batch index: 165, learning rate: [1], loss:2.331037998199463\n",
      "Epoch: 2, batch index: 166, learning rate: [1], loss:2.2766077518463135\n",
      "Epoch: 2, batch index: 167, learning rate: [1], loss:2.2834181785583496\n",
      "Epoch: 2, batch index: 168, learning rate: [1], loss:2.3049354553222656\n",
      "Epoch: 2, batch index: 169, learning rate: [1], loss:2.305816173553467\n",
      "Epoch: 2, batch index: 170, learning rate: [1], loss:2.3421926498413086\n",
      "Epoch: 2, batch index: 171, learning rate: [0.1], loss:2.2513420581817627\n",
      "Epoch: 2, batch index: 172, learning rate: [0.1], loss:2.3311843872070312\n",
      "Epoch: 2, batch index: 173, learning rate: [0.1], loss:2.3118228912353516\n",
      "Epoch: 2, batch index: 174, learning rate: [0.1], loss:2.3042736053466797\n",
      "Epoch: 2, batch index: 175, learning rate: [0.1], loss:2.3348467350006104\n",
      "Epoch: 2, batch index: 176, learning rate: [0.1], loss:2.3089652061462402\n",
      "Epoch: 2, batch index: 177, learning rate: [0.1], loss:2.167912006378174\n",
      "Epoch: 2, batch index: 178, learning rate: [0.1], loss:2.2842347621917725\n",
      "Epoch: 2, batch index: 179, learning rate: [0.1], loss:2.3634228706359863\n",
      "Epoch: 2, batch index: 180, learning rate: [0.1], loss:2.2040040493011475\n",
      "Epoch: 2, batch index: 181, learning rate: [0.1], loss:2.298619031906128\n",
      "Epoch: 2, batch index: 182, learning rate: [0.1], loss:2.352747678756714\n",
      "Epoch: 2, batch index: 183, learning rate: [0.1], loss:2.3120615482330322\n",
      "Epoch: 2, batch index: 184, learning rate: [0.1], loss:2.2421364784240723\n",
      "Epoch: 2, batch index: 185, learning rate: [0.1], loss:2.311060667037964\n",
      "Epoch: 2, batch index: 186, learning rate: [0.1], loss:2.1994566917419434\n",
      "Epoch: 2, batch index: 187, learning rate: [0.1], loss:2.319877862930298\n",
      "Epoch: 2, batch index: 188, learning rate: [0.1], loss:2.3080830574035645\n",
      "Epoch: 2, batch index: 189, learning rate: [0.1], loss:2.2951231002807617\n",
      "Epoch: 2, batch index: 190, learning rate: [0.1], loss:2.3723702430725098\n",
      "Epoch: 2, batch index: 191, learning rate: [0.1], loss:2.316528558731079\n",
      "Epoch: 2, batch index: 192, learning rate: [0.1], loss:2.2522902488708496\n",
      "Epoch: 2, batch index: 193, learning rate: [0.1], loss:2.223320245742798\n",
      "Epoch: 2, batch index: 194, learning rate: [0.1], loss:2.2552952766418457\n",
      "Epoch: 2, batch index: 195, learning rate: [0.1], loss:2.313746929168701\n",
      "Epoch: 2, batch index: 196, learning rate: [0.1], loss:2.230962038040161\n",
      "Epoch: 2, batch index: 197, learning rate: [0.1], loss:2.3184492588043213\n",
      "Epoch: 2, batch index: 198, learning rate: [0.1], loss:2.3322484493255615\n",
      "Epoch: 2, batch index: 199, learning rate: [0.1], loss:2.2850840091705322\n",
      "Epoch: 2, batch index: 200, learning rate: [0.1], loss:2.2539494037628174\n",
      "Epoch: 2, batch index: 201, learning rate: [0.1], loss:2.3152503967285156\n",
      "Epoch: 2, batch index: 202, learning rate: [0.1], loss:2.2838289737701416\n",
      "Epoch: 2, batch index: 203, learning rate: [0.1], loss:2.287968158721924\n",
      "Epoch: 2, batch index: 204, learning rate: [0.1], loss:2.372293710708618\n",
      "Epoch: 2, batch index: 205, learning rate: [0.1], loss:2.2558956146240234\n",
      "Epoch: 2, batch index: 206, learning rate: [0.1], loss:2.35569167137146\n",
      "Epoch: 2, batch index: 207, learning rate: [0.1], loss:2.297550916671753\n",
      "Epoch: 2, batch index: 208, learning rate: [0.1], loss:2.1896727085113525\n",
      "Epoch: 2, batch index: 209, learning rate: [0.1], loss:2.2934529781341553\n",
      "Epoch: 2, batch index: 210, learning rate: [0.1], loss:2.3194003105163574\n",
      "Epoch: 2, batch index: 211, learning rate: [0.1], loss:2.3023791313171387\n",
      "Epoch: 2, batch index: 212, learning rate: [0.1], loss:2.2776122093200684\n",
      "Epoch: 2, batch index: 213, learning rate: [0.1], loss:2.3029000759124756\n",
      "Epoch: 2, batch index: 214, learning rate: [0.1], loss:2.3187997341156006\n",
      "Epoch: 2, batch index: 215, learning rate: [0.1], loss:2.3426356315612793\n",
      "Epoch: 2, batch index: 216, learning rate: [0.1], loss:2.271583318710327\n",
      "Epoch: 2, batch index: 217, learning rate: [0.1], loss:2.256941795349121\n",
      "Epoch: 2, batch index: 218, learning rate: [0.1], loss:2.315063953399658\n",
      "Epoch: 2, batch index: 219, learning rate: [0.1], loss:2.281564474105835\n",
      "Epoch: 2, batch index: 220, learning rate: [0.1], loss:2.222705125808716\n",
      "Epoch: 2, batch index: 221, learning rate: [0.1], loss:2.314863443374634\n",
      "Epoch: 2, batch index: 222, learning rate: [0.1], loss:2.296694040298462\n",
      "Epoch: 2, batch index: 223, learning rate: [0.1], loss:2.2842955589294434\n",
      "Epoch: 2, batch index: 224, learning rate: [0.1], loss:2.294374942779541\n",
      "Epoch: 2, batch index: 225, learning rate: [0.1], loss:2.2917251586914062\n",
      "Epoch: 2, batch index: 226, learning rate: [0.1], loss:2.2650468349456787\n",
      "Epoch: 2, batch index: 227, learning rate: [0.1], loss:2.293149709701538\n",
      "Epoch: 2, batch index: 228, learning rate: [0.1], loss:2.2803783416748047\n",
      "Epoch: 2, batch index: 229, learning rate: [0.010000000000000002], loss:2.247479200363159\n",
      "Epoch: 2, batch index: 230, learning rate: [0.010000000000000002], loss:2.300987482070923\n",
      "Epoch: 2, batch index: 231, learning rate: [0.010000000000000002], loss:2.2950305938720703\n",
      "Epoch: 2, batch index: 232, learning rate: [0.010000000000000002], loss:2.2974116802215576\n",
      "Epoch: 2, batch index: 233, learning rate: [0.010000000000000002], loss:2.3105711936950684\n",
      "Epoch: 2, batch index: 234, learning rate: [0.010000000000000002], loss:2.2796037197113037\n",
      "Epoch: 2, batch index: 235, learning rate: [0.010000000000000002], loss:2.254302501678467\n",
      "Epoch: 2, batch index: 236, learning rate: [0.010000000000000002], loss:2.1235098838806152\n",
      "Calculating validation accuracy\n",
      "Epoch 2, validation accuracy score0.4234375\n",
      "Epoch: 3, batch index: 237, learning rate: [0.010000000000000002], loss:2.308134078979492\n",
      "Epoch: 3, batch index: 238, learning rate: [0.010000000000000002], loss:2.294938325881958\n",
      "Epoch: 3, batch index: 239, learning rate: [0.010000000000000002], loss:2.2886741161346436\n",
      "Epoch: 3, batch index: 240, learning rate: [0.010000000000000002], loss:2.3527824878692627\n",
      "Epoch: 3, batch index: 241, learning rate: [0.010000000000000002], loss:2.2405529022216797\n",
      "Epoch: 3, batch index: 242, learning rate: [0.010000000000000002], loss:2.2649006843566895\n",
      "Epoch: 3, batch index: 243, learning rate: [0.010000000000000002], loss:2.221986770629883\n",
      "Epoch: 3, batch index: 244, learning rate: [0.010000000000000002], loss:2.2928764820098877\n",
      "Epoch: 3, batch index: 245, learning rate: [0.010000000000000002], loss:2.304349660873413\n",
      "Epoch: 3, batch index: 246, learning rate: [0.010000000000000002], loss:2.3435754776000977\n",
      "Epoch: 3, batch index: 247, learning rate: [0.010000000000000002], loss:2.317991018295288\n",
      "Epoch: 3, batch index: 248, learning rate: [0.010000000000000002], loss:2.256211519241333\n",
      "Epoch: 3, batch index: 249, learning rate: [0.010000000000000002], loss:2.2552950382232666\n",
      "Epoch: 3, batch index: 250, learning rate: [0.010000000000000002], loss:2.2947351932525635\n",
      "Epoch: 3, batch index: 251, learning rate: [0.010000000000000002], loss:2.3012924194335938\n",
      "Epoch: 3, batch index: 252, learning rate: [0.010000000000000002], loss:2.300546646118164\n",
      "Epoch: 3, batch index: 253, learning rate: [0.010000000000000002], loss:2.278472661972046\n",
      "Epoch: 3, batch index: 254, learning rate: [0.010000000000000002], loss:2.290273904800415\n",
      "Epoch: 3, batch index: 255, learning rate: [0.010000000000000002], loss:2.2839295864105225\n",
      "Epoch: 3, batch index: 256, learning rate: [0.010000000000000002], loss:2.3194727897644043\n",
      "Epoch: 3, batch index: 257, learning rate: [0.010000000000000002], loss:2.3069424629211426\n",
      "Epoch: 3, batch index: 258, learning rate: [0.010000000000000002], loss:2.232802152633667\n",
      "Epoch: 3, batch index: 259, learning rate: [0.010000000000000002], loss:2.2650458812713623\n",
      "Epoch: 3, batch index: 260, learning rate: [0.010000000000000002], loss:2.275794744491577\n",
      "Epoch: 3, batch index: 261, learning rate: [0.010000000000000002], loss:2.3294527530670166\n",
      "Epoch: 3, batch index: 262, learning rate: [0.010000000000000002], loss:2.259181022644043\n",
      "Epoch: 3, batch index: 263, learning rate: [0.010000000000000002], loss:2.2167935371398926\n",
      "Epoch: 3, batch index: 264, learning rate: [0.010000000000000002], loss:2.2786366939544678\n",
      "Epoch: 3, batch index: 265, learning rate: [0.010000000000000002], loss:2.334275722503662\n",
      "Epoch: 3, batch index: 266, learning rate: [0.010000000000000002], loss:2.2450599670410156\n",
      "Epoch: 3, batch index: 267, learning rate: [0.010000000000000002], loss:2.251392126083374\n",
      "Epoch: 3, batch index: 268, learning rate: [0.010000000000000002], loss:2.2801718711853027\n",
      "Epoch: 3, batch index: 269, learning rate: [0.010000000000000002], loss:2.269315242767334\n",
      "Epoch: 3, batch index: 270, learning rate: [0.010000000000000002], loss:2.2416796684265137\n",
      "Epoch: 3, batch index: 271, learning rate: [0.010000000000000002], loss:2.2940495014190674\n",
      "Epoch: 3, batch index: 272, learning rate: [0.010000000000000002], loss:2.2922136783599854\n",
      "Epoch: 3, batch index: 273, learning rate: [0.010000000000000002], loss:2.354218006134033\n",
      "Epoch: 3, batch index: 274, learning rate: [0.010000000000000002], loss:2.2318015098571777\n",
      "Epoch: 3, batch index: 275, learning rate: [0.010000000000000002], loss:2.261268138885498\n",
      "Epoch: 3, batch index: 276, learning rate: [0.010000000000000002], loss:2.359476327896118\n",
      "Epoch: 3, batch index: 277, learning rate: [0.010000000000000002], loss:2.3170697689056396\n",
      "Epoch: 3, batch index: 278, learning rate: [0.010000000000000002], loss:2.3291451930999756\n",
      "Epoch: 3, batch index: 279, learning rate: [0.010000000000000002], loss:2.2564404010772705\n",
      "Epoch: 3, batch index: 280, learning rate: [0.010000000000000002], loss:2.323892593383789\n",
      "Epoch: 3, batch index: 281, learning rate: [0.010000000000000002], loss:2.273984670639038\n",
      "Epoch: 3, batch index: 282, learning rate: [0.010000000000000002], loss:2.2760424613952637\n",
      "Epoch: 3, batch index: 283, learning rate: [0.010000000000000002], loss:2.3581104278564453\n",
      "Epoch: 3, batch index: 284, learning rate: [0.010000000000000002], loss:2.281536817550659\n",
      "Epoch: 3, batch index: 285, learning rate: [0.010000000000000002], loss:2.2791965007781982\n",
      "Epoch: 3, batch index: 286, learning rate: [0.010000000000000002], loss:2.283865451812744\n",
      "Epoch: 3, batch index: 287, learning rate: [0.010000000000000002], loss:2.279557704925537\n",
      "Epoch: 3, batch index: 288, learning rate: [0.0010000000000000002], loss:2.228947877883911\n",
      "Epoch: 3, batch index: 289, learning rate: [0.0010000000000000002], loss:2.3002681732177734\n",
      "Epoch: 3, batch index: 290, learning rate: [0.0010000000000000002], loss:2.2947452068328857\n",
      "Epoch: 3, batch index: 291, learning rate: [0.0010000000000000002], loss:2.2956645488739014\n",
      "Epoch: 3, batch index: 292, learning rate: [0.0010000000000000002], loss:2.2754883766174316\n",
      "Epoch: 3, batch index: 293, learning rate: [0.0010000000000000002], loss:2.3260955810546875\n",
      "Epoch: 3, batch index: 294, learning rate: [0.0010000000000000002], loss:2.2459754943847656\n",
      "Epoch: 3, batch index: 295, learning rate: [0.0010000000000000002], loss:2.349750518798828\n",
      "Epoch: 3, batch index: 296, learning rate: [0.0010000000000000002], loss:2.2743425369262695\n",
      "Epoch: 3, batch index: 297, learning rate: [0.0010000000000000002], loss:2.268042802810669\n",
      "Epoch: 3, batch index: 298, learning rate: [0.0010000000000000002], loss:2.2790720462799072\n",
      "Epoch: 3, batch index: 299, learning rate: [0.0010000000000000002], loss:2.362199306488037\n",
      "Epoch: 3, batch index: 300, learning rate: [0.0010000000000000002], loss:2.257929801940918\n",
      "Epoch: 3, batch index: 301, learning rate: [0.0010000000000000002], loss:2.266947031021118\n",
      "Epoch: 3, batch index: 302, learning rate: [0.0010000000000000002], loss:2.2611281871795654\n",
      "Epoch: 3, batch index: 303, learning rate: [0.0010000000000000002], loss:2.266810894012451\n",
      "Epoch: 3, batch index: 304, learning rate: [0.0010000000000000002], loss:2.2970898151397705\n",
      "Epoch: 3, batch index: 305, learning rate: [0.0010000000000000002], loss:2.3443150520324707\n",
      "Epoch: 3, batch index: 306, learning rate: [0.0010000000000000002], loss:2.283318281173706\n",
      "Epoch: 3, batch index: 307, learning rate: [0.0010000000000000002], loss:2.267826795578003\n",
      "Epoch: 3, batch index: 308, learning rate: [0.0010000000000000002], loss:2.1890172958374023\n",
      "Epoch: 3, batch index: 309, learning rate: [0.0010000000000000002], loss:2.2632055282592773\n",
      "Epoch: 3, batch index: 310, learning rate: [0.0010000000000000002], loss:2.3312602043151855\n",
      "Epoch: 3, batch index: 311, learning rate: [0.0010000000000000002], loss:2.239976406097412\n",
      "Epoch: 3, batch index: 312, learning rate: [0.0010000000000000002], loss:2.2472643852233887\n",
      "Epoch: 3, batch index: 313, learning rate: [0.0010000000000000002], loss:2.301516056060791\n",
      "Epoch: 3, batch index: 314, learning rate: [0.0010000000000000002], loss:2.2801928520202637\n",
      "Epoch: 3, batch index: 315, learning rate: [0.0010000000000000002], loss:2.487854242324829\n",
      "Calculating validation accuracy\n",
      "Epoch 3, validation accuracy score0.42578125\n",
      "Epoch: 4, batch index: 316, learning rate: [0.0010000000000000002], loss:2.3003792762756348\n",
      "Epoch: 4, batch index: 317, learning rate: [0.0010000000000000002], loss:2.2703421115875244\n",
      "Epoch: 4, batch index: 318, learning rate: [0.0010000000000000002], loss:2.3042173385620117\n",
      "Epoch: 4, batch index: 319, learning rate: [0.0010000000000000002], loss:2.2769627571105957\n",
      "Epoch: 4, batch index: 320, learning rate: [0.0010000000000000002], loss:2.2256879806518555\n",
      "Epoch: 4, batch index: 321, learning rate: [0.0010000000000000002], loss:2.3010032176971436\n",
      "Epoch: 4, batch index: 322, learning rate: [0.0010000000000000002], loss:2.2649128437042236\n",
      "Epoch: 4, batch index: 323, learning rate: [0.0010000000000000002], loss:2.286149740219116\n",
      "Epoch: 4, batch index: 324, learning rate: [0.0010000000000000002], loss:2.278822183609009\n",
      "Epoch: 4, batch index: 325, learning rate: [0.0010000000000000002], loss:2.2753705978393555\n",
      "Epoch: 4, batch index: 326, learning rate: [0.0010000000000000002], loss:2.2457759380340576\n",
      "Epoch: 4, batch index: 327, learning rate: [0.0010000000000000002], loss:2.333833932876587\n",
      "Epoch: 4, batch index: 328, learning rate: [0.0010000000000000002], loss:2.3527328968048096\n",
      "Epoch: 4, batch index: 329, learning rate: [0.0010000000000000002], loss:2.2830722332000732\n",
      "Epoch: 4, batch index: 330, learning rate: [0.0010000000000000002], loss:2.2630672454833984\n",
      "Epoch: 4, batch index: 331, learning rate: [0.0010000000000000002], loss:2.3053553104400635\n",
      "Epoch: 4, batch index: 332, learning rate: [0.0010000000000000002], loss:2.253535747528076\n",
      "Epoch: 4, batch index: 333, learning rate: [0.0010000000000000002], loss:2.279758930206299\n",
      "Epoch: 4, batch index: 334, learning rate: [0.0010000000000000002], loss:2.277202844619751\n",
      "Epoch: 4, batch index: 335, learning rate: [0.0010000000000000002], loss:2.2479848861694336\n",
      "Epoch: 4, batch index: 336, learning rate: [0.0010000000000000002], loss:2.3232228755950928\n",
      "Epoch: 4, batch index: 337, learning rate: [0.0010000000000000002], loss:2.285764694213867\n",
      "Epoch: 4, batch index: 338, learning rate: [0.0010000000000000002], loss:2.2871999740600586\n",
      "Epoch: 4, batch index: 339, learning rate: [0.0010000000000000002], loss:2.188417911529541\n",
      "Epoch: 4, batch index: 340, learning rate: [0.0010000000000000002], loss:2.378746509552002\n",
      "Epoch: 4, batch index: 341, learning rate: [0.0010000000000000002], loss:2.3206710815429688\n",
      "Epoch: 4, batch index: 342, learning rate: [0.0010000000000000002], loss:2.275836229324341\n",
      "Epoch: 4, batch index: 343, learning rate: [0.0010000000000000002], loss:2.271179676055908\n",
      "Epoch: 4, batch index: 344, learning rate: [0.0010000000000000002], loss:2.323333501815796\n",
      "Epoch: 4, batch index: 345, learning rate: [0.0010000000000000002], loss:2.3006505966186523\n",
      "Epoch: 4, batch index: 346, learning rate: [0.00010000000000000003], loss:2.351494312286377\n",
      "Epoch: 4, batch index: 347, learning rate: [0.00010000000000000003], loss:2.324589490890503\n",
      "Epoch: 4, batch index: 348, learning rate: [0.00010000000000000003], loss:2.2556333541870117\n",
      "Epoch: 4, batch index: 349, learning rate: [0.00010000000000000003], loss:2.2982990741729736\n",
      "Epoch: 4, batch index: 350, learning rate: [0.00010000000000000003], loss:2.3125593662261963\n",
      "Epoch: 4, batch index: 351, learning rate: [0.00010000000000000003], loss:2.2769384384155273\n",
      "Epoch: 4, batch index: 352, learning rate: [0.00010000000000000003], loss:2.299825429916382\n",
      "Epoch: 4, batch index: 353, learning rate: [0.00010000000000000003], loss:2.261763334274292\n",
      "Epoch: 4, batch index: 354, learning rate: [0.00010000000000000003], loss:2.3501555919647217\n",
      "Epoch: 4, batch index: 355, learning rate: [0.00010000000000000003], loss:2.2632391452789307\n",
      "Epoch: 4, batch index: 356, learning rate: [0.00010000000000000003], loss:2.246896266937256\n",
      "Epoch: 4, batch index: 357, learning rate: [0.00010000000000000003], loss:2.3039727210998535\n",
      "Epoch: 4, batch index: 358, learning rate: [0.00010000000000000003], loss:2.2888081073760986\n",
      "Epoch: 4, batch index: 359, learning rate: [0.00010000000000000003], loss:2.2611875534057617\n",
      "Epoch: 4, batch index: 360, learning rate: [0.00010000000000000003], loss:2.2514121532440186\n",
      "Epoch: 4, batch index: 361, learning rate: [0.00010000000000000003], loss:2.2822117805480957\n",
      "Epoch: 4, batch index: 362, learning rate: [0.00010000000000000003], loss:2.2851004600524902\n",
      "Epoch: 4, batch index: 363, learning rate: [0.00010000000000000003], loss:2.271132230758667\n",
      "Epoch: 4, batch index: 364, learning rate: [0.00010000000000000003], loss:2.1969757080078125\n",
      "Epoch: 4, batch index: 365, learning rate: [0.00010000000000000003], loss:2.189180374145508\n",
      "Epoch: 4, batch index: 366, learning rate: [0.00010000000000000003], loss:2.3264729976654053\n",
      "Epoch: 4, batch index: 367, learning rate: [0.00010000000000000003], loss:2.2796576023101807\n",
      "Epoch: 4, batch index: 368, learning rate: [0.00010000000000000003], loss:2.3609697818756104\n",
      "Epoch: 4, batch index: 369, learning rate: [0.00010000000000000003], loss:2.253953695297241\n",
      "Epoch: 4, batch index: 370, learning rate: [0.00010000000000000003], loss:2.2724461555480957\n",
      "Epoch: 4, batch index: 371, learning rate: [0.00010000000000000003], loss:2.260362148284912\n",
      "Epoch: 4, batch index: 372, learning rate: [0.00010000000000000003], loss:2.2288904190063477\n",
      "Epoch: 4, batch index: 373, learning rate: [0.00010000000000000003], loss:2.2423410415649414\n",
      "Epoch: 4, batch index: 374, learning rate: [0.00010000000000000003], loss:2.300032377243042\n",
      "Epoch: 4, batch index: 375, learning rate: [0.00010000000000000003], loss:2.355821371078491\n",
      "Epoch: 4, batch index: 376, learning rate: [0.00010000000000000003], loss:2.2623887062072754\n",
      "Epoch: 4, batch index: 377, learning rate: [0.00010000000000000003], loss:2.2775092124938965\n",
      "Epoch: 4, batch index: 378, learning rate: [0.00010000000000000003], loss:2.302741527557373\n",
      "Epoch: 4, batch index: 379, learning rate: [0.00010000000000000003], loss:2.2987778186798096\n",
      "Epoch: 4, batch index: 380, learning rate: [0.00010000000000000003], loss:2.3149220943450928\n",
      "Epoch: 4, batch index: 381, learning rate: [0.00010000000000000003], loss:2.30204701423645\n",
      "Epoch: 4, batch index: 382, learning rate: [0.00010000000000000003], loss:2.2063491344451904\n",
      "Epoch: 4, batch index: 383, learning rate: [0.00010000000000000003], loss:2.3225221633911133\n",
      "Epoch: 4, batch index: 384, learning rate: [0.00010000000000000003], loss:2.2898547649383545\n",
      "Epoch: 4, batch index: 385, learning rate: [0.00010000000000000003], loss:2.301438808441162\n",
      "Epoch: 4, batch index: 386, learning rate: [0.00010000000000000003], loss:2.2743992805480957\n",
      "Epoch: 4, batch index: 387, learning rate: [0.00010000000000000003], loss:2.301724672317505\n",
      "Epoch: 4, batch index: 388, learning rate: [0.00010000000000000003], loss:2.268381357192993\n",
      "Epoch: 4, batch index: 389, learning rate: [0.00010000000000000003], loss:2.255159854888916\n",
      "Epoch: 4, batch index: 390, learning rate: [0.00010000000000000003], loss:2.3342092037200928\n",
      "Epoch: 4, batch index: 391, learning rate: [0.00010000000000000003], loss:2.2508621215820312\n",
      "Epoch: 4, batch index: 392, learning rate: [0.00010000000000000003], loss:2.25301194190979\n",
      "Epoch: 4, batch index: 393, learning rate: [0.00010000000000000003], loss:2.3368172645568848\n",
      "Epoch: 4, batch index: 394, learning rate: [0.00010000000000000003], loss:2.3561439514160156\n",
      "Calculating validation accuracy\n",
      "Epoch 4, validation accuracy score0.42421875\n",
      "Epoch: 5, batch index: 395, learning rate: [0.00010000000000000003], loss:2.3371987342834473\n",
      "Epoch: 5, batch index: 396, learning rate: [0.00010000000000000003], loss:2.215622901916504\n",
      "Epoch: 5, batch index: 397, learning rate: [0.00010000000000000003], loss:2.3173983097076416\n",
      "Epoch: 5, batch index: 398, learning rate: [0.00010000000000000003], loss:2.2511417865753174\n",
      "Epoch: 5, batch index: 399, learning rate: [0.00010000000000000003], loss:2.2395224571228027\n",
      "Epoch: 5, batch index: 400, learning rate: [0.00010000000000000003], loss:2.3279943466186523\n",
      "Epoch: 5, batch index: 401, learning rate: [0.00010000000000000003], loss:2.3051838874816895\n",
      "Epoch: 5, batch index: 402, learning rate: [0.00010000000000000003], loss:2.334017276763916\n",
      "Epoch: 5, batch index: 403, learning rate: [0.00010000000000000003], loss:2.2089180946350098\n",
      "Epoch: 5, batch index: 404, learning rate: [1.0000000000000004e-05], loss:2.2554569244384766\n",
      "Epoch: 5, batch index: 405, learning rate: [1.0000000000000004e-05], loss:2.253098726272583\n",
      "Epoch: 5, batch index: 406, learning rate: [1.0000000000000004e-05], loss:2.293104410171509\n",
      "Epoch: 5, batch index: 407, learning rate: [1.0000000000000004e-05], loss:2.23770809173584\n",
      "Epoch: 5, batch index: 408, learning rate: [1.0000000000000004e-05], loss:2.2385685443878174\n",
      "Epoch: 5, batch index: 409, learning rate: [1.0000000000000004e-05], loss:2.283494472503662\n",
      "Epoch: 5, batch index: 410, learning rate: [1.0000000000000004e-05], loss:2.2730302810668945\n",
      "Epoch: 5, batch index: 411, learning rate: [1.0000000000000004e-05], loss:2.2890913486480713\n",
      "Epoch: 5, batch index: 412, learning rate: [1.0000000000000004e-05], loss:2.333592653274536\n",
      "Epoch: 5, batch index: 413, learning rate: [1.0000000000000004e-05], loss:2.325639486312866\n",
      "Epoch: 5, batch index: 414, learning rate: [1.0000000000000004e-05], loss:2.3463494777679443\n",
      "Epoch: 5, batch index: 415, learning rate: [1.0000000000000004e-05], loss:2.358927011489868\n",
      "Epoch: 5, batch index: 416, learning rate: [1.0000000000000004e-05], loss:2.3054983615875244\n",
      "Epoch: 5, batch index: 417, learning rate: [1.0000000000000004e-05], loss:2.345288038253784\n",
      "Epoch: 5, batch index: 418, learning rate: [1.0000000000000004e-05], loss:2.3032302856445312\n",
      "Epoch: 5, batch index: 419, learning rate: [1.0000000000000004e-05], loss:2.2467806339263916\n",
      "Epoch: 5, batch index: 420, learning rate: [1.0000000000000004e-05], loss:2.2514026165008545\n",
      "Epoch: 5, batch index: 421, learning rate: [1.0000000000000004e-05], loss:2.2376456260681152\n",
      "Epoch: 5, batch index: 422, learning rate: [1.0000000000000004e-05], loss:2.359867572784424\n",
      "Epoch: 5, batch index: 423, learning rate: [1.0000000000000004e-05], loss:2.3118185997009277\n",
      "Epoch: 5, batch index: 424, learning rate: [1.0000000000000004e-05], loss:2.25104022026062\n",
      "Epoch: 5, batch index: 425, learning rate: [1.0000000000000004e-05], loss:2.275972366333008\n",
      "Epoch: 5, batch index: 426, learning rate: [1.0000000000000004e-05], loss:2.169280767440796\n",
      "Epoch: 5, batch index: 427, learning rate: [1.0000000000000004e-05], loss:2.30157732963562\n",
      "Epoch: 5, batch index: 428, learning rate: [1.0000000000000004e-05], loss:2.2806379795074463\n",
      "Epoch: 5, batch index: 429, learning rate: [1.0000000000000004e-05], loss:2.221620559692383\n",
      "Epoch: 5, batch index: 430, learning rate: [1.0000000000000004e-05], loss:2.2770092487335205\n",
      "Epoch: 5, batch index: 431, learning rate: [1.0000000000000004e-05], loss:2.345252275466919\n",
      "Epoch: 5, batch index: 432, learning rate: [1.0000000000000004e-05], loss:2.2968783378601074\n",
      "Epoch: 5, batch index: 433, learning rate: [1.0000000000000004e-05], loss:2.303680896759033\n",
      "Epoch: 5, batch index: 434, learning rate: [1.0000000000000004e-05], loss:2.2986068725585938\n",
      "Epoch: 5, batch index: 435, learning rate: [1.0000000000000004e-05], loss:2.3268213272094727\n",
      "Epoch: 5, batch index: 436, learning rate: [1.0000000000000004e-05], loss:2.2712807655334473\n",
      "Epoch: 5, batch index: 437, learning rate: [1.0000000000000004e-05], loss:2.2779030799865723\n",
      "Epoch: 5, batch index: 438, learning rate: [1.0000000000000004e-05], loss:2.2617595195770264\n",
      "Epoch: 5, batch index: 439, learning rate: [1.0000000000000004e-05], loss:2.2310476303100586\n",
      "Epoch: 5, batch index: 440, learning rate: [1.0000000000000004e-05], loss:2.2634737491607666\n",
      "Epoch: 5, batch index: 441, learning rate: [1.0000000000000004e-05], loss:2.2827725410461426\n",
      "Epoch: 5, batch index: 442, learning rate: [1.0000000000000004e-05], loss:2.3155956268310547\n",
      "Epoch: 5, batch index: 443, learning rate: [1.0000000000000004e-05], loss:2.3150742053985596\n",
      "Epoch: 5, batch index: 444, learning rate: [1.0000000000000004e-05], loss:2.2847747802734375\n",
      "Epoch: 5, batch index: 445, learning rate: [1.0000000000000004e-05], loss:2.23348069190979\n",
      "Epoch: 5, batch index: 446, learning rate: [1.0000000000000004e-05], loss:2.30780291557312\n",
      "Epoch: 5, batch index: 447, learning rate: [1.0000000000000004e-05], loss:2.3432443141937256\n",
      "Epoch: 5, batch index: 448, learning rate: [1.0000000000000004e-05], loss:2.252401351928711\n",
      "Epoch: 5, batch index: 449, learning rate: [1.0000000000000004e-05], loss:2.2572460174560547\n",
      "Epoch: 5, batch index: 450, learning rate: [1.0000000000000004e-05], loss:2.2635765075683594\n",
      "Epoch: 5, batch index: 451, learning rate: [1.0000000000000004e-05], loss:2.2392897605895996\n",
      "Epoch: 5, batch index: 452, learning rate: [1.0000000000000004e-05], loss:2.247739315032959\n",
      "Epoch: 5, batch index: 453, learning rate: [1.0000000000000004e-05], loss:2.2821667194366455\n",
      "Epoch: 5, batch index: 454, learning rate: [1.0000000000000004e-05], loss:2.2415709495544434\n",
      "Epoch: 5, batch index: 455, learning rate: [1.0000000000000004e-05], loss:2.3292174339294434\n",
      "Epoch: 5, batch index: 456, learning rate: [1.0000000000000004e-05], loss:2.280251979827881\n",
      "Epoch: 5, batch index: 457, learning rate: [1.0000000000000004e-05], loss:2.301363468170166\n",
      "Epoch: 5, batch index: 458, learning rate: [1.0000000000000004e-05], loss:2.273181915283203\n",
      "Epoch: 5, batch index: 459, learning rate: [1.0000000000000004e-05], loss:2.302737236022949\n",
      "Epoch: 5, batch index: 460, learning rate: [1.0000000000000004e-05], loss:2.307504892349243\n",
      "Epoch: 5, batch index: 461, learning rate: [1.0000000000000004e-05], loss:2.3134844303131104\n",
      "Epoch: 5, batch index: 462, learning rate: [1.0000000000000004e-06], loss:2.3003618717193604\n",
      "Epoch: 5, batch index: 463, learning rate: [1.0000000000000004e-06], loss:2.3151822090148926\n",
      "Epoch: 5, batch index: 464, learning rate: [1.0000000000000004e-06], loss:2.3575680255889893\n",
      "Epoch: 5, batch index: 465, learning rate: [1.0000000000000004e-06], loss:2.2666354179382324\n",
      "Epoch: 5, batch index: 466, learning rate: [1.0000000000000004e-06], loss:2.2643332481384277\n",
      "Epoch: 5, batch index: 467, learning rate: [1.0000000000000004e-06], loss:2.299140691757202\n",
      "Epoch: 5, batch index: 468, learning rate: [1.0000000000000004e-06], loss:2.3361666202545166\n",
      "Epoch: 5, batch index: 469, learning rate: [1.0000000000000004e-06], loss:2.2824740409851074\n",
      "Epoch: 5, batch index: 470, learning rate: [1.0000000000000004e-06], loss:2.2235875129699707\n",
      "Epoch: 5, batch index: 471, learning rate: [1.0000000000000004e-06], loss:2.3390395641326904\n",
      "Epoch: 5, batch index: 472, learning rate: [1.0000000000000004e-06], loss:2.2365546226501465\n",
      "Epoch: 5, batch index: 473, learning rate: [1.0000000000000004e-06], loss:2.423633575439453\n",
      "Calculating validation accuracy\n",
      "Epoch 5, validation accuracy score0.425390625\n",
      "Epoch: 6, batch index: 474, learning rate: [1.0000000000000004e-06], loss:2.3121914863586426\n",
      "Epoch: 6, batch index: 475, learning rate: [1.0000000000000004e-06], loss:2.3189868927001953\n",
      "Epoch: 6, batch index: 476, learning rate: [1.0000000000000004e-06], loss:2.2548511028289795\n",
      "Epoch: 6, batch index: 477, learning rate: [1.0000000000000004e-06], loss:2.2461273670196533\n",
      "Epoch: 6, batch index: 478, learning rate: [1.0000000000000004e-06], loss:2.2578988075256348\n",
      "Epoch: 6, batch index: 479, learning rate: [1.0000000000000004e-06], loss:2.325148820877075\n",
      "Epoch: 6, batch index: 480, learning rate: [1.0000000000000004e-06], loss:2.280684232711792\n",
      "Epoch: 6, batch index: 481, learning rate: [1.0000000000000004e-06], loss:2.283750534057617\n",
      "Epoch: 6, batch index: 482, learning rate: [1.0000000000000004e-06], loss:2.302396535873413\n",
      "Epoch: 6, batch index: 483, learning rate: [1.0000000000000004e-06], loss:2.3101232051849365\n",
      "Epoch: 6, batch index: 484, learning rate: [1.0000000000000004e-06], loss:2.228206157684326\n",
      "Epoch: 6, batch index: 485, learning rate: [1.0000000000000004e-06], loss:2.256134271621704\n",
      "Epoch: 6, batch index: 486, learning rate: [1.0000000000000004e-06], loss:2.2402889728546143\n",
      "Epoch: 6, batch index: 487, learning rate: [1.0000000000000004e-06], loss:2.351564645767212\n",
      "Epoch: 6, batch index: 488, learning rate: [1.0000000000000004e-06], loss:2.2515056133270264\n",
      "Epoch: 6, batch index: 489, learning rate: [1.0000000000000004e-06], loss:2.2869815826416016\n",
      "Epoch: 6, batch index: 490, learning rate: [1.0000000000000004e-06], loss:2.3014657497406006\n",
      "Epoch: 6, batch index: 491, learning rate: [1.0000000000000004e-06], loss:2.275169610977173\n",
      "Epoch: 6, batch index: 492, learning rate: [1.0000000000000004e-06], loss:2.3012897968292236\n",
      "Epoch: 6, batch index: 493, learning rate: [1.0000000000000004e-06], loss:2.3034772872924805\n",
      "Epoch: 6, batch index: 494, learning rate: [1.0000000000000004e-06], loss:2.386173963546753\n",
      "Epoch: 6, batch index: 495, learning rate: [1.0000000000000004e-06], loss:2.296787738800049\n",
      "Epoch: 6, batch index: 496, learning rate: [1.0000000000000004e-06], loss:2.2487237453460693\n",
      "Epoch: 6, batch index: 497, learning rate: [1.0000000000000004e-06], loss:2.1655867099761963\n",
      "Epoch: 6, batch index: 498, learning rate: [1.0000000000000004e-06], loss:2.2987349033355713\n",
      "Epoch: 6, batch index: 499, learning rate: [1.0000000000000004e-06], loss:2.314779281616211\n",
      "Epoch: 6, batch index: 500, learning rate: [1.0000000000000004e-06], loss:2.31005859375\n",
      "Epoch: 6, batch index: 501, learning rate: [1.0000000000000004e-06], loss:2.2218899726867676\n",
      "Epoch: 6, batch index: 502, learning rate: [1.0000000000000004e-06], loss:2.338006019592285\n",
      "Epoch: 6, batch index: 503, learning rate: [1.0000000000000004e-06], loss:2.2682907581329346\n",
      "Epoch: 6, batch index: 504, learning rate: [1.0000000000000004e-06], loss:2.2476563453674316\n",
      "Epoch: 6, batch index: 505, learning rate: [1.0000000000000004e-06], loss:2.3268325328826904\n",
      "Epoch: 6, batch index: 506, learning rate: [1.0000000000000004e-06], loss:2.265540838241577\n",
      "Epoch: 6, batch index: 507, learning rate: [1.0000000000000004e-06], loss:2.3649325370788574\n",
      "Epoch: 6, batch index: 508, learning rate: [1.0000000000000004e-06], loss:2.350459098815918\n",
      "Epoch: 6, batch index: 509, learning rate: [1.0000000000000004e-06], loss:2.29549503326416\n",
      "Epoch: 6, batch index: 510, learning rate: [1.0000000000000004e-06], loss:2.258877992630005\n",
      "Epoch: 6, batch index: 511, learning rate: [1.0000000000000004e-06], loss:2.2261781692504883\n",
      "Epoch: 6, batch index: 512, learning rate: [1.0000000000000004e-06], loss:2.372748374938965\n",
      "Epoch: 6, batch index: 513, learning rate: [1.0000000000000004e-06], loss:2.294569492340088\n",
      "Epoch: 6, batch index: 514, learning rate: [1.0000000000000004e-06], loss:2.290410041809082\n",
      "Epoch: 6, batch index: 515, learning rate: [1.0000000000000004e-06], loss:2.213399887084961\n",
      "Epoch: 6, batch index: 516, learning rate: [1.0000000000000004e-06], loss:2.2525131702423096\n",
      "Epoch: 6, batch index: 517, learning rate: [1.0000000000000004e-06], loss:2.310567617416382\n",
      "Epoch: 6, batch index: 518, learning rate: [1.0000000000000004e-06], loss:2.25471830368042\n",
      "Epoch: 6, batch index: 519, learning rate: [1.0000000000000004e-06], loss:2.318629741668701\n",
      "Epoch: 6, batch index: 520, learning rate: [1.0000000000000005e-07], loss:2.3201773166656494\n",
      "Epoch: 6, batch index: 521, learning rate: [1.0000000000000005e-07], loss:2.260279655456543\n",
      "Epoch: 6, batch index: 522, learning rate: [1.0000000000000005e-07], loss:2.2312424182891846\n",
      "Epoch: 6, batch index: 523, learning rate: [1.0000000000000005e-07], loss:2.2808806896209717\n",
      "Epoch: 6, batch index: 524, learning rate: [1.0000000000000005e-07], loss:2.3089797496795654\n",
      "Epoch: 6, batch index: 525, learning rate: [1.0000000000000005e-07], loss:2.2258434295654297\n",
      "Epoch: 6, batch index: 526, learning rate: [1.0000000000000005e-07], loss:2.2598156929016113\n",
      "Epoch: 6, batch index: 527, learning rate: [1.0000000000000005e-07], loss:2.268514633178711\n",
      "Epoch: 6, batch index: 528, learning rate: [1.0000000000000005e-07], loss:2.309586763381958\n",
      "Epoch: 6, batch index: 529, learning rate: [1.0000000000000005e-07], loss:2.2558164596557617\n",
      "Epoch: 6, batch index: 530, learning rate: [1.0000000000000005e-07], loss:2.274087429046631\n",
      "Epoch: 6, batch index: 531, learning rate: [1.0000000000000005e-07], loss:2.2714531421661377\n",
      "Epoch: 6, batch index: 532, learning rate: [1.0000000000000005e-07], loss:2.254037857055664\n",
      "Epoch: 6, batch index: 533, learning rate: [1.0000000000000005e-07], loss:2.313159227371216\n",
      "Epoch: 6, batch index: 534, learning rate: [1.0000000000000005e-07], loss:2.286621570587158\n",
      "Epoch: 6, batch index: 535, learning rate: [1.0000000000000005e-07], loss:2.2902016639709473\n",
      "Epoch: 6, batch index: 536, learning rate: [1.0000000000000005e-07], loss:2.222730875015259\n",
      "Epoch: 6, batch index: 537, learning rate: [1.0000000000000005e-07], loss:2.3127846717834473\n",
      "Epoch: 6, batch index: 538, learning rate: [1.0000000000000005e-07], loss:2.2997844219207764\n",
      "Epoch: 6, batch index: 539, learning rate: [1.0000000000000005e-07], loss:2.2371256351470947\n",
      "Epoch: 6, batch index: 540, learning rate: [1.0000000000000005e-07], loss:2.2329390048980713\n",
      "Epoch: 6, batch index: 541, learning rate: [1.0000000000000005e-07], loss:2.334487199783325\n",
      "Epoch: 6, batch index: 542, learning rate: [1.0000000000000005e-07], loss:2.213685989379883\n",
      "Epoch: 6, batch index: 543, learning rate: [1.0000000000000005e-07], loss:2.3157331943511963\n",
      "Epoch: 6, batch index: 544, learning rate: [1.0000000000000005e-07], loss:2.3235526084899902\n",
      "Epoch: 6, batch index: 545, learning rate: [1.0000000000000005e-07], loss:2.2723116874694824\n",
      "Epoch: 6, batch index: 546, learning rate: [1.0000000000000005e-07], loss:2.3198418617248535\n",
      "Epoch: 6, batch index: 547, learning rate: [1.0000000000000005e-07], loss:2.241312265396118\n",
      "Epoch: 6, batch index: 548, learning rate: [1.0000000000000005e-07], loss:2.2423410415649414\n",
      "Epoch: 6, batch index: 549, learning rate: [1.0000000000000005e-07], loss:2.3028132915496826\n",
      "Epoch: 6, batch index: 550, learning rate: [1.0000000000000005e-07], loss:2.308199644088745\n",
      "Epoch: 6, batch index: 551, learning rate: [1.0000000000000005e-07], loss:2.294506549835205\n",
      "Epoch: 6, batch index: 552, learning rate: [1.0000000000000005e-07], loss:2.1825156211853027\n",
      "Calculating validation accuracy\n",
      "Epoch 6, validation accuracy score0.42421875\n",
      "Epoch: 7, batch index: 553, learning rate: [1.0000000000000005e-07], loss:2.359119176864624\n",
      "Epoch: 7, batch index: 554, learning rate: [1.0000000000000005e-07], loss:2.3074135780334473\n",
      "Epoch: 7, batch index: 555, learning rate: [1.0000000000000005e-07], loss:2.303987741470337\n",
      "Epoch: 7, batch index: 556, learning rate: [1.0000000000000005e-07], loss:2.286085367202759\n",
      "Epoch: 7, batch index: 557, learning rate: [1.0000000000000005e-07], loss:2.2704243659973145\n",
      "Epoch: 7, batch index: 558, learning rate: [1.0000000000000005e-07], loss:2.29809832572937\n",
      "Epoch: 7, batch index: 559, learning rate: [1.0000000000000005e-07], loss:2.2173678874969482\n",
      "Epoch: 7, batch index: 560, learning rate: [1.0000000000000005e-07], loss:2.3077728748321533\n",
      "Epoch: 7, batch index: 561, learning rate: [1.0000000000000005e-07], loss:2.324288845062256\n",
      "Epoch: 7, batch index: 562, learning rate: [1.0000000000000005e-07], loss:2.2959039211273193\n",
      "Epoch: 7, batch index: 563, learning rate: [1.0000000000000005e-07], loss:2.247142791748047\n",
      "Epoch: 7, batch index: 564, learning rate: [1.0000000000000005e-07], loss:2.282580852508545\n",
      "Epoch: 7, batch index: 565, learning rate: [1.0000000000000005e-07], loss:2.266536235809326\n",
      "Epoch: 7, batch index: 566, learning rate: [1.0000000000000005e-07], loss:2.3584845066070557\n",
      "Epoch: 7, batch index: 567, learning rate: [1.0000000000000005e-07], loss:2.223916530609131\n",
      "Epoch: 7, batch index: 568, learning rate: [1.0000000000000005e-07], loss:2.2522969245910645\n",
      "Epoch: 7, batch index: 569, learning rate: [1.0000000000000005e-07], loss:2.331169843673706\n",
      "Epoch: 7, batch index: 570, learning rate: [1.0000000000000005e-07], loss:2.2138490676879883\n",
      "Epoch: 7, batch index: 571, learning rate: [1.0000000000000005e-07], loss:2.1867666244506836\n",
      "Epoch: 7, batch index: 572, learning rate: [1.0000000000000005e-07], loss:2.2175557613372803\n",
      "Epoch: 7, batch index: 573, learning rate: [1.0000000000000005e-07], loss:2.3205814361572266\n",
      "Epoch: 7, batch index: 574, learning rate: [1.0000000000000005e-07], loss:2.3067190647125244\n",
      "Epoch: 7, batch index: 575, learning rate: [1.0000000000000005e-07], loss:2.315481662750244\n",
      "Epoch: 7, batch index: 576, learning rate: [1.0000000000000005e-07], loss:2.2846760749816895\n",
      "Epoch: 7, batch index: 577, learning rate: [1.0000000000000005e-07], loss:2.2844078540802\n",
      "Epoch: 7, batch index: 578, learning rate: [1.0000000000000005e-08], loss:2.2687177658081055\n",
      "Epoch: 7, batch index: 579, learning rate: [1.0000000000000005e-08], loss:2.318855047225952\n",
      "Epoch: 7, batch index: 580, learning rate: [1.0000000000000005e-08], loss:2.225641965866089\n",
      "Epoch: 7, batch index: 581, learning rate: [1.0000000000000005e-08], loss:2.3028697967529297\n",
      "Epoch: 7, batch index: 582, learning rate: [1.0000000000000005e-08], loss:2.236560583114624\n",
      "Epoch: 7, batch index: 583, learning rate: [1.0000000000000005e-08], loss:2.2064242362976074\n",
      "Epoch: 7, batch index: 584, learning rate: [1.0000000000000005e-08], loss:2.2980833053588867\n",
      "Epoch: 7, batch index: 585, learning rate: [1.0000000000000005e-08], loss:2.2799363136291504\n",
      "Epoch: 7, batch index: 586, learning rate: [1.0000000000000005e-08], loss:2.2264022827148438\n",
      "Epoch: 7, batch index: 587, learning rate: [1.0000000000000005e-08], loss:2.2886340618133545\n",
      "Epoch: 7, batch index: 588, learning rate: [1.0000000000000005e-08], loss:2.2032577991485596\n",
      "Epoch: 7, batch index: 589, learning rate: [1.0000000000000005e-08], loss:2.2489845752716064\n",
      "Epoch: 7, batch index: 590, learning rate: [1.0000000000000005e-08], loss:2.346879005432129\n",
      "Epoch: 7, batch index: 591, learning rate: [1.0000000000000005e-08], loss:2.3113083839416504\n",
      "Epoch: 7, batch index: 592, learning rate: [1.0000000000000005e-08], loss:2.2426252365112305\n",
      "Epoch: 7, batch index: 593, learning rate: [1.0000000000000005e-08], loss:2.220539093017578\n",
      "Epoch: 7, batch index: 594, learning rate: [1.0000000000000005e-08], loss:2.3213255405426025\n",
      "Epoch: 7, batch index: 595, learning rate: [1.0000000000000005e-08], loss:2.2920541763305664\n",
      "Epoch: 7, batch index: 596, learning rate: [1.0000000000000005e-08], loss:2.268130302429199\n",
      "Epoch: 7, batch index: 597, learning rate: [1.0000000000000005e-08], loss:2.305140733718872\n",
      "Epoch: 7, batch index: 598, learning rate: [1.0000000000000005e-08], loss:2.3459651470184326\n",
      "Epoch: 7, batch index: 599, learning rate: [1.0000000000000005e-08], loss:2.2506301403045654\n",
      "Epoch: 7, batch index: 600, learning rate: [1.0000000000000005e-08], loss:2.170182704925537\n",
      "Epoch: 7, batch index: 601, learning rate: [1.0000000000000005e-08], loss:2.331159830093384\n",
      "Epoch: 7, batch index: 602, learning rate: [1.0000000000000005e-08], loss:2.2871479988098145\n",
      "Epoch: 7, batch index: 603, learning rate: [1.0000000000000005e-08], loss:2.3494863510131836\n",
      "Epoch: 7, batch index: 604, learning rate: [1.0000000000000005e-08], loss:2.245739221572876\n",
      "Epoch: 7, batch index: 605, learning rate: [1.0000000000000005e-08], loss:2.2864184379577637\n",
      "Epoch: 7, batch index: 606, learning rate: [1.0000000000000005e-08], loss:2.306326150894165\n",
      "Epoch: 7, batch index: 607, learning rate: [1.0000000000000005e-08], loss:2.281123161315918\n",
      "Epoch: 7, batch index: 608, learning rate: [1.0000000000000005e-08], loss:2.284227132797241\n",
      "Epoch: 7, batch index: 609, learning rate: [1.0000000000000005e-08], loss:2.296234607696533\n",
      "Epoch: 7, batch index: 610, learning rate: [1.0000000000000005e-08], loss:2.275250196456909\n",
      "Epoch: 7, batch index: 611, learning rate: [1.0000000000000005e-08], loss:2.244408369064331\n",
      "Epoch: 7, batch index: 612, learning rate: [1.0000000000000005e-08], loss:2.295952796936035\n",
      "Epoch: 7, batch index: 613, learning rate: [1.0000000000000005e-08], loss:2.3078811168670654\n",
      "Epoch: 7, batch index: 614, learning rate: [1.0000000000000005e-08], loss:2.242016553878784\n",
      "Epoch: 7, batch index: 615, learning rate: [1.0000000000000005e-08], loss:2.301863431930542\n",
      "Epoch: 7, batch index: 616, learning rate: [1.0000000000000005e-08], loss:2.3226373195648193\n",
      "Epoch: 7, batch index: 617, learning rate: [1.0000000000000005e-08], loss:2.3668458461761475\n",
      "Epoch: 7, batch index: 618, learning rate: [1.0000000000000005e-08], loss:2.311396837234497\n",
      "Epoch: 7, batch index: 619, learning rate: [1.0000000000000005e-08], loss:2.282369613647461\n",
      "Epoch: 7, batch index: 620, learning rate: [1.0000000000000005e-08], loss:2.329462766647339\n",
      "Epoch: 7, batch index: 621, learning rate: [1.0000000000000005e-08], loss:2.3543334007263184\n",
      "Epoch: 7, batch index: 622, learning rate: [1.0000000000000005e-08], loss:2.2657079696655273\n",
      "Epoch: 7, batch index: 623, learning rate: [1.0000000000000005e-08], loss:2.2662971019744873\n",
      "Epoch: 7, batch index: 624, learning rate: [1.0000000000000005e-08], loss:2.301072835922241\n",
      "Epoch: 7, batch index: 625, learning rate: [1.0000000000000005e-08], loss:2.3483195304870605\n",
      "Epoch: 7, batch index: 626, learning rate: [1.0000000000000005e-08], loss:2.310171127319336\n",
      "Epoch: 7, batch index: 627, learning rate: [1.0000000000000005e-08], loss:2.285679578781128\n",
      "Epoch: 7, batch index: 628, learning rate: [1.0000000000000005e-08], loss:2.293674945831299\n",
      "Epoch: 7, batch index: 629, learning rate: [1.0000000000000005e-08], loss:2.2276611328125\n",
      "Epoch: 7, batch index: 630, learning rate: [1.0000000000000005e-08], loss:2.3766586780548096\n",
      "Epoch: 7, batch index: 631, learning rate: [1.0000000000000005e-08], loss:2.2134499549865723\n",
      "Calculating validation accuracy\n",
      "Epoch 7, validation accuracy score0.425390625\n",
      "Epoch: 8, batch index: 632, learning rate: [1.0000000000000005e-08], loss:2.2586634159088135\n",
      "Epoch: 8, batch index: 633, learning rate: [1.0000000000000005e-08], loss:2.192344903945923\n",
      "Epoch: 8, batch index: 634, learning rate: [1.0000000000000005e-08], loss:2.2693145275115967\n",
      "Epoch: 8, batch index: 635, learning rate: [1.0000000000000005e-08], loss:2.275345802307129\n",
      "Epoch: 8, batch index: 636, learning rate: [1.0000000000000005e-09], loss:2.233548164367676\n",
      "Epoch: 8, batch index: 637, learning rate: [1.0000000000000005e-09], loss:2.256258487701416\n",
      "Epoch: 8, batch index: 638, learning rate: [1.0000000000000005e-09], loss:2.243940830230713\n",
      "Epoch: 8, batch index: 639, learning rate: [1.0000000000000005e-09], loss:2.2003746032714844\n",
      "Epoch: 8, batch index: 640, learning rate: [1.0000000000000005e-09], loss:2.3232641220092773\n",
      "Epoch: 8, batch index: 641, learning rate: [1.0000000000000005e-09], loss:2.289003372192383\n",
      "Epoch: 8, batch index: 642, learning rate: [1.0000000000000005e-09], loss:2.2858245372772217\n",
      "Epoch: 8, batch index: 643, learning rate: [1.0000000000000005e-09], loss:2.262904644012451\n",
      "Epoch: 8, batch index: 644, learning rate: [1.0000000000000005e-09], loss:2.2646498680114746\n",
      "Epoch: 8, batch index: 645, learning rate: [1.0000000000000005e-09], loss:2.339773654937744\n",
      "Epoch: 8, batch index: 646, learning rate: [1.0000000000000005e-09], loss:2.301656484603882\n",
      "Epoch: 8, batch index: 647, learning rate: [1.0000000000000005e-09], loss:2.330380916595459\n",
      "Epoch: 8, batch index: 648, learning rate: [1.0000000000000005e-09], loss:2.295825958251953\n",
      "Epoch: 8, batch index: 649, learning rate: [1.0000000000000005e-09], loss:2.248124837875366\n",
      "Epoch: 8, batch index: 650, learning rate: [1.0000000000000005e-09], loss:2.321317434310913\n",
      "Epoch: 8, batch index: 651, learning rate: [1.0000000000000005e-09], loss:2.1869537830352783\n",
      "Epoch: 8, batch index: 652, learning rate: [1.0000000000000005e-09], loss:2.283115863800049\n",
      "Epoch: 8, batch index: 653, learning rate: [1.0000000000000005e-09], loss:2.256615161895752\n",
      "Epoch: 8, batch index: 654, learning rate: [1.0000000000000005e-09], loss:2.3123865127563477\n",
      "Epoch: 8, batch index: 655, learning rate: [1.0000000000000005e-09], loss:2.2531635761260986\n",
      "Epoch: 8, batch index: 656, learning rate: [1.0000000000000005e-09], loss:2.2619731426239014\n",
      "Epoch: 8, batch index: 657, learning rate: [1.0000000000000005e-09], loss:2.1421563625335693\n",
      "Epoch: 8, batch index: 658, learning rate: [1.0000000000000005e-09], loss:2.331085443496704\n",
      "Epoch: 8, batch index: 659, learning rate: [1.0000000000000005e-09], loss:2.321434259414673\n",
      "Epoch: 8, batch index: 660, learning rate: [1.0000000000000005e-09], loss:2.1923108100891113\n",
      "Epoch: 8, batch index: 661, learning rate: [1.0000000000000005e-09], loss:2.314004421234131\n",
      "Epoch: 8, batch index: 662, learning rate: [1.0000000000000005e-09], loss:2.2711918354034424\n",
      "Epoch: 8, batch index: 663, learning rate: [1.0000000000000005e-09], loss:2.287611246109009\n",
      "Epoch: 8, batch index: 664, learning rate: [1.0000000000000005e-09], loss:2.206380844116211\n",
      "Epoch: 8, batch index: 665, learning rate: [1.0000000000000005e-09], loss:2.3167202472686768\n",
      "Epoch: 8, batch index: 666, learning rate: [1.0000000000000005e-09], loss:2.3109524250030518\n",
      "Epoch: 8, batch index: 667, learning rate: [1.0000000000000005e-09], loss:2.310299873352051\n",
      "Epoch: 8, batch index: 668, learning rate: [1.0000000000000005e-09], loss:2.34525465965271\n",
      "Epoch: 8, batch index: 669, learning rate: [1.0000000000000005e-09], loss:2.2384274005889893\n",
      "Epoch: 8, batch index: 670, learning rate: [1.0000000000000005e-09], loss:2.329017400741577\n",
      "Epoch: 8, batch index: 671, learning rate: [1.0000000000000005e-09], loss:2.259916067123413\n",
      "Epoch: 8, batch index: 672, learning rate: [1.0000000000000005e-09], loss:2.320652723312378\n",
      "Epoch: 8, batch index: 673, learning rate: [1.0000000000000005e-09], loss:2.2771997451782227\n",
      "Epoch: 8, batch index: 674, learning rate: [1.0000000000000005e-09], loss:2.246288299560547\n",
      "Epoch: 8, batch index: 675, learning rate: [1.0000000000000005e-09], loss:2.25040340423584\n",
      "Epoch: 8, batch index: 676, learning rate: [1.0000000000000005e-09], loss:2.294613838195801\n",
      "Epoch: 8, batch index: 677, learning rate: [1.0000000000000005e-09], loss:2.2659568786621094\n",
      "Epoch: 8, batch index: 678, learning rate: [1.0000000000000005e-09], loss:2.2584965229034424\n",
      "Epoch: 8, batch index: 679, learning rate: [1.0000000000000005e-09], loss:2.3514325618743896\n",
      "Epoch: 8, batch index: 680, learning rate: [1.0000000000000005e-09], loss:2.2855358123779297\n",
      "Epoch: 8, batch index: 681, learning rate: [1.0000000000000005e-09], loss:2.3061575889587402\n",
      "Epoch: 8, batch index: 682, learning rate: [1.0000000000000005e-09], loss:2.2959585189819336\n",
      "Epoch: 8, batch index: 683, learning rate: [1.0000000000000005e-09], loss:2.311678409576416\n",
      "Epoch: 8, batch index: 684, learning rate: [1.0000000000000005e-09], loss:2.3262722492218018\n",
      "Epoch: 8, batch index: 685, learning rate: [1.0000000000000005e-09], loss:2.2782540321350098\n",
      "Epoch: 8, batch index: 686, learning rate: [1.0000000000000005e-09], loss:2.353877305984497\n",
      "Epoch: 8, batch index: 687, learning rate: [1.0000000000000005e-09], loss:2.3122828006744385\n",
      "Epoch: 8, batch index: 688, learning rate: [1.0000000000000005e-09], loss:2.3431878089904785\n",
      "Epoch: 8, batch index: 689, learning rate: [1.0000000000000005e-09], loss:2.2834255695343018\n",
      "Epoch: 8, batch index: 690, learning rate: [1.0000000000000005e-09], loss:2.3125569820404053\n",
      "Epoch: 8, batch index: 691, learning rate: [1.0000000000000005e-09], loss:2.2574784755706787\n",
      "Epoch: 8, batch index: 692, learning rate: [1.0000000000000005e-09], loss:2.2886741161346436\n",
      "Epoch: 8, batch index: 693, learning rate: [1.0000000000000005e-09], loss:2.2571861743927\n",
      "Epoch: 8, batch index: 694, learning rate: [1.0000000000000006e-10], loss:2.316619873046875\n",
      "Epoch: 8, batch index: 695, learning rate: [1.0000000000000006e-10], loss:2.2400131225585938\n",
      "Epoch: 8, batch index: 696, learning rate: [1.0000000000000006e-10], loss:2.2999441623687744\n",
      "Epoch: 8, batch index: 697, learning rate: [1.0000000000000006e-10], loss:2.347871780395508\n",
      "Epoch: 8, batch index: 698, learning rate: [1.0000000000000006e-10], loss:2.277028799057007\n",
      "Epoch: 8, batch index: 699, learning rate: [1.0000000000000006e-10], loss:2.2962324619293213\n",
      "Epoch: 8, batch index: 700, learning rate: [1.0000000000000006e-10], loss:2.3152618408203125\n",
      "Epoch: 8, batch index: 701, learning rate: [1.0000000000000006e-10], loss:2.337524175643921\n",
      "Epoch: 8, batch index: 702, learning rate: [1.0000000000000006e-10], loss:2.245129346847534\n",
      "Epoch: 8, batch index: 703, learning rate: [1.0000000000000006e-10], loss:2.3406503200531006\n",
      "Epoch: 8, batch index: 704, learning rate: [1.0000000000000006e-10], loss:2.2335965633392334\n",
      "Epoch: 8, batch index: 705, learning rate: [1.0000000000000006e-10], loss:2.290562391281128\n",
      "Epoch: 8, batch index: 706, learning rate: [1.0000000000000006e-10], loss:2.2983591556549072\n",
      "Epoch: 8, batch index: 707, learning rate: [1.0000000000000006e-10], loss:2.2748453617095947\n",
      "Epoch: 8, batch index: 708, learning rate: [1.0000000000000006e-10], loss:2.199943780899048\n",
      "Epoch: 8, batch index: 709, learning rate: [1.0000000000000006e-10], loss:2.346782922744751\n",
      "Epoch: 8, batch index: 710, learning rate: [1.0000000000000006e-10], loss:2.3744447231292725\n",
      "Calculating validation accuracy\n",
      "Epoch 8, validation accuracy score0.421484375\n",
      "Epoch: 9, batch index: 711, learning rate: [1.0000000000000006e-10], loss:2.3013877868652344\n",
      "Epoch: 9, batch index: 712, learning rate: [1.0000000000000006e-10], loss:2.318666934967041\n",
      "Epoch: 9, batch index: 713, learning rate: [1.0000000000000006e-10], loss:2.2051308155059814\n",
      "Epoch: 9, batch index: 714, learning rate: [1.0000000000000006e-10], loss:2.2780654430389404\n",
      "Epoch: 9, batch index: 715, learning rate: [1.0000000000000006e-10], loss:2.3273963928222656\n",
      "Epoch: 9, batch index: 716, learning rate: [1.0000000000000006e-10], loss:2.38187313079834\n",
      "Epoch: 9, batch index: 717, learning rate: [1.0000000000000006e-10], loss:2.259219169616699\n",
      "Epoch: 9, batch index: 718, learning rate: [1.0000000000000006e-10], loss:2.2918319702148438\n",
      "Epoch: 9, batch index: 719, learning rate: [1.0000000000000006e-10], loss:2.2949581146240234\n",
      "Epoch: 9, batch index: 720, learning rate: [1.0000000000000006e-10], loss:2.3241777420043945\n",
      "Epoch: 9, batch index: 721, learning rate: [1.0000000000000006e-10], loss:2.2345614433288574\n",
      "Epoch: 9, batch index: 722, learning rate: [1.0000000000000006e-10], loss:2.3176004886627197\n",
      "Epoch: 9, batch index: 723, learning rate: [1.0000000000000006e-10], loss:2.2279369831085205\n",
      "Epoch: 9, batch index: 724, learning rate: [1.0000000000000006e-10], loss:2.3256378173828125\n",
      "Epoch: 9, batch index: 725, learning rate: [1.0000000000000006e-10], loss:2.251558303833008\n",
      "Epoch: 9, batch index: 726, learning rate: [1.0000000000000006e-10], loss:2.252746105194092\n",
      "Epoch: 9, batch index: 727, learning rate: [1.0000000000000006e-10], loss:2.304044485092163\n",
      "Epoch: 9, batch index: 728, learning rate: [1.0000000000000006e-10], loss:2.2963201999664307\n",
      "Epoch: 9, batch index: 729, learning rate: [1.0000000000000006e-10], loss:2.2621519565582275\n",
      "Epoch: 9, batch index: 730, learning rate: [1.0000000000000006e-10], loss:2.2385826110839844\n",
      "Epoch: 9, batch index: 731, learning rate: [1.0000000000000006e-10], loss:2.3060989379882812\n",
      "Epoch: 9, batch index: 732, learning rate: [1.0000000000000006e-10], loss:2.245290756225586\n",
      "Epoch: 9, batch index: 733, learning rate: [1.0000000000000006e-10], loss:2.2767984867095947\n",
      "Epoch: 9, batch index: 734, learning rate: [1.0000000000000006e-10], loss:2.3638479709625244\n",
      "Epoch: 9, batch index: 735, learning rate: [1.0000000000000006e-10], loss:2.3259329795837402\n",
      "Epoch: 9, batch index: 736, learning rate: [1.0000000000000006e-10], loss:2.194058418273926\n",
      "Epoch: 9, batch index: 737, learning rate: [1.0000000000000006e-10], loss:2.252363681793213\n",
      "Epoch: 9, batch index: 738, learning rate: [1.0000000000000006e-10], loss:2.273521661758423\n",
      "Epoch: 9, batch index: 739, learning rate: [1.0000000000000006e-10], loss:2.303230047225952\n",
      "Epoch: 9, batch index: 740, learning rate: [1.0000000000000006e-10], loss:2.2923784255981445\n",
      "Epoch: 9, batch index: 741, learning rate: [1.0000000000000006e-10], loss:2.260896682739258\n",
      "Epoch: 9, batch index: 742, learning rate: [1.0000000000000006e-10], loss:2.1938514709472656\n",
      "Epoch: 9, batch index: 743, learning rate: [1.0000000000000006e-10], loss:2.2619822025299072\n",
      "Epoch: 9, batch index: 744, learning rate: [1.0000000000000006e-10], loss:2.3864989280700684\n",
      "Epoch: 9, batch index: 745, learning rate: [1.0000000000000006e-10], loss:2.312046766281128\n",
      "Epoch: 9, batch index: 746, learning rate: [1.0000000000000006e-10], loss:2.2939043045043945\n",
      "Epoch: 9, batch index: 747, learning rate: [1.0000000000000006e-10], loss:2.3325746059417725\n",
      "Epoch: 9, batch index: 748, learning rate: [1.0000000000000006e-10], loss:2.3199374675750732\n",
      "Epoch: 9, batch index: 749, learning rate: [1.0000000000000006e-10], loss:2.26206636428833\n",
      "Epoch: 9, batch index: 750, learning rate: [1.0000000000000006e-10], loss:2.313793182373047\n",
      "Epoch: 9, batch index: 751, learning rate: [1.0000000000000006e-10], loss:2.3347396850585938\n",
      "Epoch: 9, batch index: 752, learning rate: [1.0000000000000006e-11], loss:2.294344902038574\n",
      "Epoch: 9, batch index: 753, learning rate: [1.0000000000000006e-11], loss:2.320244312286377\n",
      "Epoch: 9, batch index: 754, learning rate: [1.0000000000000006e-11], loss:2.3219101428985596\n",
      "Epoch: 9, batch index: 755, learning rate: [1.0000000000000006e-11], loss:2.2959444522857666\n",
      "Epoch: 9, batch index: 756, learning rate: [1.0000000000000006e-11], loss:2.291487693786621\n",
      "Epoch: 9, batch index: 757, learning rate: [1.0000000000000006e-11], loss:2.262025833129883\n",
      "Epoch: 9, batch index: 758, learning rate: [1.0000000000000006e-11], loss:2.2699503898620605\n",
      "Epoch: 9, batch index: 759, learning rate: [1.0000000000000006e-11], loss:2.2814009189605713\n",
      "Epoch: 9, batch index: 760, learning rate: [1.0000000000000006e-11], loss:2.2935335636138916\n",
      "Epoch: 9, batch index: 761, learning rate: [1.0000000000000006e-11], loss:2.2976231575012207\n",
      "Epoch: 9, batch index: 762, learning rate: [1.0000000000000006e-11], loss:2.2856619358062744\n",
      "Epoch: 9, batch index: 763, learning rate: [1.0000000000000006e-11], loss:2.312769889831543\n",
      "Epoch: 9, batch index: 764, learning rate: [1.0000000000000006e-11], loss:2.2835211753845215\n",
      "Epoch: 9, batch index: 765, learning rate: [1.0000000000000006e-11], loss:2.2578396797180176\n",
      "Epoch: 9, batch index: 766, learning rate: [1.0000000000000006e-11], loss:2.300478935241699\n",
      "Epoch: 9, batch index: 767, learning rate: [1.0000000000000006e-11], loss:2.230555295944214\n",
      "Epoch: 9, batch index: 768, learning rate: [1.0000000000000006e-11], loss:2.258042335510254\n",
      "Epoch: 9, batch index: 769, learning rate: [1.0000000000000006e-11], loss:2.200415849685669\n",
      "Epoch: 9, batch index: 770, learning rate: [1.0000000000000006e-11], loss:2.279773712158203\n",
      "Epoch: 9, batch index: 771, learning rate: [1.0000000000000006e-11], loss:2.288062572479248\n",
      "Epoch: 9, batch index: 772, learning rate: [1.0000000000000006e-11], loss:2.3235597610473633\n",
      "Epoch: 9, batch index: 773, learning rate: [1.0000000000000006e-11], loss:2.3221588134765625\n",
      "Epoch: 9, batch index: 774, learning rate: [1.0000000000000006e-11], loss:2.2361011505126953\n",
      "Epoch: 9, batch index: 775, learning rate: [1.0000000000000006e-11], loss:2.290926694869995\n",
      "Epoch: 9, batch index: 776, learning rate: [1.0000000000000006e-11], loss:2.236995220184326\n",
      "Epoch: 9, batch index: 777, learning rate: [1.0000000000000006e-11], loss:2.279492139816284\n",
      "Epoch: 9, batch index: 778, learning rate: [1.0000000000000006e-11], loss:2.265591621398926\n",
      "Epoch: 9, batch index: 779, learning rate: [1.0000000000000006e-11], loss:2.2848353385925293\n",
      "Epoch: 9, batch index: 780, learning rate: [1.0000000000000006e-11], loss:2.2239925861358643\n",
      "Epoch: 9, batch index: 781, learning rate: [1.0000000000000006e-11], loss:2.2861328125\n",
      "Epoch: 9, batch index: 782, learning rate: [1.0000000000000006e-11], loss:2.32334303855896\n",
      "Epoch: 9, batch index: 783, learning rate: [1.0000000000000006e-11], loss:2.2547659873962402\n",
      "Epoch: 9, batch index: 784, learning rate: [1.0000000000000006e-11], loss:2.233992338180542\n",
      "Epoch: 9, batch index: 785, learning rate: [1.0000000000000006e-11], loss:2.351200580596924\n",
      "Epoch: 9, batch index: 786, learning rate: [1.0000000000000006e-11], loss:2.3103976249694824\n",
      "Epoch: 9, batch index: 787, learning rate: [1.0000000000000006e-11], loss:2.2625603675842285\n",
      "Epoch: 9, batch index: 788, learning rate: [1.0000000000000006e-11], loss:2.3372182846069336\n",
      "Epoch: 9, batch index: 789, learning rate: [1.0000000000000006e-11], loss:2.4756171703338623\n",
      "Calculating validation accuracy\n",
      "Epoch 9, validation accuracy score0.425\n",
      "Epoch: 10, batch index: 790, learning rate: [1.0000000000000006e-11], loss:2.292773723602295\n",
      "Epoch: 10, batch index: 791, learning rate: [1.0000000000000006e-11], loss:2.227557897567749\n",
      "Epoch: 10, batch index: 792, learning rate: [1.0000000000000006e-11], loss:2.2699313163757324\n",
      "Epoch: 10, batch index: 793, learning rate: [1.0000000000000006e-11], loss:2.3050498962402344\n",
      "Epoch: 10, batch index: 794, learning rate: [1.0000000000000006e-11], loss:2.288431167602539\n",
      "Epoch: 10, batch index: 795, learning rate: [1.0000000000000006e-11], loss:2.353541374206543\n",
      "Epoch: 10, batch index: 796, learning rate: [1.0000000000000006e-11], loss:2.309678316116333\n",
      "Epoch: 10, batch index: 797, learning rate: [1.0000000000000006e-11], loss:2.246183156967163\n",
      "Epoch: 10, batch index: 798, learning rate: [1.0000000000000006e-11], loss:2.2923903465270996\n",
      "Epoch: 10, batch index: 799, learning rate: [1.0000000000000006e-11], loss:2.2799875736236572\n",
      "Epoch: 10, batch index: 800, learning rate: [1.0000000000000006e-11], loss:2.297177791595459\n",
      "Epoch: 10, batch index: 801, learning rate: [1.0000000000000006e-11], loss:2.297567367553711\n",
      "Epoch: 10, batch index: 802, learning rate: [1.0000000000000006e-11], loss:2.2930586338043213\n",
      "Epoch: 10, batch index: 803, learning rate: [1.0000000000000006e-11], loss:2.279013156890869\n",
      "Epoch: 10, batch index: 804, learning rate: [1.0000000000000006e-11], loss:2.218003988265991\n",
      "Epoch: 10, batch index: 805, learning rate: [1.0000000000000006e-11], loss:2.2494263648986816\n",
      "Epoch: 10, batch index: 806, learning rate: [1.0000000000000006e-11], loss:2.1877431869506836\n",
      "Epoch: 10, batch index: 807, learning rate: [1.0000000000000006e-11], loss:2.2655725479125977\n",
      "Epoch: 10, batch index: 808, learning rate: [1.0000000000000006e-11], loss:2.2977325916290283\n",
      "Epoch: 10, batch index: 809, learning rate: [1.0000000000000006e-11], loss:2.32269287109375\n",
      "Epoch: 10, batch index: 810, learning rate: [1.0000000000000006e-12], loss:2.314196825027466\n",
      "Epoch: 10, batch index: 811, learning rate: [1.0000000000000006e-12], loss:2.3285980224609375\n",
      "Epoch: 10, batch index: 812, learning rate: [1.0000000000000006e-12], loss:2.2646350860595703\n",
      "Epoch: 10, batch index: 813, learning rate: [1.0000000000000006e-12], loss:2.217787981033325\n",
      "Epoch: 10, batch index: 814, learning rate: [1.0000000000000006e-12], loss:2.2783327102661133\n",
      "Epoch: 10, batch index: 815, learning rate: [1.0000000000000006e-12], loss:2.256376028060913\n",
      "Epoch: 10, batch index: 816, learning rate: [1.0000000000000006e-12], loss:2.291825294494629\n",
      "Epoch: 10, batch index: 817, learning rate: [1.0000000000000006e-12], loss:2.3298768997192383\n",
      "Epoch: 10, batch index: 818, learning rate: [1.0000000000000006e-12], loss:2.2692205905914307\n",
      "Epoch: 10, batch index: 819, learning rate: [1.0000000000000006e-12], loss:2.29488468170166\n",
      "Epoch: 10, batch index: 820, learning rate: [1.0000000000000006e-12], loss:2.3029305934906006\n",
      "Epoch: 10, batch index: 821, learning rate: [1.0000000000000006e-12], loss:2.2911415100097656\n",
      "Epoch: 10, batch index: 822, learning rate: [1.0000000000000006e-12], loss:2.310389757156372\n",
      "Epoch: 10, batch index: 823, learning rate: [1.0000000000000006e-12], loss:2.2597105503082275\n",
      "Epoch: 10, batch index: 824, learning rate: [1.0000000000000006e-12], loss:2.330596923828125\n",
      "Epoch: 10, batch index: 825, learning rate: [1.0000000000000006e-12], loss:2.3580644130706787\n",
      "Epoch: 10, batch index: 826, learning rate: [1.0000000000000006e-12], loss:2.2616238594055176\n",
      "Epoch: 10, batch index: 827, learning rate: [1.0000000000000006e-12], loss:2.251473903656006\n",
      "Epoch: 10, batch index: 828, learning rate: [1.0000000000000006e-12], loss:2.2568883895874023\n",
      "Epoch: 10, batch index: 829, learning rate: [1.0000000000000006e-12], loss:2.3773891925811768\n",
      "Epoch: 10, batch index: 830, learning rate: [1.0000000000000006e-12], loss:2.288775682449341\n",
      "Epoch: 10, batch index: 831, learning rate: [1.0000000000000006e-12], loss:2.25272274017334\n",
      "Epoch: 10, batch index: 832, learning rate: [1.0000000000000006e-12], loss:2.352121353149414\n",
      "Epoch: 10, batch index: 833, learning rate: [1.0000000000000006e-12], loss:2.299161434173584\n",
      "Epoch: 10, batch index: 834, learning rate: [1.0000000000000006e-12], loss:2.226377010345459\n",
      "Epoch: 10, batch index: 835, learning rate: [1.0000000000000006e-12], loss:2.319760322570801\n",
      "Epoch: 10, batch index: 836, learning rate: [1.0000000000000006e-12], loss:2.2434186935424805\n",
      "Epoch: 10, batch index: 837, learning rate: [1.0000000000000006e-12], loss:2.196018695831299\n",
      "Epoch: 10, batch index: 838, learning rate: [1.0000000000000006e-12], loss:2.2411069869995117\n",
      "Epoch: 10, batch index: 839, learning rate: [1.0000000000000006e-12], loss:2.327329397201538\n",
      "Epoch: 10, batch index: 840, learning rate: [1.0000000000000006e-12], loss:2.3342769145965576\n",
      "Epoch: 10, batch index: 841, learning rate: [1.0000000000000006e-12], loss:2.3111956119537354\n",
      "Epoch: 10, batch index: 842, learning rate: [1.0000000000000006e-12], loss:2.3480865955352783\n",
      "Epoch: 10, batch index: 843, learning rate: [1.0000000000000006e-12], loss:2.2996954917907715\n",
      "Epoch: 10, batch index: 844, learning rate: [1.0000000000000006e-12], loss:2.275275707244873\n",
      "Epoch: 10, batch index: 845, learning rate: [1.0000000000000006e-12], loss:2.2573490142822266\n",
      "Epoch: 10, batch index: 846, learning rate: [1.0000000000000006e-12], loss:2.2242703437805176\n",
      "Epoch: 10, batch index: 847, learning rate: [1.0000000000000006e-12], loss:2.2936906814575195\n",
      "Epoch: 10, batch index: 848, learning rate: [1.0000000000000006e-12], loss:2.2749016284942627\n",
      "Epoch: 10, batch index: 849, learning rate: [1.0000000000000006e-12], loss:2.2884695529937744\n",
      "Epoch: 10, batch index: 850, learning rate: [1.0000000000000006e-12], loss:2.2602248191833496\n",
      "Epoch: 10, batch index: 851, learning rate: [1.0000000000000006e-12], loss:2.2094202041625977\n",
      "Epoch: 10, batch index: 852, learning rate: [1.0000000000000006e-12], loss:2.2450222969055176\n",
      "Epoch: 10, batch index: 853, learning rate: [1.0000000000000006e-12], loss:2.3071866035461426\n",
      "Epoch: 10, batch index: 854, learning rate: [1.0000000000000006e-12], loss:2.276078462600708\n",
      "Epoch: 10, batch index: 855, learning rate: [1.0000000000000006e-12], loss:2.269559621810913\n",
      "Epoch: 10, batch index: 856, learning rate: [1.0000000000000006e-12], loss:2.223602533340454\n",
      "Epoch: 10, batch index: 857, learning rate: [1.0000000000000006e-12], loss:2.311147928237915\n",
      "Epoch: 10, batch index: 858, learning rate: [1.0000000000000006e-12], loss:2.304725408554077\n",
      "Epoch: 10, batch index: 859, learning rate: [1.0000000000000006e-12], loss:2.302887201309204\n",
      "Epoch: 10, batch index: 860, learning rate: [1.0000000000000006e-12], loss:2.2981176376342773\n",
      "Epoch: 10, batch index: 861, learning rate: [1.0000000000000006e-12], loss:2.3010778427124023\n",
      "Epoch: 10, batch index: 862, learning rate: [1.0000000000000006e-12], loss:2.2890026569366455\n",
      "Epoch: 10, batch index: 863, learning rate: [1.0000000000000006e-12], loss:2.3390209674835205\n",
      "Epoch: 10, batch index: 864, learning rate: [1.0000000000000006e-12], loss:2.2503433227539062\n",
      "Epoch: 10, batch index: 865, learning rate: [1.0000000000000006e-12], loss:2.326939582824707\n",
      "Epoch: 10, batch index: 866, learning rate: [1.0000000000000006e-12], loss:2.2540929317474365\n",
      "Epoch: 10, batch index: 867, learning rate: [1.0000000000000006e-12], loss:2.2578370571136475\n",
      "Epoch: 10, batch index: 868, learning rate: [1.0000000000000007e-13], loss:2.413064479827881\n",
      "Calculating validation accuracy\n",
      "Epoch 10, validation accuracy score0.4265625\n",
      "Epoch: 11, batch index: 869, learning rate: [1.0000000000000007e-13], loss:2.22121262550354\n",
      "Epoch: 11, batch index: 870, learning rate: [1.0000000000000007e-13], loss:2.256989002227783\n",
      "Epoch: 11, batch index: 871, learning rate: [1.0000000000000007e-13], loss:2.2253036499023438\n",
      "Epoch: 11, batch index: 872, learning rate: [1.0000000000000007e-13], loss:2.3460118770599365\n",
      "Epoch: 11, batch index: 873, learning rate: [1.0000000000000007e-13], loss:2.265463352203369\n",
      "Epoch: 11, batch index: 874, learning rate: [1.0000000000000007e-13], loss:2.297004222869873\n",
      "Epoch: 11, batch index: 875, learning rate: [1.0000000000000007e-13], loss:2.2312984466552734\n",
      "Epoch: 11, batch index: 876, learning rate: [1.0000000000000007e-13], loss:2.3069865703582764\n",
      "Epoch: 11, batch index: 877, learning rate: [1.0000000000000007e-13], loss:2.251377582550049\n",
      "Epoch: 11, batch index: 878, learning rate: [1.0000000000000007e-13], loss:2.2996344566345215\n",
      "Epoch: 11, batch index: 879, learning rate: [1.0000000000000007e-13], loss:2.3015949726104736\n",
      "Epoch: 11, batch index: 880, learning rate: [1.0000000000000007e-13], loss:2.290464162826538\n",
      "Epoch: 11, batch index: 881, learning rate: [1.0000000000000007e-13], loss:2.3069345951080322\n",
      "Epoch: 11, batch index: 882, learning rate: [1.0000000000000007e-13], loss:2.270669460296631\n",
      "Epoch: 11, batch index: 883, learning rate: [1.0000000000000007e-13], loss:2.1751904487609863\n",
      "Epoch: 11, batch index: 884, learning rate: [1.0000000000000007e-13], loss:2.22410249710083\n",
      "Epoch: 11, batch index: 885, learning rate: [1.0000000000000007e-13], loss:2.265077829360962\n",
      "Epoch: 11, batch index: 886, learning rate: [1.0000000000000007e-13], loss:2.276702880859375\n",
      "Epoch: 11, batch index: 887, learning rate: [1.0000000000000007e-13], loss:2.2289717197418213\n",
      "Epoch: 11, batch index: 888, learning rate: [1.0000000000000007e-13], loss:2.310264825820923\n",
      "Epoch: 11, batch index: 889, learning rate: [1.0000000000000007e-13], loss:2.2685890197753906\n",
      "Epoch: 11, batch index: 890, learning rate: [1.0000000000000007e-13], loss:2.3007280826568604\n",
      "Epoch: 11, batch index: 891, learning rate: [1.0000000000000007e-13], loss:2.2805709838867188\n",
      "Epoch: 11, batch index: 892, learning rate: [1.0000000000000007e-13], loss:2.330101251602173\n",
      "Epoch: 11, batch index: 893, learning rate: [1.0000000000000007e-13], loss:2.312708854675293\n",
      "Epoch: 11, batch index: 894, learning rate: [1.0000000000000007e-13], loss:2.289396047592163\n",
      "Epoch: 11, batch index: 895, learning rate: [1.0000000000000007e-13], loss:2.211162805557251\n",
      "Epoch: 11, batch index: 896, learning rate: [1.0000000000000007e-13], loss:2.2733898162841797\n",
      "Epoch: 11, batch index: 897, learning rate: [1.0000000000000007e-13], loss:2.4100565910339355\n",
      "Epoch: 11, batch index: 898, learning rate: [1.0000000000000007e-13], loss:2.2935843467712402\n",
      "Epoch: 11, batch index: 899, learning rate: [1.0000000000000007e-13], loss:2.328531265258789\n",
      "Epoch: 11, batch index: 900, learning rate: [1.0000000000000007e-13], loss:2.2862725257873535\n",
      "Epoch: 11, batch index: 901, learning rate: [1.0000000000000007e-13], loss:2.305706262588501\n",
      "Epoch: 11, batch index: 902, learning rate: [1.0000000000000007e-13], loss:2.328568696975708\n",
      "Epoch: 11, batch index: 903, learning rate: [1.0000000000000007e-13], loss:2.2956788539886475\n",
      "Epoch: 11, batch index: 904, learning rate: [1.0000000000000007e-13], loss:2.2999672889709473\n",
      "Epoch: 11, batch index: 905, learning rate: [1.0000000000000007e-13], loss:2.2777230739593506\n",
      "Epoch: 11, batch index: 906, learning rate: [1.0000000000000007e-13], loss:2.2048990726470947\n",
      "Epoch: 11, batch index: 907, learning rate: [1.0000000000000007e-13], loss:2.3249268531799316\n",
      "Epoch: 11, batch index: 908, learning rate: [1.0000000000000007e-13], loss:2.2624032497406006\n",
      "Epoch: 11, batch index: 909, learning rate: [1.0000000000000007e-13], loss:2.25869083404541\n",
      "Epoch: 11, batch index: 910, learning rate: [1.0000000000000007e-13], loss:2.279733419418335\n",
      "Epoch: 11, batch index: 911, learning rate: [1.0000000000000007e-13], loss:2.292738437652588\n",
      "Epoch: 11, batch index: 912, learning rate: [1.0000000000000007e-13], loss:2.2609899044036865\n",
      "Epoch: 11, batch index: 913, learning rate: [1.0000000000000007e-13], loss:2.275707721710205\n",
      "Epoch: 11, batch index: 914, learning rate: [1.0000000000000007e-13], loss:2.175065517425537\n",
      "Epoch: 11, batch index: 915, learning rate: [1.0000000000000007e-13], loss:2.359372138977051\n",
      "Epoch: 11, batch index: 916, learning rate: [1.0000000000000007e-13], loss:2.2675621509552\n",
      "Epoch: 11, batch index: 917, learning rate: [1.0000000000000007e-13], loss:2.215320110321045\n",
      "Epoch: 11, batch index: 918, learning rate: [1.0000000000000007e-13], loss:2.3602499961853027\n",
      "Epoch: 11, batch index: 919, learning rate: [1.0000000000000007e-13], loss:2.268052101135254\n",
      "Epoch: 11, batch index: 920, learning rate: [1.0000000000000007e-13], loss:2.3119499683380127\n",
      "Epoch: 11, batch index: 921, learning rate: [1.0000000000000007e-13], loss:2.2791998386383057\n",
      "Epoch: 11, batch index: 922, learning rate: [1.0000000000000007e-13], loss:2.2052149772644043\n",
      "Epoch: 11, batch index: 923, learning rate: [1.0000000000000007e-13], loss:2.2375638484954834\n",
      "Epoch: 11, batch index: 924, learning rate: [1.0000000000000007e-13], loss:2.254034996032715\n",
      "Epoch: 11, batch index: 925, learning rate: [1.0000000000000007e-13], loss:2.2773590087890625\n",
      "Epoch: 11, batch index: 926, learning rate: [1.0000000000000008e-14], loss:2.2639877796173096\n",
      "Epoch: 11, batch index: 927, learning rate: [1.0000000000000008e-14], loss:2.305972099304199\n",
      "Epoch: 11, batch index: 928, learning rate: [1.0000000000000008e-14], loss:2.3226118087768555\n",
      "Epoch: 11, batch index: 929, learning rate: [1.0000000000000008e-14], loss:2.306335210800171\n",
      "Epoch: 11, batch index: 930, learning rate: [1.0000000000000008e-14], loss:2.3827996253967285\n",
      "Epoch: 11, batch index: 931, learning rate: [1.0000000000000008e-14], loss:2.358670711517334\n",
      "Epoch: 11, batch index: 932, learning rate: [1.0000000000000008e-14], loss:2.267314910888672\n",
      "Epoch: 11, batch index: 933, learning rate: [1.0000000000000008e-14], loss:2.3221120834350586\n",
      "Epoch: 11, batch index: 934, learning rate: [1.0000000000000008e-14], loss:2.280386447906494\n",
      "Epoch: 11, batch index: 935, learning rate: [1.0000000000000008e-14], loss:2.285161256790161\n",
      "Epoch: 11, batch index: 936, learning rate: [1.0000000000000008e-14], loss:2.2206716537475586\n",
      "Epoch: 11, batch index: 937, learning rate: [1.0000000000000008e-14], loss:2.2754058837890625\n",
      "Epoch: 11, batch index: 938, learning rate: [1.0000000000000008e-14], loss:2.2636306285858154\n",
      "Epoch: 11, batch index: 939, learning rate: [1.0000000000000008e-14], loss:2.2381091117858887\n",
      "Epoch: 11, batch index: 940, learning rate: [1.0000000000000008e-14], loss:2.29654860496521\n",
      "Epoch: 11, batch index: 941, learning rate: [1.0000000000000008e-14], loss:2.3040332794189453\n",
      "Epoch: 11, batch index: 942, learning rate: [1.0000000000000008e-14], loss:2.3222804069519043\n",
      "Epoch: 11, batch index: 943, learning rate: [1.0000000000000008e-14], loss:2.3042051792144775\n",
      "Epoch: 11, batch index: 944, learning rate: [1.0000000000000008e-14], loss:2.264763116836548\n",
      "Epoch: 11, batch index: 945, learning rate: [1.0000000000000008e-14], loss:2.2702035903930664\n",
      "Epoch: 11, batch index: 946, learning rate: [1.0000000000000008e-14], loss:2.3205299377441406\n",
      "Epoch: 11, batch index: 947, learning rate: [1.0000000000000008e-14], loss:2.4725446701049805\n",
      "Calculating validation accuracy\n",
      "Epoch 11, validation accuracy score0.43046875\n",
      "Epoch: 12, batch index: 948, learning rate: [1.0000000000000008e-14], loss:2.3578288555145264\n",
      "Epoch: 12, batch index: 949, learning rate: [1.0000000000000008e-14], loss:2.301980972290039\n",
      "Epoch: 12, batch index: 950, learning rate: [1.0000000000000008e-14], loss:2.3014273643493652\n",
      "Epoch: 12, batch index: 951, learning rate: [1.0000000000000008e-14], loss:2.2807209491729736\n",
      "Epoch: 12, batch index: 952, learning rate: [1.0000000000000008e-14], loss:2.2681615352630615\n",
      "Epoch: 12, batch index: 953, learning rate: [1.0000000000000008e-14], loss:2.266265869140625\n",
      "Epoch: 12, batch index: 954, learning rate: [1.0000000000000008e-14], loss:2.212745428085327\n",
      "Epoch: 12, batch index: 955, learning rate: [1.0000000000000008e-14], loss:2.301210880279541\n",
      "Epoch: 12, batch index: 956, learning rate: [1.0000000000000008e-14], loss:2.2552907466888428\n",
      "Epoch: 12, batch index: 957, learning rate: [1.0000000000000008e-14], loss:2.318899154663086\n",
      "Epoch: 12, batch index: 958, learning rate: [1.0000000000000008e-14], loss:2.2941718101501465\n",
      "Epoch: 12, batch index: 959, learning rate: [1.0000000000000008e-14], loss:2.2800633907318115\n",
      "Epoch: 12, batch index: 960, learning rate: [1.0000000000000008e-14], loss:2.2735815048217773\n",
      "Epoch: 12, batch index: 961, learning rate: [1.0000000000000008e-14], loss:2.3209171295166016\n",
      "Epoch: 12, batch index: 962, learning rate: [1.0000000000000008e-14], loss:2.267244338989258\n",
      "Epoch: 12, batch index: 963, learning rate: [1.0000000000000008e-14], loss:2.295600652694702\n",
      "Epoch: 12, batch index: 964, learning rate: [1.0000000000000008e-14], loss:2.2640886306762695\n",
      "Epoch: 12, batch index: 965, learning rate: [1.0000000000000008e-14], loss:2.3059439659118652\n",
      "Epoch: 12, batch index: 966, learning rate: [1.0000000000000008e-14], loss:2.3516170978546143\n",
      "Epoch: 12, batch index: 967, learning rate: [1.0000000000000008e-14], loss:2.294827938079834\n",
      "Epoch: 12, batch index: 968, learning rate: [1.0000000000000008e-14], loss:2.3702921867370605\n",
      "Epoch: 12, batch index: 969, learning rate: [1.0000000000000008e-14], loss:2.2758548259735107\n",
      "Epoch: 12, batch index: 970, learning rate: [1.0000000000000008e-14], loss:2.2952070236206055\n",
      "Epoch: 12, batch index: 971, learning rate: [1.0000000000000008e-14], loss:2.3444933891296387\n",
      "Epoch: 12, batch index: 972, learning rate: [1.0000000000000008e-14], loss:2.2580008506774902\n",
      "Epoch: 12, batch index: 973, learning rate: [1.0000000000000008e-14], loss:2.2734036445617676\n",
      "Epoch: 12, batch index: 974, learning rate: [1.0000000000000008e-14], loss:2.313655376434326\n",
      "Epoch: 12, batch index: 975, learning rate: [1.0000000000000008e-14], loss:2.2372968196868896\n",
      "Epoch: 12, batch index: 976, learning rate: [1.0000000000000008e-14], loss:2.222379446029663\n",
      "Epoch: 12, batch index: 977, learning rate: [1.0000000000000008e-14], loss:2.2421412467956543\n",
      "Epoch: 12, batch index: 978, learning rate: [1.0000000000000008e-14], loss:2.3598008155822754\n",
      "Epoch: 12, batch index: 979, learning rate: [1.0000000000000008e-14], loss:2.30336856842041\n",
      "Epoch: 12, batch index: 980, learning rate: [1.0000000000000008e-14], loss:2.3394229412078857\n",
      "Epoch: 12, batch index: 981, learning rate: [1.0000000000000008e-14], loss:2.2935080528259277\n",
      "Epoch: 12, batch index: 982, learning rate: [1.0000000000000008e-14], loss:2.232532024383545\n",
      "Epoch: 12, batch index: 983, learning rate: [1.0000000000000008e-14], loss:2.290905237197876\n",
      "Epoch: 12, batch index: 984, learning rate: [1.0000000000000009e-15], loss:2.261786460876465\n",
      "Epoch: 12, batch index: 985, learning rate: [1.0000000000000009e-15], loss:2.275468587875366\n",
      "Epoch: 12, batch index: 986, learning rate: [1.0000000000000009e-15], loss:2.2584941387176514\n",
      "Epoch: 12, batch index: 987, learning rate: [1.0000000000000009e-15], loss:2.302962064743042\n",
      "Epoch: 12, batch index: 988, learning rate: [1.0000000000000009e-15], loss:2.316798686981201\n",
      "Epoch: 12, batch index: 989, learning rate: [1.0000000000000009e-15], loss:2.2749905586242676\n",
      "Epoch: 12, batch index: 990, learning rate: [1.0000000000000009e-15], loss:2.291599988937378\n",
      "Epoch: 12, batch index: 991, learning rate: [1.0000000000000009e-15], loss:2.264164447784424\n",
      "Epoch: 12, batch index: 992, learning rate: [1.0000000000000009e-15], loss:2.2378201484680176\n",
      "Epoch: 12, batch index: 993, learning rate: [1.0000000000000009e-15], loss:2.2861106395721436\n",
      "Epoch: 12, batch index: 994, learning rate: [1.0000000000000009e-15], loss:2.298806667327881\n",
      "Epoch: 12, batch index: 995, learning rate: [1.0000000000000009e-15], loss:2.309969663619995\n",
      "Epoch: 12, batch index: 996, learning rate: [1.0000000000000009e-15], loss:2.2880232334136963\n",
      "Epoch: 12, batch index: 997, learning rate: [1.0000000000000009e-15], loss:2.2378087043762207\n",
      "Epoch: 12, batch index: 998, learning rate: [1.0000000000000009e-15], loss:2.2486379146575928\n",
      "Epoch: 12, batch index: 999, learning rate: [1.0000000000000009e-15], loss:2.348376512527466\n",
      "Epoch: 12, batch index: 1000, learning rate: [1.0000000000000009e-15], loss:2.2424895763397217\n",
      "Epoch: 12, batch index: 1001, learning rate: [1.0000000000000009e-15], loss:2.314532518386841\n",
      "Epoch: 12, batch index: 1002, learning rate: [1.0000000000000009e-15], loss:2.234997034072876\n",
      "Epoch: 12, batch index: 1003, learning rate: [1.0000000000000009e-15], loss:2.2583870887756348\n",
      "Epoch: 12, batch index: 1004, learning rate: [1.0000000000000009e-15], loss:2.299513816833496\n",
      "Epoch: 12, batch index: 1005, learning rate: [1.0000000000000009e-15], loss:2.2583014965057373\n",
      "Epoch: 12, batch index: 1006, learning rate: [1.0000000000000009e-15], loss:2.198618173599243\n",
      "Epoch: 12, batch index: 1007, learning rate: [1.0000000000000009e-15], loss:2.354259967803955\n",
      "Epoch: 12, batch index: 1008, learning rate: [1.0000000000000009e-15], loss:2.268925189971924\n",
      "Epoch: 12, batch index: 1009, learning rate: [1.0000000000000009e-15], loss:2.2432751655578613\n",
      "Epoch: 12, batch index: 1010, learning rate: [1.0000000000000009e-15], loss:2.3418374061584473\n",
      "Epoch: 12, batch index: 1011, learning rate: [1.0000000000000009e-15], loss:2.298900842666626\n",
      "Epoch: 12, batch index: 1012, learning rate: [1.0000000000000009e-15], loss:2.3087844848632812\n",
      "Epoch: 12, batch index: 1013, learning rate: [1.0000000000000009e-15], loss:2.335312604904175\n",
      "Epoch: 12, batch index: 1014, learning rate: [1.0000000000000009e-15], loss:2.2447874546051025\n",
      "Epoch: 12, batch index: 1015, learning rate: [1.0000000000000009e-15], loss:2.309394121170044\n",
      "Epoch: 12, batch index: 1016, learning rate: [1.0000000000000009e-15], loss:2.301948308944702\n",
      "Epoch: 12, batch index: 1017, learning rate: [1.0000000000000009e-15], loss:2.2691659927368164\n",
      "Epoch: 12, batch index: 1018, learning rate: [1.0000000000000009e-15], loss:2.2592928409576416\n",
      "Epoch: 12, batch index: 1019, learning rate: [1.0000000000000009e-15], loss:2.287505865097046\n",
      "Epoch: 12, batch index: 1020, learning rate: [1.0000000000000009e-15], loss:2.342055082321167\n",
      "Epoch: 12, batch index: 1021, learning rate: [1.0000000000000009e-15], loss:2.227524518966675\n",
      "Epoch: 12, batch index: 1022, learning rate: [1.0000000000000009e-15], loss:2.257488489151001\n",
      "Epoch: 12, batch index: 1023, learning rate: [1.0000000000000009e-15], loss:2.243183135986328\n",
      "Epoch: 12, batch index: 1024, learning rate: [1.0000000000000009e-15], loss:2.300813913345337\n",
      "Epoch: 12, batch index: 1025, learning rate: [1.0000000000000009e-15], loss:2.2645347118377686\n",
      "Epoch: 12, batch index: 1026, learning rate: [1.0000000000000009e-15], loss:2.2501683235168457\n",
      "Calculating validation accuracy\n",
      "Epoch 12, validation accuracy score0.42734375\n",
      "Epoch: 13, batch index: 1027, learning rate: [1.0000000000000009e-15], loss:2.308396577835083\n",
      "Epoch: 13, batch index: 1028, learning rate: [1.0000000000000009e-15], loss:2.2498996257781982\n",
      "Epoch: 13, batch index: 1029, learning rate: [1.0000000000000009e-15], loss:2.37703275680542\n",
      "Epoch: 13, batch index: 1030, learning rate: [1.0000000000000009e-15], loss:2.2938404083251953\n",
      "Epoch: 13, batch index: 1031, learning rate: [1.0000000000000009e-15], loss:2.2603085041046143\n",
      "Epoch: 13, batch index: 1032, learning rate: [1.0000000000000009e-15], loss:2.360570192337036\n",
      "Epoch: 13, batch index: 1033, learning rate: [1.0000000000000009e-15], loss:2.256695032119751\n",
      "Epoch: 13, batch index: 1034, learning rate: [1.0000000000000009e-15], loss:2.331284999847412\n",
      "Epoch: 13, batch index: 1035, learning rate: [1.0000000000000009e-15], loss:2.243297576904297\n",
      "Epoch: 13, batch index: 1036, learning rate: [1.0000000000000009e-15], loss:2.2234556674957275\n",
      "Epoch: 13, batch index: 1037, learning rate: [1.0000000000000009e-15], loss:2.2913050651550293\n",
      "Epoch: 13, batch index: 1038, learning rate: [1.0000000000000009e-15], loss:2.2793400287628174\n",
      "Epoch: 13, batch index: 1039, learning rate: [1.0000000000000009e-15], loss:2.2706992626190186\n",
      "Epoch: 13, batch index: 1040, learning rate: [1.0000000000000009e-15], loss:2.301278829574585\n",
      "Epoch: 13, batch index: 1041, learning rate: [1.0000000000000009e-15], loss:2.259655475616455\n",
      "Epoch: 13, batch index: 1042, learning rate: [1.000000000000001e-16], loss:2.235518455505371\n",
      "Epoch: 13, batch index: 1043, learning rate: [1.000000000000001e-16], loss:2.25319242477417\n",
      "Epoch: 13, batch index: 1044, learning rate: [1.000000000000001e-16], loss:2.252554178237915\n",
      "Epoch: 13, batch index: 1045, learning rate: [1.000000000000001e-16], loss:2.2389426231384277\n",
      "Epoch: 13, batch index: 1046, learning rate: [1.000000000000001e-16], loss:2.3363583087921143\n",
      "Epoch: 13, batch index: 1047, learning rate: [1.000000000000001e-16], loss:2.2622499465942383\n",
      "Epoch: 13, batch index: 1048, learning rate: [1.000000000000001e-16], loss:2.235441207885742\n",
      "Epoch: 13, batch index: 1049, learning rate: [1.000000000000001e-16], loss:2.26971173286438\n",
      "Epoch: 13, batch index: 1050, learning rate: [1.000000000000001e-16], loss:2.3036599159240723\n",
      "Epoch: 13, batch index: 1051, learning rate: [1.000000000000001e-16], loss:2.2680089473724365\n",
      "Epoch: 13, batch index: 1052, learning rate: [1.000000000000001e-16], loss:2.253070831298828\n",
      "Epoch: 13, batch index: 1053, learning rate: [1.000000000000001e-16], loss:2.3124356269836426\n",
      "Epoch: 13, batch index: 1054, learning rate: [1.000000000000001e-16], loss:2.2648203372955322\n",
      "Epoch: 13, batch index: 1055, learning rate: [1.000000000000001e-16], loss:2.2464747428894043\n",
      "Epoch: 13, batch index: 1056, learning rate: [1.000000000000001e-16], loss:2.2940099239349365\n",
      "Epoch: 13, batch index: 1057, learning rate: [1.000000000000001e-16], loss:2.338381052017212\n",
      "Epoch: 13, batch index: 1058, learning rate: [1.000000000000001e-16], loss:2.3384552001953125\n",
      "Epoch: 13, batch index: 1059, learning rate: [1.000000000000001e-16], loss:2.2409989833831787\n",
      "Epoch: 13, batch index: 1060, learning rate: [1.000000000000001e-16], loss:2.2759909629821777\n",
      "Epoch: 13, batch index: 1061, learning rate: [1.000000000000001e-16], loss:2.2449352741241455\n",
      "Epoch: 13, batch index: 1062, learning rate: [1.000000000000001e-16], loss:2.2943215370178223\n",
      "Epoch: 13, batch index: 1063, learning rate: [1.000000000000001e-16], loss:2.2960128784179688\n",
      "Epoch: 13, batch index: 1064, learning rate: [1.000000000000001e-16], loss:2.3052642345428467\n",
      "Epoch: 13, batch index: 1065, learning rate: [1.000000000000001e-16], loss:2.293330430984497\n",
      "Epoch: 13, batch index: 1066, learning rate: [1.000000000000001e-16], loss:2.2816967964172363\n",
      "Epoch: 13, batch index: 1067, learning rate: [1.000000000000001e-16], loss:2.2839365005493164\n",
      "Epoch: 13, batch index: 1068, learning rate: [1.000000000000001e-16], loss:2.2724688053131104\n",
      "Epoch: 13, batch index: 1069, learning rate: [1.000000000000001e-16], loss:2.2724127769470215\n",
      "Epoch: 13, batch index: 1070, learning rate: [1.000000000000001e-16], loss:2.2707881927490234\n",
      "Epoch: 13, batch index: 1071, learning rate: [1.000000000000001e-16], loss:2.2678909301757812\n",
      "Epoch: 13, batch index: 1072, learning rate: [1.000000000000001e-16], loss:2.2508862018585205\n",
      "Epoch: 13, batch index: 1073, learning rate: [1.000000000000001e-16], loss:2.2807717323303223\n",
      "Epoch: 13, batch index: 1074, learning rate: [1.000000000000001e-16], loss:2.3115811347961426\n",
      "Epoch: 13, batch index: 1075, learning rate: [1.000000000000001e-16], loss:2.2794241905212402\n",
      "Epoch: 13, batch index: 1076, learning rate: [1.000000000000001e-16], loss:2.3179478645324707\n",
      "Epoch: 13, batch index: 1077, learning rate: [1.000000000000001e-16], loss:2.235957384109497\n",
      "Epoch: 13, batch index: 1078, learning rate: [1.000000000000001e-16], loss:2.284189224243164\n",
      "Epoch: 13, batch index: 1079, learning rate: [1.000000000000001e-16], loss:2.21128511428833\n",
      "Epoch: 13, batch index: 1080, learning rate: [1.000000000000001e-16], loss:2.267083168029785\n",
      "Epoch: 13, batch index: 1081, learning rate: [1.000000000000001e-16], loss:2.2997052669525146\n",
      "Epoch: 13, batch index: 1082, learning rate: [1.000000000000001e-16], loss:2.2734150886535645\n",
      "Epoch: 13, batch index: 1083, learning rate: [1.000000000000001e-16], loss:2.205596923828125\n",
      "Epoch: 13, batch index: 1084, learning rate: [1.000000000000001e-16], loss:2.3331193923950195\n",
      "Epoch: 13, batch index: 1085, learning rate: [1.000000000000001e-16], loss:2.2353005409240723\n",
      "Epoch: 13, batch index: 1086, learning rate: [1.000000000000001e-16], loss:2.3492889404296875\n",
      "Epoch: 13, batch index: 1087, learning rate: [1.000000000000001e-16], loss:2.2433948516845703\n",
      "Epoch: 13, batch index: 1088, learning rate: [1.000000000000001e-16], loss:2.2546443939208984\n",
      "Epoch: 13, batch index: 1089, learning rate: [1.000000000000001e-16], loss:2.2836787700653076\n",
      "Epoch: 13, batch index: 1090, learning rate: [1.000000000000001e-16], loss:2.3365612030029297\n",
      "Epoch: 13, batch index: 1091, learning rate: [1.000000000000001e-16], loss:2.339313507080078\n",
      "Epoch: 13, batch index: 1092, learning rate: [1.000000000000001e-16], loss:2.2545697689056396\n",
      "Epoch: 13, batch index: 1093, learning rate: [1.000000000000001e-16], loss:2.3004977703094482\n",
      "Epoch: 13, batch index: 1094, learning rate: [1.000000000000001e-16], loss:2.2534291744232178\n",
      "Epoch: 13, batch index: 1095, learning rate: [1.000000000000001e-16], loss:2.287848711013794\n",
      "Epoch: 13, batch index: 1096, learning rate: [1.000000000000001e-16], loss:2.250704765319824\n",
      "Epoch: 13, batch index: 1097, learning rate: [1.000000000000001e-16], loss:2.334832191467285\n",
      "Epoch: 13, batch index: 1098, learning rate: [1.000000000000001e-16], loss:2.2790143489837646\n",
      "Epoch: 13, batch index: 1099, learning rate: [1.000000000000001e-16], loss:2.3240275382995605\n",
      "Epoch: 13, batch index: 1100, learning rate: [1.000000000000001e-17], loss:2.2059836387634277\n",
      "Epoch: 13, batch index: 1101, learning rate: [1.000000000000001e-17], loss:2.354391098022461\n",
      "Epoch: 13, batch index: 1102, learning rate: [1.000000000000001e-17], loss:2.2212095260620117\n",
      "Epoch: 13, batch index: 1103, learning rate: [1.000000000000001e-17], loss:2.3089218139648438\n",
      "Epoch: 13, batch index: 1104, learning rate: [1.000000000000001e-17], loss:2.328932523727417\n",
      "Epoch: 13, batch index: 1105, learning rate: [1.000000000000001e-17], loss:2.2447702884674072\n",
      "Calculating validation accuracy\n",
      "Epoch 13, validation accuracy score0.426171875\n",
      "Epoch: 14, batch index: 1106, learning rate: [1.000000000000001e-17], loss:2.250469446182251\n",
      "Epoch: 14, batch index: 1107, learning rate: [1.000000000000001e-17], loss:2.2246108055114746\n",
      "Epoch: 14, batch index: 1108, learning rate: [1.000000000000001e-17], loss:2.263418197631836\n",
      "Epoch: 14, batch index: 1109, learning rate: [1.000000000000001e-17], loss:2.249143362045288\n",
      "Epoch: 14, batch index: 1110, learning rate: [1.000000000000001e-17], loss:2.2803492546081543\n",
      "Epoch: 14, batch index: 1111, learning rate: [1.000000000000001e-17], loss:2.259523630142212\n",
      "Epoch: 14, batch index: 1112, learning rate: [1.000000000000001e-17], loss:2.3533174991607666\n",
      "Epoch: 14, batch index: 1113, learning rate: [1.000000000000001e-17], loss:2.2861990928649902\n",
      "Epoch: 14, batch index: 1114, learning rate: [1.000000000000001e-17], loss:2.2648558616638184\n",
      "Epoch: 14, batch index: 1115, learning rate: [1.000000000000001e-17], loss:2.2567460536956787\n",
      "Epoch: 14, batch index: 1116, learning rate: [1.000000000000001e-17], loss:2.2291810512542725\n",
      "Epoch: 14, batch index: 1117, learning rate: [1.000000000000001e-17], loss:2.310903310775757\n",
      "Epoch: 14, batch index: 1118, learning rate: [1.000000000000001e-17], loss:2.294140577316284\n",
      "Epoch: 14, batch index: 1119, learning rate: [1.000000000000001e-17], loss:2.302565097808838\n",
      "Epoch: 14, batch index: 1120, learning rate: [1.000000000000001e-17], loss:2.28263521194458\n",
      "Epoch: 14, batch index: 1121, learning rate: [1.000000000000001e-17], loss:2.341763973236084\n",
      "Epoch: 14, batch index: 1122, learning rate: [1.000000000000001e-17], loss:2.311937093734741\n",
      "Epoch: 14, batch index: 1123, learning rate: [1.000000000000001e-17], loss:2.2977397441864014\n",
      "Epoch: 14, batch index: 1124, learning rate: [1.000000000000001e-17], loss:2.2580180168151855\n",
      "Epoch: 14, batch index: 1125, learning rate: [1.000000000000001e-17], loss:2.325237274169922\n",
      "Epoch: 14, batch index: 1126, learning rate: [1.000000000000001e-17], loss:2.217423439025879\n",
      "Epoch: 14, batch index: 1127, learning rate: [1.000000000000001e-17], loss:2.246389150619507\n",
      "Epoch: 14, batch index: 1128, learning rate: [1.000000000000001e-17], loss:2.314286708831787\n",
      "Epoch: 14, batch index: 1129, learning rate: [1.000000000000001e-17], loss:2.29296612739563\n",
      "Epoch: 14, batch index: 1130, learning rate: [1.000000000000001e-17], loss:2.2655129432678223\n",
      "Epoch: 14, batch index: 1131, learning rate: [1.000000000000001e-17], loss:2.2624728679656982\n",
      "Epoch: 14, batch index: 1132, learning rate: [1.000000000000001e-17], loss:2.3543262481689453\n",
      "Epoch: 14, batch index: 1133, learning rate: [1.000000000000001e-17], loss:2.332979679107666\n",
      "Epoch: 14, batch index: 1134, learning rate: [1.000000000000001e-17], loss:2.341330051422119\n",
      "Epoch: 14, batch index: 1135, learning rate: [1.000000000000001e-17], loss:2.339109420776367\n",
      "Epoch: 14, batch index: 1136, learning rate: [1.000000000000001e-17], loss:2.2133677005767822\n",
      "Epoch: 14, batch index: 1137, learning rate: [1.000000000000001e-17], loss:2.273029088973999\n",
      "Epoch: 14, batch index: 1138, learning rate: [1.000000000000001e-17], loss:2.2936415672302246\n",
      "Epoch: 14, batch index: 1139, learning rate: [1.000000000000001e-17], loss:2.2344777584075928\n",
      "Epoch: 14, batch index: 1140, learning rate: [1.000000000000001e-17], loss:2.191714286804199\n",
      "Epoch: 14, batch index: 1141, learning rate: [1.000000000000001e-17], loss:2.314436197280884\n",
      "Epoch: 14, batch index: 1142, learning rate: [1.000000000000001e-17], loss:2.1773860454559326\n",
      "Epoch: 14, batch index: 1143, learning rate: [1.000000000000001e-17], loss:2.2502896785736084\n",
      "Epoch: 14, batch index: 1144, learning rate: [1.000000000000001e-17], loss:2.335176706314087\n",
      "Epoch: 14, batch index: 1145, learning rate: [1.000000000000001e-17], loss:2.3133203983306885\n",
      "Epoch: 14, batch index: 1146, learning rate: [1.000000000000001e-17], loss:2.3065812587738037\n",
      "Epoch: 14, batch index: 1147, learning rate: [1.000000000000001e-17], loss:2.2950639724731445\n",
      "Epoch: 14, batch index: 1148, learning rate: [1.000000000000001e-17], loss:2.3165321350097656\n",
      "Epoch: 14, batch index: 1149, learning rate: [1.000000000000001e-17], loss:2.3339781761169434\n",
      "Epoch: 14, batch index: 1150, learning rate: [1.000000000000001e-17], loss:2.3723576068878174\n",
      "Epoch: 14, batch index: 1151, learning rate: [1.000000000000001e-17], loss:2.2322640419006348\n",
      "Epoch: 14, batch index: 1152, learning rate: [1.000000000000001e-17], loss:2.4061484336853027\n",
      "Epoch: 14, batch index: 1153, learning rate: [1.000000000000001e-17], loss:2.2553884983062744\n",
      "Epoch: 14, batch index: 1154, learning rate: [1.000000000000001e-17], loss:2.2969236373901367\n",
      "Epoch: 14, batch index: 1155, learning rate: [1.000000000000001e-17], loss:2.2693042755126953\n",
      "Epoch: 14, batch index: 1156, learning rate: [1.000000000000001e-17], loss:2.348161458969116\n",
      "Epoch: 14, batch index: 1157, learning rate: [1.000000000000001e-17], loss:2.3283040523529053\n",
      "Epoch: 14, batch index: 1158, learning rate: [1.000000000000001e-18], loss:2.2219655513763428\n",
      "Epoch: 14, batch index: 1159, learning rate: [1.000000000000001e-18], loss:2.2824347019195557\n",
      "Epoch: 14, batch index: 1160, learning rate: [1.000000000000001e-18], loss:2.2893919944763184\n",
      "Epoch: 14, batch index: 1161, learning rate: [1.000000000000001e-18], loss:2.2820920944213867\n",
      "Epoch: 14, batch index: 1162, learning rate: [1.000000000000001e-18], loss:2.230525016784668\n",
      "Epoch: 14, batch index: 1163, learning rate: [1.000000000000001e-18], loss:2.335175037384033\n",
      "Epoch: 14, batch index: 1164, learning rate: [1.000000000000001e-18], loss:2.353281259536743\n",
      "Epoch: 14, batch index: 1165, learning rate: [1.000000000000001e-18], loss:2.290127754211426\n",
      "Epoch: 14, batch index: 1166, learning rate: [1.000000000000001e-18], loss:2.295579195022583\n",
      "Epoch: 14, batch index: 1167, learning rate: [1.000000000000001e-18], loss:2.2317848205566406\n",
      "Epoch: 14, batch index: 1168, learning rate: [1.000000000000001e-18], loss:2.283216953277588\n",
      "Epoch: 14, batch index: 1169, learning rate: [1.000000000000001e-18], loss:2.308784008026123\n",
      "Epoch: 14, batch index: 1170, learning rate: [1.000000000000001e-18], loss:2.257006883621216\n",
      "Epoch: 14, batch index: 1171, learning rate: [1.000000000000001e-18], loss:2.3115081787109375\n",
      "Epoch: 14, batch index: 1172, learning rate: [1.000000000000001e-18], loss:2.2753169536590576\n",
      "Epoch: 14, batch index: 1173, learning rate: [1.000000000000001e-18], loss:2.254730463027954\n",
      "Epoch: 14, batch index: 1174, learning rate: [1.000000000000001e-18], loss:2.2344419956207275\n",
      "Epoch: 14, batch index: 1175, learning rate: [1.000000000000001e-18], loss:2.253756284713745\n",
      "Epoch: 14, batch index: 1176, learning rate: [1.000000000000001e-18], loss:2.2914605140686035\n",
      "Epoch: 14, batch index: 1177, learning rate: [1.000000000000001e-18], loss:2.358919143676758\n",
      "Epoch: 14, batch index: 1178, learning rate: [1.000000000000001e-18], loss:2.3113181591033936\n",
      "Epoch: 14, batch index: 1179, learning rate: [1.000000000000001e-18], loss:2.263674020767212\n",
      "Epoch: 14, batch index: 1180, learning rate: [1.000000000000001e-18], loss:2.247481346130371\n",
      "Epoch: 14, batch index: 1181, learning rate: [1.000000000000001e-18], loss:2.3529653549194336\n",
      "Epoch: 14, batch index: 1182, learning rate: [1.000000000000001e-18], loss:2.3565680980682373\n",
      "Epoch: 14, batch index: 1183, learning rate: [1.000000000000001e-18], loss:2.3034589290618896\n",
      "Epoch: 14, batch index: 1184, learning rate: [1.000000000000001e-18], loss:2.147876024246216\n",
      "Calculating validation accuracy\n",
      "Epoch 14, validation accuracy score0.423046875\n",
      "Epoch: 15, batch index: 1185, learning rate: [1.000000000000001e-18], loss:2.316915273666382\n",
      "Epoch: 15, batch index: 1186, learning rate: [1.000000000000001e-18], loss:2.307495594024658\n",
      "Epoch: 15, batch index: 1187, learning rate: [1.000000000000001e-18], loss:2.3185484409332275\n",
      "Epoch: 15, batch index: 1188, learning rate: [1.000000000000001e-18], loss:2.2309317588806152\n",
      "Epoch: 15, batch index: 1189, learning rate: [1.000000000000001e-18], loss:2.3519718647003174\n",
      "Epoch: 15, batch index: 1190, learning rate: [1.000000000000001e-18], loss:2.2891533374786377\n",
      "Epoch: 15, batch index: 1191, learning rate: [1.000000000000001e-18], loss:2.318863868713379\n",
      "Epoch: 15, batch index: 1192, learning rate: [1.000000000000001e-18], loss:2.260633707046509\n",
      "Epoch: 15, batch index: 1193, learning rate: [1.000000000000001e-18], loss:2.285701036453247\n",
      "Epoch: 15, batch index: 1194, learning rate: [1.000000000000001e-18], loss:2.2695839405059814\n",
      "Epoch: 15, batch index: 1195, learning rate: [1.000000000000001e-18], loss:2.2985079288482666\n",
      "Epoch: 15, batch index: 1196, learning rate: [1.000000000000001e-18], loss:2.254910469055176\n",
      "Epoch: 15, batch index: 1197, learning rate: [1.000000000000001e-18], loss:2.24069881439209\n",
      "Epoch: 15, batch index: 1198, learning rate: [1.000000000000001e-18], loss:2.1948719024658203\n",
      "Epoch: 15, batch index: 1199, learning rate: [1.000000000000001e-18], loss:2.2668941020965576\n",
      "Epoch: 15, batch index: 1200, learning rate: [1.000000000000001e-18], loss:2.30499529838562\n",
      "Epoch: 15, batch index: 1201, learning rate: [1.000000000000001e-18], loss:2.2928977012634277\n",
      "Epoch: 15, batch index: 1202, learning rate: [1.000000000000001e-18], loss:2.269155979156494\n",
      "Epoch: 15, batch index: 1203, learning rate: [1.000000000000001e-18], loss:2.274683713912964\n",
      "Epoch: 15, batch index: 1204, learning rate: [1.000000000000001e-18], loss:2.2864532470703125\n",
      "Epoch: 15, batch index: 1205, learning rate: [1.000000000000001e-18], loss:2.2351560592651367\n",
      "Epoch: 15, batch index: 1206, learning rate: [1.000000000000001e-18], loss:2.245225429534912\n",
      "Epoch: 15, batch index: 1207, learning rate: [1.000000000000001e-18], loss:2.3026692867279053\n",
      "Epoch: 15, batch index: 1208, learning rate: [1.000000000000001e-18], loss:2.2886898517608643\n",
      "Epoch: 15, batch index: 1209, learning rate: [1.000000000000001e-18], loss:2.2941219806671143\n",
      "Epoch: 15, batch index: 1210, learning rate: [1.000000000000001e-18], loss:2.1880295276641846\n",
      "Epoch: 15, batch index: 1211, learning rate: [1.000000000000001e-18], loss:2.2514376640319824\n",
      "Epoch: 15, batch index: 1212, learning rate: [1.000000000000001e-18], loss:2.196053981781006\n",
      "Epoch: 15, batch index: 1213, learning rate: [1.000000000000001e-18], loss:2.279543161392212\n",
      "Epoch: 15, batch index: 1214, learning rate: [1.000000000000001e-18], loss:2.3290038108825684\n",
      "Epoch: 15, batch index: 1215, learning rate: [1.000000000000001e-18], loss:2.3188633918762207\n",
      "Epoch: 15, batch index: 1216, learning rate: [1.000000000000001e-19], loss:2.331514835357666\n",
      "Epoch: 15, batch index: 1217, learning rate: [1.000000000000001e-19], loss:2.3554935455322266\n",
      "Epoch: 15, batch index: 1218, learning rate: [1.000000000000001e-19], loss:2.3093152046203613\n",
      "Epoch: 15, batch index: 1219, learning rate: [1.000000000000001e-19], loss:2.326545000076294\n",
      "Epoch: 15, batch index: 1220, learning rate: [1.000000000000001e-19], loss:2.3121516704559326\n",
      "Epoch: 15, batch index: 1221, learning rate: [1.000000000000001e-19], loss:2.251209020614624\n",
      "Epoch: 15, batch index: 1222, learning rate: [1.000000000000001e-19], loss:2.2394471168518066\n",
      "Epoch: 15, batch index: 1223, learning rate: [1.000000000000001e-19], loss:2.329416513442993\n",
      "Epoch: 15, batch index: 1224, learning rate: [1.000000000000001e-19], loss:2.3015811443328857\n",
      "Epoch: 15, batch index: 1225, learning rate: [1.000000000000001e-19], loss:2.2873713970184326\n",
      "Epoch: 15, batch index: 1226, learning rate: [1.000000000000001e-19], loss:2.351982593536377\n",
      "Epoch: 15, batch index: 1227, learning rate: [1.000000000000001e-19], loss:2.2697854042053223\n",
      "Epoch: 15, batch index: 1228, learning rate: [1.000000000000001e-19], loss:2.345224618911743\n",
      "Epoch: 15, batch index: 1229, learning rate: [1.000000000000001e-19], loss:2.3027703762054443\n",
      "Epoch: 15, batch index: 1230, learning rate: [1.000000000000001e-19], loss:2.2959234714508057\n",
      "Epoch: 15, batch index: 1231, learning rate: [1.000000000000001e-19], loss:2.302205801010132\n",
      "Epoch: 15, batch index: 1232, learning rate: [1.000000000000001e-19], loss:2.3731589317321777\n",
      "Epoch: 15, batch index: 1233, learning rate: [1.000000000000001e-19], loss:2.3226962089538574\n",
      "Epoch: 15, batch index: 1234, learning rate: [1.000000000000001e-19], loss:2.272303581237793\n",
      "Epoch: 15, batch index: 1235, learning rate: [1.000000000000001e-19], loss:2.3150832653045654\n",
      "Epoch: 15, batch index: 1236, learning rate: [1.000000000000001e-19], loss:2.2343602180480957\n",
      "Epoch: 15, batch index: 1237, learning rate: [1.000000000000001e-19], loss:2.3305647373199463\n",
      "Epoch: 15, batch index: 1238, learning rate: [1.000000000000001e-19], loss:2.3486030101776123\n",
      "Epoch: 15, batch index: 1239, learning rate: [1.000000000000001e-19], loss:2.271656036376953\n",
      "Epoch: 15, batch index: 1240, learning rate: [1.000000000000001e-19], loss:2.30836820602417\n",
      "Epoch: 15, batch index: 1241, learning rate: [1.000000000000001e-19], loss:2.2377238273620605\n",
      "Epoch: 15, batch index: 1242, learning rate: [1.000000000000001e-19], loss:2.2735886573791504\n",
      "Epoch: 15, batch index: 1243, learning rate: [1.000000000000001e-19], loss:2.2419822216033936\n",
      "Epoch: 15, batch index: 1244, learning rate: [1.000000000000001e-19], loss:2.300231695175171\n",
      "Epoch: 15, batch index: 1245, learning rate: [1.000000000000001e-19], loss:2.2295968532562256\n",
      "Epoch: 15, batch index: 1246, learning rate: [1.000000000000001e-19], loss:2.293079137802124\n",
      "Epoch: 15, batch index: 1247, learning rate: [1.000000000000001e-19], loss:2.317251682281494\n",
      "Epoch: 15, batch index: 1248, learning rate: [1.000000000000001e-19], loss:2.282783031463623\n",
      "Epoch: 15, batch index: 1249, learning rate: [1.000000000000001e-19], loss:2.3324217796325684\n",
      "Epoch: 15, batch index: 1250, learning rate: [1.000000000000001e-19], loss:2.244450807571411\n",
      "Epoch: 15, batch index: 1251, learning rate: [1.000000000000001e-19], loss:2.2869558334350586\n",
      "Epoch: 15, batch index: 1252, learning rate: [1.000000000000001e-19], loss:2.2953548431396484\n",
      "Epoch: 15, batch index: 1253, learning rate: [1.000000000000001e-19], loss:2.214167833328247\n",
      "Epoch: 15, batch index: 1254, learning rate: [1.000000000000001e-19], loss:2.222733974456787\n",
      "Epoch: 15, batch index: 1255, learning rate: [1.000000000000001e-19], loss:2.2520253658294678\n",
      "Epoch: 15, batch index: 1256, learning rate: [1.000000000000001e-19], loss:2.29910945892334\n",
      "Epoch: 15, batch index: 1257, learning rate: [1.000000000000001e-19], loss:2.337733507156372\n",
      "Epoch: 15, batch index: 1258, learning rate: [1.000000000000001e-19], loss:2.253162145614624\n",
      "Epoch: 15, batch index: 1259, learning rate: [1.000000000000001e-19], loss:2.279240846633911\n",
      "Epoch: 15, batch index: 1260, learning rate: [1.000000000000001e-19], loss:2.3003973960876465\n",
      "Epoch: 15, batch index: 1261, learning rate: [1.000000000000001e-19], loss:2.275075912475586\n",
      "Epoch: 15, batch index: 1262, learning rate: [1.000000000000001e-19], loss:2.237029790878296\n",
      "Epoch: 15, batch index: 1263, learning rate: [1.000000000000001e-19], loss:2.2786202430725098\n",
      "Calculating validation accuracy\n",
      "Epoch 15, validation accuracy score0.42734375\n",
      "Epoch: 16, batch index: 1264, learning rate: [1.000000000000001e-19], loss:2.2926032543182373\n",
      "Epoch: 16, batch index: 1265, learning rate: [1.000000000000001e-19], loss:2.2344000339508057\n",
      "Epoch: 16, batch index: 1266, learning rate: [1.000000000000001e-19], loss:2.3150641918182373\n",
      "Epoch: 16, batch index: 1267, learning rate: [1.000000000000001e-19], loss:2.286642074584961\n",
      "Epoch: 16, batch index: 1268, learning rate: [1.000000000000001e-19], loss:2.374833345413208\n",
      "Epoch: 16, batch index: 1269, learning rate: [1.000000000000001e-19], loss:2.2688379287719727\n",
      "Epoch: 16, batch index: 1270, learning rate: [1.000000000000001e-19], loss:2.3414766788482666\n",
      "Epoch: 16, batch index: 1271, learning rate: [1.000000000000001e-19], loss:2.3523478507995605\n",
      "Epoch: 16, batch index: 1272, learning rate: [1.000000000000001e-19], loss:2.3204286098480225\n",
      "Epoch: 16, batch index: 1273, learning rate: [1.000000000000001e-19], loss:2.37162446975708\n",
      "Epoch: 16, batch index: 1274, learning rate: [1.0000000000000011e-20], loss:2.2589893341064453\n",
      "Epoch: 16, batch index: 1275, learning rate: [1.0000000000000011e-20], loss:2.2846555709838867\n",
      "Epoch: 16, batch index: 1276, learning rate: [1.0000000000000011e-20], loss:2.265735149383545\n",
      "Epoch: 16, batch index: 1277, learning rate: [1.0000000000000011e-20], loss:2.240056276321411\n",
      "Epoch: 16, batch index: 1278, learning rate: [1.0000000000000011e-20], loss:2.27036714553833\n",
      "Epoch: 16, batch index: 1279, learning rate: [1.0000000000000011e-20], loss:2.2990846633911133\n",
      "Epoch: 16, batch index: 1280, learning rate: [1.0000000000000011e-20], loss:2.3162295818328857\n",
      "Epoch: 16, batch index: 1281, learning rate: [1.0000000000000011e-20], loss:2.205374002456665\n",
      "Epoch: 16, batch index: 1282, learning rate: [1.0000000000000011e-20], loss:2.217904567718506\n",
      "Epoch: 16, batch index: 1283, learning rate: [1.0000000000000011e-20], loss:2.2763397693634033\n",
      "Epoch: 16, batch index: 1284, learning rate: [1.0000000000000011e-20], loss:2.2918381690979004\n",
      "Epoch: 16, batch index: 1285, learning rate: [1.0000000000000011e-20], loss:2.3015382289886475\n",
      "Epoch: 16, batch index: 1286, learning rate: [1.0000000000000011e-20], loss:2.3187029361724854\n",
      "Epoch: 16, batch index: 1287, learning rate: [1.0000000000000011e-20], loss:2.2839512825012207\n",
      "Epoch: 16, batch index: 1288, learning rate: [1.0000000000000011e-20], loss:2.316365957260132\n",
      "Epoch: 16, batch index: 1289, learning rate: [1.0000000000000011e-20], loss:2.266403913497925\n",
      "Epoch: 16, batch index: 1290, learning rate: [1.0000000000000011e-20], loss:2.2918477058410645\n",
      "Epoch: 16, batch index: 1291, learning rate: [1.0000000000000011e-20], loss:2.211803436279297\n",
      "Epoch: 16, batch index: 1292, learning rate: [1.0000000000000011e-20], loss:2.258052349090576\n",
      "Epoch: 16, batch index: 1293, learning rate: [1.0000000000000011e-20], loss:2.3179306983947754\n",
      "Epoch: 16, batch index: 1294, learning rate: [1.0000000000000011e-20], loss:2.2635438442230225\n",
      "Epoch: 16, batch index: 1295, learning rate: [1.0000000000000011e-20], loss:2.267970085144043\n",
      "Epoch: 16, batch index: 1296, learning rate: [1.0000000000000011e-20], loss:2.361727237701416\n",
      "Epoch: 16, batch index: 1297, learning rate: [1.0000000000000011e-20], loss:2.3101022243499756\n",
      "Epoch: 16, batch index: 1298, learning rate: [1.0000000000000011e-20], loss:2.291288375854492\n",
      "Epoch: 16, batch index: 1299, learning rate: [1.0000000000000011e-20], loss:2.216787815093994\n",
      "Epoch: 16, batch index: 1300, learning rate: [1.0000000000000011e-20], loss:2.2343204021453857\n",
      "Epoch: 16, batch index: 1301, learning rate: [1.0000000000000011e-20], loss:2.299689769744873\n",
      "Epoch: 16, batch index: 1302, learning rate: [1.0000000000000011e-20], loss:2.3415443897247314\n",
      "Epoch: 16, batch index: 1303, learning rate: [1.0000000000000011e-20], loss:2.279848098754883\n",
      "Epoch: 16, batch index: 1304, learning rate: [1.0000000000000011e-20], loss:2.2811689376831055\n",
      "Epoch: 16, batch index: 1305, learning rate: [1.0000000000000011e-20], loss:2.2149453163146973\n",
      "Epoch: 16, batch index: 1306, learning rate: [1.0000000000000011e-20], loss:2.2571096420288086\n",
      "Epoch: 16, batch index: 1307, learning rate: [1.0000000000000011e-20], loss:2.328604221343994\n",
      "Epoch: 16, batch index: 1308, learning rate: [1.0000000000000011e-20], loss:2.2619102001190186\n",
      "Epoch: 16, batch index: 1309, learning rate: [1.0000000000000011e-20], loss:2.2766497135162354\n",
      "Epoch: 16, batch index: 1310, learning rate: [1.0000000000000011e-20], loss:2.27036190032959\n",
      "Epoch: 16, batch index: 1311, learning rate: [1.0000000000000011e-20], loss:2.322118043899536\n",
      "Epoch: 16, batch index: 1312, learning rate: [1.0000000000000011e-20], loss:2.342792272567749\n",
      "Epoch: 16, batch index: 1313, learning rate: [1.0000000000000011e-20], loss:2.359633445739746\n",
      "Epoch: 16, batch index: 1314, learning rate: [1.0000000000000011e-20], loss:2.3402607440948486\n",
      "Epoch: 16, batch index: 1315, learning rate: [1.0000000000000011e-20], loss:2.2553141117095947\n",
      "Epoch: 16, batch index: 1316, learning rate: [1.0000000000000011e-20], loss:2.3166990280151367\n",
      "Epoch: 16, batch index: 1317, learning rate: [1.0000000000000011e-20], loss:2.34946346282959\n",
      "Epoch: 16, batch index: 1318, learning rate: [1.0000000000000011e-20], loss:2.294962167739868\n",
      "Epoch: 16, batch index: 1319, learning rate: [1.0000000000000011e-20], loss:2.284369468688965\n",
      "Epoch: 16, batch index: 1320, learning rate: [1.0000000000000011e-20], loss:2.325767755508423\n",
      "Epoch: 16, batch index: 1321, learning rate: [1.0000000000000011e-20], loss:2.2112529277801514\n",
      "Epoch: 16, batch index: 1322, learning rate: [1.0000000000000011e-20], loss:2.280094623565674\n",
      "Epoch: 16, batch index: 1323, learning rate: [1.0000000000000011e-20], loss:2.2906494140625\n",
      "Epoch: 16, batch index: 1324, learning rate: [1.0000000000000011e-20], loss:2.297801971435547\n",
      "Epoch: 16, batch index: 1325, learning rate: [1.0000000000000011e-20], loss:2.200434684753418\n",
      "Epoch: 16, batch index: 1326, learning rate: [1.0000000000000011e-20], loss:2.2904434204101562\n",
      "Epoch: 16, batch index: 1327, learning rate: [1.0000000000000011e-20], loss:2.206228256225586\n",
      "Epoch: 16, batch index: 1328, learning rate: [1.0000000000000011e-20], loss:2.240450143814087\n",
      "Epoch: 16, batch index: 1329, learning rate: [1.0000000000000011e-20], loss:2.3041999340057373\n",
      "Epoch: 16, batch index: 1330, learning rate: [1.0000000000000011e-20], loss:2.3045711517333984\n",
      "Epoch: 16, batch index: 1331, learning rate: [1.0000000000000011e-20], loss:2.3098950386047363\n",
      "Epoch: 16, batch index: 1332, learning rate: [1.0000000000000011e-20], loss:2.2651522159576416\n",
      "Epoch: 16, batch index: 1333, learning rate: [1.0000000000000011e-20], loss:2.2828328609466553\n",
      "Epoch: 16, batch index: 1334, learning rate: [1.0000000000000011e-20], loss:2.2901761531829834\n",
      "Epoch: 16, batch index: 1335, learning rate: [1.0000000000000011e-20], loss:2.263597249984741\n",
      "Epoch: 16, batch index: 1336, learning rate: [1.0000000000000011e-20], loss:2.220551013946533\n",
      "Epoch: 16, batch index: 1337, learning rate: [1.0000000000000011e-20], loss:2.23643159866333\n",
      "Epoch: 16, batch index: 1338, learning rate: [1.0000000000000011e-20], loss:2.2367801666259766\n",
      "Epoch: 16, batch index: 1339, learning rate: [1.0000000000000011e-20], loss:2.3037185668945312\n",
      "Epoch: 16, batch index: 1340, learning rate: [1.0000000000000011e-20], loss:2.2251672744750977\n",
      "Epoch: 16, batch index: 1341, learning rate: [1.0000000000000011e-20], loss:2.2892441749572754\n",
      "Epoch: 16, batch index: 1342, learning rate: [1.0000000000000011e-20], loss:2.062497615814209\n",
      "Calculating validation accuracy\n",
      "Epoch 16, validation accuracy score0.424609375\n",
      "Epoch: 17, batch index: 1343, learning rate: [1.0000000000000011e-20], loss:2.3391318321228027\n",
      "Epoch: 17, batch index: 1344, learning rate: [1.0000000000000011e-20], loss:2.2980856895446777\n",
      "Epoch: 17, batch index: 1345, learning rate: [1.0000000000000011e-20], loss:2.299178123474121\n",
      "Epoch: 17, batch index: 1346, learning rate: [1.0000000000000011e-20], loss:2.2546136379241943\n",
      "Epoch: 17, batch index: 1347, learning rate: [1.0000000000000011e-20], loss:2.2734479904174805\n",
      "Epoch: 17, batch index: 1348, learning rate: [1.0000000000000011e-20], loss:2.3149590492248535\n",
      "Epoch: 17, batch index: 1349, learning rate: [1.0000000000000011e-20], loss:2.3161778450012207\n",
      "Epoch: 17, batch index: 1350, learning rate: [1.0000000000000011e-20], loss:2.2309041023254395\n",
      "Epoch: 17, batch index: 1351, learning rate: [1.0000000000000011e-20], loss:2.3161566257476807\n",
      "Epoch: 17, batch index: 1352, learning rate: [1.0000000000000011e-20], loss:2.1911027431488037\n",
      "Epoch: 17, batch index: 1353, learning rate: [1.0000000000000011e-20], loss:2.1997852325439453\n",
      "Epoch: 17, batch index: 1354, learning rate: [1.0000000000000011e-20], loss:2.3202688694000244\n",
      "Epoch: 17, batch index: 1355, learning rate: [1.0000000000000011e-20], loss:2.3150956630706787\n",
      "Epoch: 17, batch index: 1356, learning rate: [1.0000000000000011e-20], loss:2.2970170974731445\n",
      "Epoch: 17, batch index: 1357, learning rate: [1.0000000000000011e-20], loss:2.3025765419006348\n",
      "Epoch: 17, batch index: 1358, learning rate: [1.0000000000000011e-20], loss:2.315579414367676\n",
      "Epoch: 17, batch index: 1359, learning rate: [1.0000000000000011e-20], loss:2.278244733810425\n",
      "Epoch: 17, batch index: 1360, learning rate: [1.0000000000000011e-20], loss:2.325176239013672\n",
      "Epoch: 17, batch index: 1361, learning rate: [1.0000000000000011e-20], loss:2.307028293609619\n",
      "Epoch: 17, batch index: 1362, learning rate: [1.0000000000000011e-20], loss:2.246250629425049\n",
      "Epoch: 17, batch index: 1363, learning rate: [1.0000000000000011e-20], loss:2.2344682216644287\n",
      "Epoch: 17, batch index: 1364, learning rate: [1.0000000000000011e-20], loss:2.3002419471740723\n",
      "Epoch: 17, batch index: 1365, learning rate: [1.0000000000000011e-20], loss:2.303192615509033\n",
      "Epoch: 17, batch index: 1366, learning rate: [1.0000000000000011e-20], loss:2.2555880546569824\n",
      "Epoch: 17, batch index: 1367, learning rate: [1.0000000000000011e-20], loss:2.301380157470703\n",
      "Epoch: 17, batch index: 1368, learning rate: [1.0000000000000011e-20], loss:2.2778661251068115\n",
      "Epoch: 17, batch index: 1369, learning rate: [1.0000000000000011e-20], loss:2.2821402549743652\n",
      "Epoch: 17, batch index: 1370, learning rate: [1.0000000000000011e-20], loss:2.3151912689208984\n",
      "Epoch: 17, batch index: 1371, learning rate: [1.0000000000000011e-20], loss:2.3032474517822266\n",
      "Epoch: 17, batch index: 1372, learning rate: [1.0000000000000011e-20], loss:2.2514524459838867\n",
      "Epoch: 17, batch index: 1373, learning rate: [1.0000000000000011e-20], loss:2.2552411556243896\n",
      "Epoch: 17, batch index: 1374, learning rate: [1.0000000000000011e-20], loss:2.2935352325439453\n",
      "Epoch: 17, batch index: 1375, learning rate: [1.0000000000000011e-20], loss:2.308626890182495\n",
      "Epoch: 17, batch index: 1376, learning rate: [1.0000000000000011e-20], loss:2.35038423538208\n",
      "Epoch: 17, batch index: 1377, learning rate: [1.0000000000000011e-20], loss:2.3049867153167725\n",
      "Epoch: 17, batch index: 1378, learning rate: [1.0000000000000011e-20], loss:2.236643075942993\n",
      "Epoch: 17, batch index: 1379, learning rate: [1.0000000000000011e-20], loss:2.277658700942993\n",
      "Epoch: 17, batch index: 1380, learning rate: [1.0000000000000011e-20], loss:2.266019344329834\n",
      "Epoch: 17, batch index: 1381, learning rate: [1.0000000000000011e-20], loss:2.3068530559539795\n",
      "Epoch: 17, batch index: 1382, learning rate: [1.0000000000000011e-20], loss:2.3356494903564453\n",
      "Epoch: 17, batch index: 1383, learning rate: [1.0000000000000011e-20], loss:2.27644944190979\n",
      "Epoch: 17, batch index: 1384, learning rate: [1.0000000000000011e-20], loss:2.3011491298675537\n",
      "Epoch: 17, batch index: 1385, learning rate: [1.0000000000000011e-20], loss:2.3348984718322754\n",
      "Epoch: 17, batch index: 1386, learning rate: [1.0000000000000011e-20], loss:2.271275520324707\n",
      "Epoch: 17, batch index: 1387, learning rate: [1.0000000000000011e-20], loss:2.3448421955108643\n",
      "Epoch: 17, batch index: 1388, learning rate: [1.0000000000000011e-20], loss:2.223994731903076\n",
      "Epoch: 17, batch index: 1389, learning rate: [1.0000000000000011e-20], loss:2.3301947116851807\n",
      "Epoch: 17, batch index: 1390, learning rate: [1.0000000000000011e-20], loss:2.3000683784484863\n",
      "Epoch: 17, batch index: 1391, learning rate: [1.0000000000000011e-20], loss:2.2917368412017822\n",
      "Epoch: 17, batch index: 1392, learning rate: [1.0000000000000011e-20], loss:2.267970561981201\n",
      "Epoch: 17, batch index: 1393, learning rate: [1.0000000000000011e-20], loss:2.2497127056121826\n",
      "Epoch: 17, batch index: 1394, learning rate: [1.0000000000000011e-20], loss:2.2652699947357178\n",
      "Epoch: 17, batch index: 1395, learning rate: [1.0000000000000011e-20], loss:2.2159085273742676\n",
      "Epoch: 17, batch index: 1396, learning rate: [1.0000000000000011e-20], loss:2.193687677383423\n",
      "Epoch: 17, batch index: 1397, learning rate: [1.0000000000000011e-20], loss:2.3026957511901855\n",
      "Epoch: 17, batch index: 1398, learning rate: [1.0000000000000011e-20], loss:2.3259081840515137\n",
      "Epoch: 17, batch index: 1399, learning rate: [1.0000000000000011e-20], loss:2.298197031021118\n",
      "Epoch: 17, batch index: 1400, learning rate: [1.0000000000000011e-20], loss:2.230753183364868\n",
      "Epoch: 17, batch index: 1401, learning rate: [1.0000000000000011e-20], loss:2.283921718597412\n",
      "Epoch: 17, batch index: 1402, learning rate: [1.0000000000000011e-20], loss:2.2354302406311035\n",
      "Epoch: 17, batch index: 1403, learning rate: [1.0000000000000011e-20], loss:2.250357151031494\n",
      "Epoch: 17, batch index: 1404, learning rate: [1.0000000000000011e-20], loss:2.252332925796509\n",
      "Epoch: 17, batch index: 1405, learning rate: [1.0000000000000011e-20], loss:2.2642667293548584\n",
      "Epoch: 17, batch index: 1406, learning rate: [1.0000000000000011e-20], loss:2.272404432296753\n",
      "Epoch: 17, batch index: 1407, learning rate: [1.0000000000000011e-20], loss:2.261648416519165\n",
      "Epoch: 17, batch index: 1408, learning rate: [1.0000000000000011e-20], loss:2.2952663898468018\n",
      "Epoch: 17, batch index: 1409, learning rate: [1.0000000000000011e-20], loss:2.4255599975585938\n",
      "Epoch: 17, batch index: 1410, learning rate: [1.0000000000000011e-20], loss:2.284315347671509\n",
      "Epoch: 17, batch index: 1411, learning rate: [1.0000000000000011e-20], loss:2.343076467514038\n",
      "Epoch: 17, batch index: 1412, learning rate: [1.0000000000000011e-20], loss:2.235250949859619\n",
      "Epoch: 17, batch index: 1413, learning rate: [1.0000000000000011e-20], loss:2.37743878364563\n",
      "Epoch: 17, batch index: 1414, learning rate: [1.0000000000000011e-20], loss:2.261843204498291\n",
      "Epoch: 17, batch index: 1415, learning rate: [1.0000000000000011e-20], loss:2.304048538208008\n",
      "Epoch: 17, batch index: 1416, learning rate: [1.0000000000000011e-20], loss:2.303541660308838\n",
      "Epoch: 17, batch index: 1417, learning rate: [1.0000000000000011e-20], loss:2.2700836658477783\n",
      "Epoch: 17, batch index: 1418, learning rate: [1.0000000000000011e-20], loss:2.3248322010040283\n",
      "Epoch: 17, batch index: 1419, learning rate: [1.0000000000000011e-20], loss:2.2035744190216064\n",
      "Epoch: 17, batch index: 1420, learning rate: [1.0000000000000011e-20], loss:2.2526307106018066\n",
      "Epoch: 17, batch index: 1421, learning rate: [1.0000000000000011e-20], loss:2.457296848297119\n",
      "Calculating validation accuracy\n",
      "Epoch 17, validation accuracy score0.422265625\n",
      "Epoch: 18, batch index: 1422, learning rate: [1.0000000000000011e-20], loss:2.2940192222595215\n",
      "Epoch: 18, batch index: 1423, learning rate: [1.0000000000000011e-20], loss:2.2845065593719482\n",
      "Epoch: 18, batch index: 1424, learning rate: [1.0000000000000011e-20], loss:2.316087484359741\n",
      "Epoch: 18, batch index: 1425, learning rate: [1.0000000000000011e-20], loss:2.2829296588897705\n",
      "Epoch: 18, batch index: 1426, learning rate: [1.0000000000000011e-20], loss:2.2041475772857666\n",
      "Epoch: 18, batch index: 1427, learning rate: [1.0000000000000011e-20], loss:2.3287432193756104\n",
      "Epoch: 18, batch index: 1428, learning rate: [1.0000000000000011e-20], loss:2.222503185272217\n",
      "Epoch: 18, batch index: 1429, learning rate: [1.0000000000000011e-20], loss:2.2886288166046143\n",
      "Epoch: 18, batch index: 1430, learning rate: [1.0000000000000011e-20], loss:2.327669382095337\n",
      "Epoch: 18, batch index: 1431, learning rate: [1.0000000000000011e-20], loss:2.3435754776000977\n",
      "Epoch: 18, batch index: 1432, learning rate: [1.0000000000000011e-20], loss:2.2902257442474365\n",
      "Epoch: 18, batch index: 1433, learning rate: [1.0000000000000011e-20], loss:2.2963521480560303\n",
      "Epoch: 18, batch index: 1434, learning rate: [1.0000000000000011e-20], loss:2.3368804454803467\n",
      "Epoch: 18, batch index: 1435, learning rate: [1.0000000000000011e-20], loss:2.2997477054595947\n",
      "Epoch: 18, batch index: 1436, learning rate: [1.0000000000000011e-20], loss:2.2429842948913574\n",
      "Epoch: 18, batch index: 1437, learning rate: [1.0000000000000011e-20], loss:2.260270833969116\n",
      "Epoch: 18, batch index: 1438, learning rate: [1.0000000000000011e-20], loss:2.247997283935547\n",
      "Epoch: 18, batch index: 1439, learning rate: [1.0000000000000011e-20], loss:2.2899389266967773\n",
      "Epoch: 18, batch index: 1440, learning rate: [1.0000000000000011e-20], loss:2.271965980529785\n",
      "Epoch: 18, batch index: 1441, learning rate: [1.0000000000000011e-20], loss:2.2930071353912354\n",
      "Epoch: 18, batch index: 1442, learning rate: [1.0000000000000011e-20], loss:2.350574493408203\n",
      "Epoch: 18, batch index: 1443, learning rate: [1.0000000000000011e-20], loss:2.2181687355041504\n",
      "Epoch: 18, batch index: 1444, learning rate: [1.0000000000000011e-20], loss:2.2811460494995117\n",
      "Epoch: 18, batch index: 1445, learning rate: [1.0000000000000011e-20], loss:2.2973806858062744\n",
      "Epoch: 18, batch index: 1446, learning rate: [1.0000000000000011e-20], loss:2.2734742164611816\n",
      "Epoch: 18, batch index: 1447, learning rate: [1.0000000000000011e-20], loss:2.247077703475952\n",
      "Epoch: 18, batch index: 1448, learning rate: [1.0000000000000011e-20], loss:2.286672353744507\n",
      "Epoch: 18, batch index: 1449, learning rate: [1.0000000000000011e-20], loss:2.2854480743408203\n",
      "Epoch: 18, batch index: 1450, learning rate: [1.0000000000000011e-20], loss:2.248148202896118\n",
      "Epoch: 18, batch index: 1451, learning rate: [1.0000000000000011e-20], loss:2.2317135334014893\n",
      "Epoch: 18, batch index: 1452, learning rate: [1.0000000000000011e-20], loss:2.315023183822632\n",
      "Epoch: 18, batch index: 1453, learning rate: [1.0000000000000011e-20], loss:2.3163363933563232\n",
      "Epoch: 18, batch index: 1454, learning rate: [1.0000000000000011e-20], loss:2.2689671516418457\n",
      "Epoch: 18, batch index: 1455, learning rate: [1.0000000000000011e-20], loss:2.3007500171661377\n",
      "Epoch: 18, batch index: 1456, learning rate: [1.0000000000000011e-20], loss:2.2543013095855713\n",
      "Epoch: 18, batch index: 1457, learning rate: [1.0000000000000011e-20], loss:2.2666754722595215\n",
      "Epoch: 18, batch index: 1458, learning rate: [1.0000000000000011e-20], loss:2.2876718044281006\n",
      "Epoch: 18, batch index: 1459, learning rate: [1.0000000000000011e-20], loss:2.3338396549224854\n",
      "Epoch: 18, batch index: 1460, learning rate: [1.0000000000000011e-20], loss:2.2864620685577393\n",
      "Epoch: 18, batch index: 1461, learning rate: [1.0000000000000011e-20], loss:2.281360626220703\n",
      "Epoch: 18, batch index: 1462, learning rate: [1.0000000000000011e-20], loss:2.27370023727417\n",
      "Epoch: 18, batch index: 1463, learning rate: [1.0000000000000011e-20], loss:2.231806755065918\n",
      "Epoch: 18, batch index: 1464, learning rate: [1.0000000000000011e-20], loss:2.2542107105255127\n",
      "Epoch: 18, batch index: 1465, learning rate: [1.0000000000000011e-20], loss:2.3212873935699463\n",
      "Epoch: 18, batch index: 1466, learning rate: [1.0000000000000011e-20], loss:2.249285936355591\n",
      "Epoch: 18, batch index: 1467, learning rate: [1.0000000000000011e-20], loss:2.309488534927368\n",
      "Epoch: 18, batch index: 1468, learning rate: [1.0000000000000011e-20], loss:2.207106113433838\n",
      "Epoch: 18, batch index: 1469, learning rate: [1.0000000000000011e-20], loss:2.275327444076538\n",
      "Epoch: 18, batch index: 1470, learning rate: [1.0000000000000011e-20], loss:2.309661388397217\n",
      "Epoch: 18, batch index: 1471, learning rate: [1.0000000000000011e-20], loss:2.3150503635406494\n",
      "Epoch: 18, batch index: 1472, learning rate: [1.0000000000000011e-20], loss:2.2307260036468506\n",
      "Epoch: 18, batch index: 1473, learning rate: [1.0000000000000011e-20], loss:2.244744300842285\n",
      "Epoch: 18, batch index: 1474, learning rate: [1.0000000000000011e-20], loss:2.337073564529419\n",
      "Epoch: 18, batch index: 1475, learning rate: [1.0000000000000011e-20], loss:2.235975980758667\n",
      "Epoch: 18, batch index: 1476, learning rate: [1.0000000000000011e-20], loss:2.260829448699951\n",
      "Epoch: 18, batch index: 1477, learning rate: [1.0000000000000011e-20], loss:2.2908871173858643\n",
      "Epoch: 18, batch index: 1478, learning rate: [1.0000000000000011e-20], loss:2.255781412124634\n",
      "Epoch: 18, batch index: 1479, learning rate: [1.0000000000000011e-20], loss:2.316329002380371\n",
      "Epoch: 18, batch index: 1480, learning rate: [1.0000000000000011e-20], loss:2.288466215133667\n",
      "Epoch: 18, batch index: 1481, learning rate: [1.0000000000000011e-20], loss:2.313943386077881\n",
      "Epoch: 18, batch index: 1482, learning rate: [1.0000000000000011e-20], loss:2.2910003662109375\n",
      "Epoch: 18, batch index: 1483, learning rate: [1.0000000000000011e-20], loss:2.271827220916748\n",
      "Epoch: 18, batch index: 1484, learning rate: [1.0000000000000011e-20], loss:2.269209384918213\n",
      "Epoch: 18, batch index: 1485, learning rate: [1.0000000000000011e-20], loss:2.183253288269043\n",
      "Epoch: 18, batch index: 1486, learning rate: [1.0000000000000011e-20], loss:2.2856249809265137\n",
      "Epoch: 18, batch index: 1487, learning rate: [1.0000000000000011e-20], loss:2.2974767684936523\n",
      "Epoch: 18, batch index: 1488, learning rate: [1.0000000000000011e-20], loss:2.2769081592559814\n",
      "Epoch: 18, batch index: 1489, learning rate: [1.0000000000000011e-20], loss:2.3477494716644287\n",
      "Epoch: 18, batch index: 1490, learning rate: [1.0000000000000011e-20], loss:2.2842884063720703\n",
      "Epoch: 18, batch index: 1491, learning rate: [1.0000000000000011e-20], loss:2.294325113296509\n",
      "Epoch: 18, batch index: 1492, learning rate: [1.0000000000000011e-20], loss:2.2556965351104736\n",
      "Epoch: 18, batch index: 1493, learning rate: [1.0000000000000011e-20], loss:2.3098762035369873\n",
      "Epoch: 18, batch index: 1494, learning rate: [1.0000000000000011e-20], loss:2.2021450996398926\n",
      "Epoch: 18, batch index: 1495, learning rate: [1.0000000000000011e-20], loss:2.2594964504241943\n",
      "Epoch: 18, batch index: 1496, learning rate: [1.0000000000000011e-20], loss:2.34928035736084\n",
      "Epoch: 18, batch index: 1497, learning rate: [1.0000000000000011e-20], loss:2.3200345039367676\n",
      "Epoch: 18, batch index: 1498, learning rate: [1.0000000000000011e-20], loss:2.2679243087768555\n",
      "Epoch: 18, batch index: 1499, learning rate: [1.0000000000000011e-20], loss:2.3446907997131348\n",
      "Epoch: 18, batch index: 1500, learning rate: [1.0000000000000011e-20], loss:2.4615299701690674\n",
      "Calculating validation accuracy\n",
      "Epoch 18, validation accuracy score0.42578125\n",
      "Epoch: 19, batch index: 1501, learning rate: [1.0000000000000011e-20], loss:2.336002826690674\n",
      "Epoch: 19, batch index: 1502, learning rate: [1.0000000000000011e-20], loss:2.3242340087890625\n",
      "Epoch: 19, batch index: 1503, learning rate: [1.0000000000000011e-20], loss:2.275855779647827\n",
      "Epoch: 19, batch index: 1504, learning rate: [1.0000000000000011e-20], loss:2.347683906555176\n",
      "Epoch: 19, batch index: 1505, learning rate: [1.0000000000000011e-20], loss:2.2042551040649414\n",
      "Epoch: 19, batch index: 1506, learning rate: [1.0000000000000011e-20], loss:2.2332379817962646\n",
      "Epoch: 19, batch index: 1507, learning rate: [1.0000000000000011e-20], loss:2.320626735687256\n",
      "Epoch: 19, batch index: 1508, learning rate: [1.0000000000000011e-20], loss:2.3106980323791504\n",
      "Epoch: 19, batch index: 1509, learning rate: [1.0000000000000011e-20], loss:2.2511749267578125\n",
      "Epoch: 19, batch index: 1510, learning rate: [1.0000000000000011e-20], loss:2.3277273178100586\n",
      "Epoch: 19, batch index: 1511, learning rate: [1.0000000000000011e-20], loss:2.3139564990997314\n",
      "Epoch: 19, batch index: 1512, learning rate: [1.0000000000000011e-20], loss:2.295006275177002\n",
      "Epoch: 19, batch index: 1513, learning rate: [1.0000000000000011e-20], loss:2.3100287914276123\n",
      "Epoch: 19, batch index: 1514, learning rate: [1.0000000000000011e-20], loss:2.3439836502075195\n",
      "Epoch: 19, batch index: 1515, learning rate: [1.0000000000000011e-20], loss:2.2806296348571777\n",
      "Epoch: 19, batch index: 1516, learning rate: [1.0000000000000011e-20], loss:2.3101534843444824\n",
      "Epoch: 19, batch index: 1517, learning rate: [1.0000000000000011e-20], loss:2.2605772018432617\n",
      "Epoch: 19, batch index: 1518, learning rate: [1.0000000000000011e-20], loss:2.332660436630249\n",
      "Epoch: 19, batch index: 1519, learning rate: [1.0000000000000011e-20], loss:2.364067316055298\n",
      "Epoch: 19, batch index: 1520, learning rate: [1.0000000000000011e-20], loss:2.318065881729126\n",
      "Epoch: 19, batch index: 1521, learning rate: [1.0000000000000011e-20], loss:2.2678747177124023\n",
      "Epoch: 19, batch index: 1522, learning rate: [1.0000000000000011e-20], loss:2.295792579650879\n",
      "Epoch: 19, batch index: 1523, learning rate: [1.0000000000000011e-20], loss:2.3566980361938477\n",
      "Epoch: 19, batch index: 1524, learning rate: [1.0000000000000011e-20], loss:2.259361982345581\n",
      "Epoch: 19, batch index: 1525, learning rate: [1.0000000000000011e-20], loss:2.2119979858398438\n",
      "Epoch: 19, batch index: 1526, learning rate: [1.0000000000000011e-20], loss:2.2713892459869385\n",
      "Epoch: 19, batch index: 1527, learning rate: [1.0000000000000011e-20], loss:2.26287841796875\n",
      "Epoch: 19, batch index: 1528, learning rate: [1.0000000000000011e-20], loss:2.256197214126587\n",
      "Epoch: 19, batch index: 1529, learning rate: [1.0000000000000011e-20], loss:2.214287042617798\n",
      "Epoch: 19, batch index: 1530, learning rate: [1.0000000000000011e-20], loss:2.264883518218994\n",
      "Epoch: 19, batch index: 1531, learning rate: [1.0000000000000011e-20], loss:2.331590414047241\n",
      "Epoch: 19, batch index: 1532, learning rate: [1.0000000000000011e-20], loss:2.2370195388793945\n",
      "Epoch: 19, batch index: 1533, learning rate: [1.0000000000000011e-20], loss:2.3006467819213867\n",
      "Epoch: 19, batch index: 1534, learning rate: [1.0000000000000011e-20], loss:2.3328962326049805\n",
      "Epoch: 19, batch index: 1535, learning rate: [1.0000000000000011e-20], loss:2.2369184494018555\n",
      "Epoch: 19, batch index: 1536, learning rate: [1.0000000000000011e-20], loss:2.278679609298706\n",
      "Epoch: 19, batch index: 1537, learning rate: [1.0000000000000011e-20], loss:2.3160526752471924\n",
      "Epoch: 19, batch index: 1538, learning rate: [1.0000000000000011e-20], loss:2.305952310562134\n",
      "Epoch: 19, batch index: 1539, learning rate: [1.0000000000000011e-20], loss:2.271467924118042\n",
      "Epoch: 19, batch index: 1540, learning rate: [1.0000000000000011e-20], loss:2.2953128814697266\n",
      "Epoch: 19, batch index: 1541, learning rate: [1.0000000000000011e-20], loss:2.349019765853882\n",
      "Epoch: 19, batch index: 1542, learning rate: [1.0000000000000011e-20], loss:2.35119366645813\n",
      "Epoch: 19, batch index: 1543, learning rate: [1.0000000000000011e-20], loss:2.323370933532715\n",
      "Epoch: 19, batch index: 1544, learning rate: [1.0000000000000011e-20], loss:2.243227005004883\n",
      "Epoch: 19, batch index: 1545, learning rate: [1.0000000000000011e-20], loss:2.3128650188446045\n",
      "Epoch: 19, batch index: 1546, learning rate: [1.0000000000000011e-20], loss:2.281841516494751\n",
      "Epoch: 19, batch index: 1547, learning rate: [1.0000000000000011e-20], loss:2.2549006938934326\n",
      "Epoch: 19, batch index: 1548, learning rate: [1.0000000000000011e-20], loss:2.304619789123535\n",
      "Epoch: 19, batch index: 1549, learning rate: [1.0000000000000011e-20], loss:2.3653862476348877\n",
      "Epoch: 19, batch index: 1550, learning rate: [1.0000000000000011e-20], loss:2.269026279449463\n",
      "Epoch: 19, batch index: 1551, learning rate: [1.0000000000000011e-20], loss:2.320072889328003\n",
      "Epoch: 19, batch index: 1552, learning rate: [1.0000000000000011e-20], loss:2.188237428665161\n",
      "Epoch: 19, batch index: 1553, learning rate: [1.0000000000000011e-20], loss:2.2541091442108154\n",
      "Epoch: 19, batch index: 1554, learning rate: [1.0000000000000011e-20], loss:2.291335344314575\n",
      "Epoch: 19, batch index: 1555, learning rate: [1.0000000000000011e-20], loss:2.2642388343811035\n",
      "Epoch: 19, batch index: 1556, learning rate: [1.0000000000000011e-20], loss:2.299426317214966\n",
      "Epoch: 19, batch index: 1557, learning rate: [1.0000000000000011e-20], loss:2.283257246017456\n",
      "Epoch: 19, batch index: 1558, learning rate: [1.0000000000000011e-20], loss:2.247340440750122\n",
      "Epoch: 19, batch index: 1559, learning rate: [1.0000000000000011e-20], loss:2.2063405513763428\n",
      "Epoch: 19, batch index: 1560, learning rate: [1.0000000000000011e-20], loss:2.2144484519958496\n",
      "Epoch: 19, batch index: 1561, learning rate: [1.0000000000000011e-20], loss:2.30401873588562\n",
      "Epoch: 19, batch index: 1562, learning rate: [1.0000000000000011e-20], loss:2.2622060775756836\n",
      "Epoch: 19, batch index: 1563, learning rate: [1.0000000000000011e-20], loss:2.2265868186950684\n",
      "Epoch: 19, batch index: 1564, learning rate: [1.0000000000000011e-20], loss:2.27765154838562\n",
      "Epoch: 19, batch index: 1565, learning rate: [1.0000000000000011e-20], loss:2.2408134937286377\n",
      "Epoch: 19, batch index: 1566, learning rate: [1.0000000000000011e-20], loss:2.3355231285095215\n",
      "Epoch: 19, batch index: 1567, learning rate: [1.0000000000000011e-20], loss:2.308405637741089\n",
      "Epoch: 19, batch index: 1568, learning rate: [1.0000000000000011e-20], loss:2.2710795402526855\n",
      "Epoch: 19, batch index: 1569, learning rate: [1.0000000000000011e-20], loss:2.2629711627960205\n",
      "Epoch: 19, batch index: 1570, learning rate: [1.0000000000000011e-20], loss:2.272052764892578\n",
      "Epoch: 19, batch index: 1571, learning rate: [1.0000000000000011e-20], loss:2.2748465538024902\n",
      "Epoch: 19, batch index: 1572, learning rate: [1.0000000000000011e-20], loss:2.3149731159210205\n",
      "Epoch: 19, batch index: 1573, learning rate: [1.0000000000000011e-20], loss:2.2606043815612793\n",
      "Epoch: 19, batch index: 1574, learning rate: [1.0000000000000011e-20], loss:2.3158178329467773\n",
      "Epoch: 19, batch index: 1575, learning rate: [1.0000000000000011e-20], loss:2.267629623413086\n",
      "Epoch: 19, batch index: 1576, learning rate: [1.0000000000000011e-20], loss:2.3221065998077393\n",
      "Epoch: 19, batch index: 1577, learning rate: [1.0000000000000011e-20], loss:2.2186639308929443\n",
      "Epoch: 19, batch index: 1578, learning rate: [1.0000000000000011e-20], loss:2.374171495437622\n",
      "Epoch: 19, batch index: 1579, learning rate: [1.0000000000000011e-20], loss:2.283982515335083\n",
      "Calculating validation accuracy\n",
      "Epoch 19, validation accuracy score0.421875\n",
      "Loading best model\n",
      "The best model has validation accuracy 0.426171875\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, validation_loader, epochs=20, learning_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the image processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1414, 0.0598, 0.0514, 0.0312, 0.0159, 0.0740, 0.0484, 0.0836, 0.1105,\n",
       "         0.1903, 0.0741, 0.0781, 0.0411]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from image_processor import process_image\n",
    "\n",
    "image_tensor = process_image(\"Datasets/images/ffff23f1-59fc-47bd-b0cd-186933803287.jpg\").to(device)\n",
    "\n",
    "model(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractionClass(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X.float()\n",
    "        X = self.layers(X)\n",
    "        return X\n",
    "\n",
    "feature_extraction_model = FeatureExtractionClass()\n",
    "\n",
    "feature_extraction_model.cuda()\n",
    "\n",
    "torch.save(feature_extraction_model.state_dict(), 'model_evaluation/final_model/image_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data_loader = DataLoader(my_dataset,batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(whole_data_loader))\n",
    "\n",
    "feature, label = batch\n",
    "\n",
    "feature = feature.to(device)\n",
    "\n",
    "feature_extraction_model(feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(json_file_name:str, folder_of_images:str, df_of_keys):\n",
    "    transformer =  transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    dict_of_features = {}\n",
    "    list_of_image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "\n",
    "    for index in range(0, 100):#len(merged_df)):\n",
    "        image_path = list_of_image_paths[index]\n",
    "        with Image.open(image_path) as img:\n",
    "            features = transformer(img).unsqueeze(0)\n",
    "        features = features.to(device)\n",
    "        image_embedding = feature_extraction_model(features)\n",
    "        print(len(image_embedding))\n",
    "        image_embedding = image_embedding.tolist()\n",
    "        dict_of_features[df_of_keys['id'][index]] = image_embedding\n",
    "    \n",
    "    print(len(dict_of_features['912bb259-3ad9-457b-9db1-ce1da9016057'][0]))\n",
    "\n",
    "    with open('my_dict.json', '+w') as file:\n",
    "        json.dump(dict_of_features, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "feature_extraction('123', 'Datasets/cleaned_images_224/', merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5468047261238098,\n",
       " 0.13903814554214478,\n",
       " 0.9730392694473267,\n",
       " 0.9145287871360779,\n",
       " 0.19091081619262695,\n",
       " 0.10968810319900513,\n",
       " 0.9493284225463867,\n",
       " 0.12440842390060425,\n",
       " 0.17989367246627808,\n",
       " 0.2713906764984131,\n",
       " 0.7974596619606018,\n",
       " 0.5503571629524231,\n",
       " 0.2194211483001709,\n",
       " 0.15046101808547974,\n",
       " 0.9386085867881775,\n",
       " 0.685297429561615,\n",
       " 0.8354539275169373,\n",
       " 0.2687062621116638,\n",
       " 0.09485846757888794,\n",
       " 0.1985509991645813,\n",
       " 0.8375774025917053,\n",
       " 0.7260181903839111,\n",
       " 0.016275525093078613,\n",
       " 0.3562026619911194,\n",
       " 0.9158496260643005,\n",
       " 0.24581390619277954,\n",
       " 0.33599936962127686,\n",
       " 0.9377183318138123,\n",
       " 0.8188729882240295,\n",
       " 0.7700749635696411,\n",
       " 0.5792426466941833,\n",
       " 0.41430002450942993,\n",
       " 0.29343175888061523,\n",
       " 0.04645878076553345,\n",
       " 0.292216420173645,\n",
       " 0.6297909617424011,\n",
       " 0.3563084006309509,\n",
       " 0.9405226707458496,\n",
       " 0.26415473222732544,\n",
       " 0.8446291089057922,\n",
       " 0.4981077313423157,\n",
       " 0.6580413579940796,\n",
       " 0.3168283700942993,\n",
       " 0.42941492795944214,\n",
       " 0.7891059517860413,\n",
       " 0.1052054762840271,\n",
       " 0.6722680926322937,\n",
       " 0.15553545951843262,\n",
       " 0.32252681255340576,\n",
       " 0.21740972995758057,\n",
       " 0.701029896736145,\n",
       " 0.2735094428062439,\n",
       " 0.4273666739463806,\n",
       " 0.7489498853683472,\n",
       " 0.13057267665863037,\n",
       " 0.7408956289291382,\n",
       " 0.10468018054962158,\n",
       " 0.06295877695083618,\n",
       " 0.1753653883934021,\n",
       " 0.27292126417160034,\n",
       " 0.41651642322540283,\n",
       " 0.16155672073364258,\n",
       " 0.4817500114440918,\n",
       " 0.3451818823814392,\n",
       " 0.9938864707946777,\n",
       " 0.8353110551834106,\n",
       " 0.333220899105072,\n",
       " 0.6840872168540955,\n",
       " 0.05375891923904419,\n",
       " 0.52590411901474,\n",
       " 0.5960313677787781,\n",
       " 0.08926558494567871,\n",
       " 0.9240082502365112,\n",
       " 0.5977147221565247,\n",
       " 0.10577124357223511,\n",
       " 0.695239782333374,\n",
       " 0.06564265489578247,\n",
       " 0.4621920585632324,\n",
       " 0.6053052544593811,\n",
       " 0.33374863862991333,\n",
       " 0.7983254194259644,\n",
       " 0.39495640993118286,\n",
       " 0.18982750177383423,\n",
       " 0.6869884133338928,\n",
       " 0.8609980940818787,\n",
       " 0.16605591773986816,\n",
       " 0.4665268063545227,\n",
       " 0.09876799583435059,\n",
       " 0.0818360447883606,\n",
       " 0.8166172504425049,\n",
       " 0.49180716276168823,\n",
       " 0.5402182936668396,\n",
       " 0.9666215181350708,\n",
       " 0.016745388507843018,\n",
       " 0.2431793212890625,\n",
       " 0.7147105932235718,\n",
       " 0.5877002477645874,\n",
       " 0.30418479442596436,\n",
       " 0.9210879802703857,\n",
       " 0.9967917203903198,\n",
       " 0.5815234780311584,\n",
       " 0.25333642959594727,\n",
       " 0.30626726150512695,\n",
       " 0.9725504517555237,\n",
       " 0.4225786328315735,\n",
       " 0.8244863152503967,\n",
       " 0.4162886142730713,\n",
       " 0.5997422337532043,\n",
       " 0.5600696206092834,\n",
       " 0.5004191398620605,\n",
       " 0.8285507559776306,\n",
       " 0.9997733235359192,\n",
       " 0.6288799047470093,\n",
       " 0.005158662796020508,\n",
       " 0.6957438588142395,\n",
       " 0.06630009412765503,\n",
       " 0.6422665119171143,\n",
       " 0.993145227432251,\n",
       " 0.21572375297546387,\n",
       " 0.7024684548377991,\n",
       " 0.2630680203437805,\n",
       " 0.8988736271858215,\n",
       " 0.7391642928123474,\n",
       " 0.22134774923324585,\n",
       " 0.6789167523384094,\n",
       " 0.8040013909339905,\n",
       " 0.35439157485961914,\n",
       " 0.8093434572219849,\n",
       " 0.09001237154006958,\n",
       " 0.5445384979248047,\n",
       " 0.04341381788253784,\n",
       " 0.6682748794555664,\n",
       " 0.4987981915473938,\n",
       " 0.9434077143669128,\n",
       " 0.32322806119918823,\n",
       " 0.670275092124939,\n",
       " 0.7592061758041382,\n",
       " 0.003848731517791748,\n",
       " 0.9556252956390381,\n",
       " 0.620187520980835,\n",
       " 0.3201187252998352,\n",
       " 0.8976753354072571,\n",
       " 0.6899785399436951,\n",
       " 0.752806544303894,\n",
       " 0.9067873358726501,\n",
       " 0.9209396839141846,\n",
       " 0.3112579584121704,\n",
       " 0.6315381526947021,\n",
       " 0.6885513663291931,\n",
       " 0.001931905746459961,\n",
       " 0.8306366801261902,\n",
       " 0.48161476850509644,\n",
       " 0.8700757622718811,\n",
       " 0.26338905096054077,\n",
       " 0.6935290694236755,\n",
       " 0.9939630031585693,\n",
       " 0.10217523574829102,\n",
       " 0.4578663110733032,\n",
       " 0.1751531958580017,\n",
       " 0.9836190938949585,\n",
       " 0.6043255925178528,\n",
       " 0.21737098693847656,\n",
       " 0.060620248317718506,\n",
       " 0.7130420804023743,\n",
       " 0.026327013969421387,\n",
       " 0.4595887064933777,\n",
       " 0.16197073459625244,\n",
       " 0.6839739680290222,\n",
       " 0.4779939651489258,\n",
       " 0.8826907277107239,\n",
       " 0.08754432201385498,\n",
       " 0.013804793357849121,\n",
       " 0.8064361214637756,\n",
       " 0.9502878189086914,\n",
       " 0.7464920878410339,\n",
       " 0.8137876391410828,\n",
       " 0.7171650528907776,\n",
       " 0.7489567995071411,\n",
       " 0.8747744560241699,\n",
       " 0.5729002952575684,\n",
       " 0.9722386598587036,\n",
       " 0.6147077679634094,\n",
       " 0.14632481336593628,\n",
       " 0.24464601278305054,\n",
       " 0.879413366317749,\n",
       " 0.25008028745651245,\n",
       " 0.05729633569717407,\n",
       " 0.8013498187065125,\n",
       " 0.24635308980941772,\n",
       " 0.9206911325454712,\n",
       " 0.7552177309989929,\n",
       " 0.20008540153503418,\n",
       " 0.8999033570289612,\n",
       " 0.5825475454330444,\n",
       " 0.37884849309921265,\n",
       " 0.22555285692214966,\n",
       " 0.853053092956543,\n",
       " 0.9986898303031921,\n",
       " 0.8954740166664124,\n",
       " 0.1424354910850525,\n",
       " 0.3829208016395569,\n",
       " 0.1314799189567566,\n",
       " 0.02646470069885254,\n",
       " 0.7914786338806152,\n",
       " 0.31495606899261475,\n",
       " 0.5067151188850403,\n",
       " 0.813412070274353,\n",
       " 0.5024926662445068,\n",
       " 0.24197906255722046,\n",
       " 0.31333309412002563,\n",
       " 0.6645275354385376,\n",
       " 0.976413905620575,\n",
       " 0.9909517765045166,\n",
       " 0.5871424078941345,\n",
       " 0.6484352350234985,\n",
       " 0.8504252433776855,\n",
       " 0.12998038530349731,\n",
       " 0.6142960786819458,\n",
       " 0.2580302357673645,\n",
       " 0.29134517908096313,\n",
       " 0.4756937623023987,\n",
       " 0.8712212443351746,\n",
       " 0.027314484119415283,\n",
       " 0.47916096448898315,\n",
       " 0.2260303497314453,\n",
       " 0.17411279678344727,\n",
       " 0.5947117209434509,\n",
       " 0.2803283929824829,\n",
       " 0.6016900539398193,\n",
       " 0.9094047546386719,\n",
       " 0.8931299448013306,\n",
       " 0.40356409549713135,\n",
       " 0.9085417985916138,\n",
       " 0.20304656028747559,\n",
       " 0.667129397392273,\n",
       " 0.06829828023910522,\n",
       " 0.7848703861236572,\n",
       " 0.46463268995285034,\n",
       " 0.2674696445465088,\n",
       " 0.25352174043655396,\n",
       " 0.7574511170387268,\n",
       " 0.6276615262031555,\n",
       " 0.7179768681526184,\n",
       " 0.018248677253723145,\n",
       " 0.6653671264648438,\n",
       " 0.6184170842170715,\n",
       " 0.4450131058692932,\n",
       " 0.8024008870124817,\n",
       " 0.8661613464355469,\n",
       " 0.8105472326278687,\n",
       " 0.3087407946586609,\n",
       " 0.1619110107421875,\n",
       " 0.2984396815299988,\n",
       " 0.37512660026550293,\n",
       " 0.7437169551849365,\n",
       " 0.22144365310668945,\n",
       " 0.17651492357254028,\n",
       " 0.7806674242019653,\n",
       " 0.3818705081939697,\n",
       " 0.33072948455810547,\n",
       " 0.7857630848884583,\n",
       " 0.8194012641906738,\n",
       " 0.2954334616661072,\n",
       " 0.5653452277183533,\n",
       " 0.7887519001960754,\n",
       " 0.6079825162887573,\n",
       " 0.7883086204528809,\n",
       " 0.770317554473877,\n",
       " 0.22240805625915527,\n",
       " 0.3514571189880371,\n",
       " 0.10871708393096924,\n",
       " 0.7093568444252014,\n",
       " 0.9186282753944397,\n",
       " 0.29797160625457764,\n",
       " 0.8390017747879028,\n",
       " 0.16283851861953735,\n",
       " 0.898122251033783,\n",
       " 0.8878046274185181,\n",
       " 0.25890153646469116,\n",
       " 0.7092407941818237,\n",
       " 0.5391815900802612,\n",
       " 0.32101184129714966,\n",
       " 0.783345639705658,\n",
       " 0.4269363284111023,\n",
       " 0.06529122591018677,\n",
       " 0.059118032455444336,\n",
       " 0.01788407564163208,\n",
       " 0.18136167526245117,\n",
       " 0.5910117626190186,\n",
       " 0.6517431735992432,\n",
       " 0.19903844594955444,\n",
       " 0.6475101709365845,\n",
       " 0.05776846408843994,\n",
       " 0.320234477519989,\n",
       " 0.5142713785171509,\n",
       " 0.5583532452583313,\n",
       " 0.5299060940742493,\n",
       " 0.591018557548523,\n",
       " 0.8877642750740051,\n",
       " 0.8074135184288025,\n",
       " 0.8526110053062439,\n",
       " 0.885175347328186,\n",
       " 0.4150899052619934,\n",
       " 0.2550113797187805,\n",
       " 0.2583428621292114,\n",
       " 0.3509035110473633,\n",
       " 0.9264305830001831,\n",
       " 0.44777464866638184,\n",
       " 0.0576709508895874,\n",
       " 0.7531840205192566,\n",
       " 0.009655773639678955,\n",
       " 0.16965174674987793,\n",
       " 0.10327655076980591,\n",
       " 0.5285133123397827,\n",
       " 0.2206748127937317,\n",
       " 0.2817954421043396,\n",
       " 0.5894643664360046,\n",
       " 0.09573304653167725,\n",
       " 0.07033246755599976,\n",
       " 0.8276922702789307,\n",
       " 0.7132702469825745,\n",
       " 0.15775036811828613,\n",
       " 0.3682153820991516,\n",
       " 0.869243323802948,\n",
       " 0.1988595724105835,\n",
       " 0.9606475830078125,\n",
       " 0.810214638710022,\n",
       " 0.9904304146766663,\n",
       " 0.9351817965507507,\n",
       " 0.9293267130851746,\n",
       " 0.9046855568885803,\n",
       " 0.739124059677124,\n",
       " 0.7466392517089844,\n",
       " 0.09978479146957397,\n",
       " 0.24474841356277466,\n",
       " 0.3993372917175293,\n",
       " 0.08334386348724365,\n",
       " 0.370192289352417,\n",
       " 0.10213595628738403,\n",
       " 0.36477017402648926,\n",
       " 0.5489005446434021,\n",
       " 0.6725826859474182,\n",
       " 0.17505306005477905,\n",
       " 0.7178160548210144,\n",
       " 0.5032871961593628,\n",
       " 0.38535815477371216,\n",
       " 0.742478609085083,\n",
       " 0.15845012664794922,\n",
       " 0.6232826113700867,\n",
       " 0.7311684489250183,\n",
       " 0.19990968704223633,\n",
       " 0.21957695484161377,\n",
       " 0.4837724566459656,\n",
       " 0.6949253678321838,\n",
       " 0.712932825088501,\n",
       " 0.11315524578094482,\n",
       " 0.7840254902839661,\n",
       " 0.9242498874664307,\n",
       " 0.3973773717880249,\n",
       " 0.6208500862121582,\n",
       " 0.22951561212539673,\n",
       " 0.18151700496673584,\n",
       " 0.9872927665710449,\n",
       " 0.5052964091300964,\n",
       " 0.9709502458572388,\n",
       " 0.5192756056785583,\n",
       " 0.4831680655479431,\n",
       " 0.4014819860458374,\n",
       " 0.8231594562530518,\n",
       " 0.5169662237167358,\n",
       " 0.7585039734840393,\n",
       " 0.09730935096740723,\n",
       " 0.6232274174690247,\n",
       " 0.6669886708259583,\n",
       " 0.6407861709594727,\n",
       " 0.10662341117858887,\n",
       " 0.7979809045791626,\n",
       " 0.7527744174003601,\n",
       " 0.49584531784057617,\n",
       " 0.7901003956794739,\n",
       " 0.15549761056900024,\n",
       " 0.12814265489578247,\n",
       " 0.8109645247459412,\n",
       " 0.2873166799545288,\n",
       " 0.6135491728782654,\n",
       " 0.7246319651603699,\n",
       " 0.32358992099761963,\n",
       " 0.9147270917892456,\n",
       " 0.5949718356132507,\n",
       " 0.6889744997024536,\n",
       " 0.6828413605690002,\n",
       " 0.8306368589401245,\n",
       " 0.7233566641807556,\n",
       " 0.587185800075531,\n",
       " 0.5319886803627014,\n",
       " 0.2663806676864624,\n",
       " 0.5803049206733704,\n",
       " 0.1869868040084839,\n",
       " 0.8972686529159546,\n",
       " 0.44774383306503296,\n",
       " 0.8976309895515442,\n",
       " 0.46932798624038696,\n",
       " 0.4677653908729553,\n",
       " 0.6580087542533875,\n",
       " 0.49457281827926636,\n",
       " 0.3274371027946472,\n",
       " 0.22011303901672363,\n",
       " 0.5527346730232239,\n",
       " 0.3484380841255188,\n",
       " 0.3623352646827698,\n",
       " 0.7612910866737366,\n",
       " 0.7697179317474365,\n",
       " 0.4986596703529358,\n",
       " 0.5962828993797302,\n",
       " 0.5998974442481995,\n",
       " 0.5026366710662842,\n",
       " 0.3257744312286377,\n",
       " 0.03381752967834473,\n",
       " 0.3267093300819397,\n",
       " 0.49436861276626587,\n",
       " 0.8237601518630981,\n",
       " 0.5801730155944824,\n",
       " 0.36986595392227173,\n",
       " 0.38891834020614624,\n",
       " 0.33584320545196533,\n",
       " 0.41886401176452637,\n",
       " 0.5503759980201721,\n",
       " 0.2252187728881836,\n",
       " 0.11073994636535645,\n",
       " 0.34393519163131714,\n",
       " 0.8415501117706299,\n",
       " 0.3629634380340576,\n",
       " 0.11662071943283081,\n",
       " 0.714331328868866,\n",
       " 0.2577797770500183,\n",
       " 0.15731173753738403,\n",
       " 0.909270703792572,\n",
       " 0.8263120651245117,\n",
       " 0.6924286484718323,\n",
       " 0.8593181371688843,\n",
       " 0.6051228046417236,\n",
       " 0.5727875828742981,\n",
       " 0.5365147590637207,\n",
       " 0.4678610563278198,\n",
       " 0.7777222394943237,\n",
       " 0.008293390274047852,\n",
       " 0.25052177906036377,\n",
       " 0.8706718683242798,\n",
       " 0.037364661693573,\n",
       " 0.7118600606918335,\n",
       " 0.33538228273391724,\n",
       " 0.739903450012207,\n",
       " 0.19672048091888428,\n",
       " 0.8506006002426147,\n",
       " 0.6990200877189636,\n",
       " 0.07216787338256836,\n",
       " 0.7108917832374573,\n",
       " 0.3783787488937378,\n",
       " 0.09935343265533447,\n",
       " 0.5859708189964294,\n",
       " 0.3330864906311035,\n",
       " 0.8024729490280151,\n",
       " 0.6869310140609741,\n",
       " 0.8423553109169006,\n",
       " 0.02280271053314209,\n",
       " 0.9285290241241455,\n",
       " 0.8568893074989319,\n",
       " 0.12560689449310303,\n",
       " 0.2606313228607178,\n",
       " 0.9257752299308777,\n",
       " 0.10701173543930054,\n",
       " 0.008284211158752441,\n",
       " 0.6560054421424866,\n",
       " 0.0906597375869751,\n",
       " 0.7215650677680969,\n",
       " 0.9399287104606628,\n",
       " 0.5797085165977478,\n",
       " 0.20323169231414795,\n",
       " 0.4119492173194885,\n",
       " 0.3468882441520691,\n",
       " 0.7583786845207214,\n",
       " 0.7050208449363708,\n",
       " 0.7403837442398071,\n",
       " 0.6846193075180054,\n",
       " 0.9339163899421692,\n",
       " 0.09239518642425537,\n",
       " 0.4587472081184387,\n",
       " 0.013897538185119629,\n",
       " 0.5861786007881165,\n",
       " 0.7923494577407837,\n",
       " 0.18656015396118164,\n",
       " 0.5813653469085693,\n",
       " 0.7436767220497131,\n",
       " 0.4537907838821411,\n",
       " 0.4085026979446411,\n",
       " 0.08226227760314941,\n",
       " 0.6055610179901123,\n",
       " 0.43141603469848633,\n",
       " 0.7706182599067688,\n",
       " 0.47515869140625,\n",
       " 0.20875978469848633,\n",
       " 0.07471662759780884,\n",
       " 0.955183744430542,\n",
       " 0.1599726676940918,\n",
       " 0.5666988492012024,\n",
       " 0.5813811421394348,\n",
       " 0.42089784145355225,\n",
       " 0.9833466410636902,\n",
       " 0.9961351752281189,\n",
       " 0.43374037742614746,\n",
       " 0.9924379587173462,\n",
       " 0.14262425899505615,\n",
       " 0.9664715528488159,\n",
       " 0.6994162797927856,\n",
       " 0.5683929920196533,\n",
       " 0.8812900185585022,\n",
       " 0.8470758199691772,\n",
       " 0.4964295029640198,\n",
       " 0.2661004662513733,\n",
       " 0.7167083024978638,\n",
       " 0.01917874813079834,\n",
       " 0.23907655477523804,\n",
       " 0.6538435816764832,\n",
       " 0.7246182560920715,\n",
       " 0.5160923600196838,\n",
       " 0.2936694025993347,\n",
       " 0.5235556364059448,\n",
       " 0.13483703136444092,\n",
       " 0.02272343635559082,\n",
       " 0.25711488723754883,\n",
       " 0.5881277322769165,\n",
       " 0.4918937087059021,\n",
       " 0.15335196256637573,\n",
       " 0.7255132794380188,\n",
       " 0.2337414026260376,\n",
       " 0.47140949964523315,\n",
       " 0.16679656505584717,\n",
       " 0.05543088912963867,\n",
       " 0.8282585144042969,\n",
       " 0.911255419254303,\n",
       " 0.16349560022354126,\n",
       " 0.42097145318984985,\n",
       " 0.38485413789749146,\n",
       " 0.16423600912094116,\n",
       " 0.952430248260498,\n",
       " 0.8030121326446533,\n",
       " 0.1305943727493286,\n",
       " 0.9376044273376465,\n",
       " 0.04886752367019653,\n",
       " 0.5794118046760559,\n",
       " 0.30315423011779785,\n",
       " 0.6728137135505676,\n",
       " 0.7531926035881042,\n",
       " 0.8181132078170776,\n",
       " 0.1995905041694641,\n",
       " 0.929583728313446,\n",
       " 0.24413764476776123,\n",
       " 0.7742285132408142,\n",
       " 0.7328123450279236,\n",
       " 0.72440105676651,\n",
       " 0.022462069988250732,\n",
       " 0.9062864184379578,\n",
       " 0.9178913831710815,\n",
       " 0.8244832754135132,\n",
       " 0.2357008457183838,\n",
       " 0.6483300924301147,\n",
       " 0.07367849349975586,\n",
       " 0.12039655447006226,\n",
       " 0.4426273703575134,\n",
       " 0.28627216815948486,\n",
       " 0.4584978222846985,\n",
       " 0.8311578631401062,\n",
       " 0.5281211137771606,\n",
       " 0.02427762746810913,\n",
       " 0.5862414836883545,\n",
       " 0.5541321635246277,\n",
       " 0.9528368711471558,\n",
       " 0.9550659656524658,\n",
       " 0.8964145183563232,\n",
       " 0.30903732776641846,\n",
       " 0.961193859577179,\n",
       " 0.9688474535942078,\n",
       " 0.875,\n",
       " 0.7567446231842041,\n",
       " 0.8067413568496704,\n",
       " 0.06706029176712036,\n",
       " 0.3056197762489319,\n",
       " 0.7204355597496033,\n",
       " 0.8819230198860168,\n",
       " 0.767248272895813,\n",
       " 0.24408572912216187,\n",
       " 0.30624818801879883,\n",
       " 0.43461334705352783,\n",
       " 0.5929950475692749,\n",
       " 0.8995863795280457,\n",
       " 0.7946780920028687,\n",
       " 0.11160820722579956,\n",
       " 0.143964946269989,\n",
       " 0.000352323055267334,\n",
       " 0.4173471927642822,\n",
       " 0.1841827630996704,\n",
       " 0.9190297722816467,\n",
       " 0.2062150239944458,\n",
       " 0.039246201515197754,\n",
       " 0.6609559655189514,\n",
       " 0.8374389410018921,\n",
       " 0.3154745101928711,\n",
       " 0.9732721447944641,\n",
       " 0.9573009610176086,\n",
       " 0.7261643409729004,\n",
       " 0.6436716318130493,\n",
       " 0.6976370811462402,\n",
       " 0.7292602062225342,\n",
       " 0.7943879961967468,\n",
       " 0.362030029296875,\n",
       " 0.992958664894104,\n",
       " 0.9443995356559753,\n",
       " 0.4166838526725769,\n",
       " 0.2132718563079834,\n",
       " 0.3931359052658081,\n",
       " 0.04284107685089111,\n",
       " 0.9269698858261108,\n",
       " 0.5376984477043152,\n",
       " 0.5003983378410339,\n",
       " 0.548503041267395,\n",
       " 0.5616535544395447,\n",
       " 0.5589806437492371,\n",
       " 0.4182747006416321,\n",
       " 0.6008310914039612,\n",
       " 0.5196573138237,\n",
       " 0.1582220196723938,\n",
       " 0.2752504348754883,\n",
       " 0.2901694178581238,\n",
       " 0.11033511161804199,\n",
       " 0.06817817687988281,\n",
       " 0.23114943504333496,\n",
       " 0.4174293279647827,\n",
       " 0.495158851146698,\n",
       " 0.6190016865730286,\n",
       " 0.8170337080955505,\n",
       " 0.9684235453605652,\n",
       " 0.46947866678237915,\n",
       " 0.6641516089439392,\n",
       " 0.5981349945068359,\n",
       " 0.12533873319625854,\n",
       " 0.6731089353561401,\n",
       " 0.646799623966217,\n",
       " 0.11264532804489136,\n",
       " 0.434192419052124,\n",
       " 0.6326519846916199,\n",
       " 0.3758959174156189,\n",
       " 0.15524917840957642,\n",
       " 0.9368612766265869,\n",
       " 0.02771514654159546,\n",
       " 0.9332771301269531,\n",
       " 0.757158637046814,\n",
       " 0.5062451362609863,\n",
       " 0.8663529753684998,\n",
       " 0.8759313821792603,\n",
       " 0.04624742269515991,\n",
       " 0.7809432148933411,\n",
       " 0.4039177894592285,\n",
       " 0.7325542569160461,\n",
       " 0.15858644247055054,\n",
       " 0.5882534384727478,\n",
       " 0.8927008509635925,\n",
       " 0.9120153188705444,\n",
       " 0.6555870771408081,\n",
       " 0.5877743363380432,\n",
       " 0.4714818596839905,\n",
       " 0.8090859651565552,\n",
       " 0.5475917458534241,\n",
       " 0.8021171689033508,\n",
       " 0.3882596492767334,\n",
       " 0.050033390522003174,\n",
       " 0.5611713528633118,\n",
       " 0.8279046416282654,\n",
       " 0.21742552518844604,\n",
       " 0.43566465377807617,\n",
       " 0.593519926071167,\n",
       " 0.8069453835487366,\n",
       " 0.6011187434196472,\n",
       " 0.3795804977416992,\n",
       " 0.5520650148391724,\n",
       " 0.1415804624557495,\n",
       " 0.4684706926345825,\n",
       " 0.6970905065536499,\n",
       " 0.8870042562484741,\n",
       " 0.9863348603248596,\n",
       " 0.8715416789054871,\n",
       " 0.5208064913749695,\n",
       " 0.4459710121154785,\n",
       " 0.9066676497459412,\n",
       " 0.14109539985656738,\n",
       " 0.020910441875457764,\n",
       " 0.03732037544250488,\n",
       " 0.7979227304458618,\n",
       " 0.6498977541923523,\n",
       " 0.9808781743049622,\n",
       " 0.23431813716888428,\n",
       " 0.19005441665649414,\n",
       " 0.6437061429023743,\n",
       " 0.2968926429748535,\n",
       " 0.30249494314193726,\n",
       " 0.24906718730926514,\n",
       " 0.42843884229660034,\n",
       " 0.33043718338012695,\n",
       " 0.7434682846069336,\n",
       " 0.4056771397590637,\n",
       " 0.0954863429069519,\n",
       " 0.9147817492485046,\n",
       " 0.7686442136764526,\n",
       " 0.9097394943237305,\n",
       " 0.395318865776062,\n",
       " 0.9834705591201782,\n",
       " 0.6045637726783752,\n",
       " 0.32029497623443604,\n",
       " 0.3997490406036377,\n",
       " 0.6780056357383728,\n",
       " 0.05168747901916504,\n",
       " 0.999297022819519,\n",
       " 0.884283721446991,\n",
       " 0.9028311371803284,\n",
       " 0.4764549136161804,\n",
       " 0.3983725905418396,\n",
       " 0.23307263851165771,\n",
       " 0.3934793472290039,\n",
       " 0.9466110467910767,\n",
       " 0.9464482069015503,\n",
       " 0.699661910533905,\n",
       " 0.6581564545631409,\n",
       " 0.18981218338012695,\n",
       " 0.8024781942367554,\n",
       " 0.3751213550567627,\n",
       " 0.334458589553833,\n",
       " 0.7643264532089233,\n",
       " 0.903102695941925,\n",
       " 0.05933964252471924,\n",
       " 0.4595807194709778,\n",
       " 0.21773964166641235,\n",
       " 0.21772807836532593,\n",
       " 0.0628359317779541,\n",
       " 0.6229159832000732,\n",
       " 0.8058879971504211,\n",
       " 0.7081664204597473,\n",
       " 0.5442911982536316,\n",
       " 0.18443942070007324,\n",
       " 0.23033398389816284,\n",
       " 0.5395301580429077,\n",
       " 0.7915443181991577,\n",
       " 0.2579595446586609,\n",
       " 0.24181050062179565,\n",
       " 0.33916741609573364,\n",
       " 0.016370952129364014,\n",
       " 0.6522747874259949,\n",
       " 0.31440067291259766,\n",
       " 0.3189038038253784,\n",
       " 0.808463990688324,\n",
       " 0.004734396934509277,\n",
       " 0.0026378631591796875,\n",
       " 0.8423764109611511,\n",
       " 0.9435057640075684,\n",
       " 0.3192784786224365,\n",
       " 0.18136614561080933,\n",
       " 0.9436796307563782,\n",
       " 0.7673056721687317,\n",
       " 0.7871708273887634,\n",
       " 0.3869650363922119,\n",
       " 0.4272107481956482,\n",
       " 0.4095193147659302,\n",
       " 0.8979285955429077,\n",
       " 0.27382999658584595,\n",
       " 0.3470115065574646,\n",
       " 0.15649741888046265,\n",
       " 0.4635006785392761,\n",
       " 0.09810483455657959,\n",
       " 0.48819899559020996,\n",
       " 0.29990315437316895,\n",
       " 0.058436691761016846,\n",
       " 0.5499934554100037,\n",
       " 0.03512352705001831,\n",
       " 0.05373507738113403,\n",
       " 0.5949846506118774,\n",
       " 0.16705483198165894,\n",
       " 0.28657978773117065,\n",
       " 0.502250075340271,\n",
       " 0.8972572684288025,\n",
       " 0.295620858669281,\n",
       " 0.6419159173965454,\n",
       " 0.3292589783668518,\n",
       " 0.519138514995575,\n",
       " 0.37668538093566895,\n",
       " 0.4387434124946594,\n",
       " 0.48438161611557007,\n",
       " 0.33444684743881226,\n",
       " 0.8896388411521912,\n",
       " 0.547072172164917,\n",
       " 0.6970271468162537,\n",
       " 0.3378334641456604,\n",
       " 0.9916807413101196,\n",
       " 0.4653451442718506,\n",
       " 0.2167421579360962,\n",
       " 0.48549747467041016,\n",
       " 0.1768101453781128,\n",
       " 0.980639636516571,\n",
       " 0.0012224316596984863,\n",
       " 0.327378511428833,\n",
       " 0.5938223004341125,\n",
       " 0.016198456287384033,\n",
       " 0.12074154615402222,\n",
       " 0.5578197240829468,\n",
       " 0.25382471084594727,\n",
       " 0.3947659134864807,\n",
       " 0.7557727098464966,\n",
       " 0.6085204482078552,\n",
       " 0.6149820685386658,\n",
       " 0.8021078109741211,\n",
       " 0.47867250442504883,\n",
       " 0.14414972066879272,\n",
       " 0.31657397747039795,\n",
       " 0.3092421293258667,\n",
       " 0.6274701356887817,\n",
       " 0.22781449556350708,\n",
       " 0.7416717410087585,\n",
       " 0.3813520669937134,\n",
       " 0.240766704082489,\n",
       " 0.8965731859207153,\n",
       " 0.7916085124015808,\n",
       " 0.03571522235870361,\n",
       " 0.5586869716644287,\n",
       " 0.9794045090675354,\n",
       " 0.6172848343849182,\n",
       " 0.44762420654296875,\n",
       " 0.7329884767532349,\n",
       " 0.2388741374015808,\n",
       " 0.8451014757156372,\n",
       " 0.1818068027496338,\n",
       " 0.5230742692947388,\n",
       " 0.10437178611755371,\n",
       " 0.7064142227172852,\n",
       " 0.629827618598938,\n",
       " 0.2791312336921692,\n",
       " 0.09925401210784912,\n",
       " 0.04521721601486206,\n",
       " 0.024216055870056152,\n",
       " 0.5822557806968689,\n",
       " 0.925320029258728,\n",
       " 0.7845847606658936,\n",
       " 0.08101272583007812,\n",
       " 0.46997058391571045,\n",
       " 0.445013165473938,\n",
       " 0.6360939145088196,\n",
       " 0.09452569484710693,\n",
       " 0.5044222474098206,\n",
       " 0.2467823028564453,\n",
       " 0.2795335054397583,\n",
       " 0.6592636108398438,\n",
       " 0.4348687529563904,\n",
       " 0.0031964778900146484,\n",
       " 0.49604153633117676,\n",
       " 0.8453792333602905,\n",
       " 0.13973230123519897,\n",
       " 0.06894785165786743,\n",
       " 0.6328699588775635,\n",
       " 0.196424663066864,\n",
       " 0.36814117431640625,\n",
       " 0.19010299444198608,\n",
       " 0.9484738707542419,\n",
       " 0.49521350860595703,\n",
       " 0.5948569774627686,\n",
       " 0.5279772877693176,\n",
       " 0.6102932691574097,\n",
       " 0.4196908473968506,\n",
       " 0.04863518476486206,\n",
       " 0.7730737328529358,\n",
       " 0.4395350217819214,\n",
       " 0.19293951988220215,\n",
       " 0.393485426902771,\n",
       " 0.3779751658439636,\n",
       " 0.7042654156684875,\n",
       " 0.24419832229614258,\n",
       " 0.2755287289619446,\n",
       " 0.30953776836395264,\n",
       " 0.6882476210594177,\n",
       " 0.00897824764251709,\n",
       " 0.8417093753814697,\n",
       " 0.24201935529708862,\n",
       " 0.8575859069824219,\n",
       " 0.5585039854049683,\n",
       " 0.5425127148628235,\n",
       " 0.36892879009246826,\n",
       " 0.26902449131011963,\n",
       " 0.23224037885665894,\n",
       " 0.9356959462165833,\n",
       " 0.8597556352615356,\n",
       " 0.609472930431366,\n",
       " 0.758965015411377,\n",
       " 0.27011221647262573,\n",
       " 0.8015804886817932,\n",
       " 0.41928255558013916,\n",
       " 0.41926562786102295,\n",
       " 0.9252530336380005,\n",
       " 0.23010969161987305,\n",
       " 0.7191148996353149,\n",
       " 0.5257682204246521,\n",
       " 0.7299785017967224,\n",
       " 0.08194833993911743,\n",
       " 0.41102486848831177,\n",
       " 0.9509848356246948,\n",
       " 0.43763649463653564,\n",
       " 0.3604821562767029,\n",
       " 0.8965984582901001,\n",
       " 0.10212552547454834,\n",
       " 0.8624499440193176,\n",
       " 0.37480831146240234,\n",
       " 0.2987518310546875,\n",
       " 0.5739509463310242,\n",
       " 0.5130413770675659,\n",
       " 0.43661361932754517,\n",
       " 0.6448957324028015,\n",
       " 0.5560146570205688,\n",
       " 0.7645385265350342,\n",
       " 0.7565867900848389,\n",
       " 0.6290979981422424,\n",
       " 0.02885305881500244,\n",
       " 0.5724392533302307,\n",
       " 0.014715135097503662,\n",
       " 0.7581226825714111,\n",
       " 0.786858856678009,\n",
       " 0.08436799049377441,\n",
       " 0.42348140478134155,\n",
       " 0.7045149207115173,\n",
       " 0.8587519526481628,\n",
       " 0.3932152986526489,\n",
       " 0.4474315643310547,\n",
       " 0.7236901521682739,\n",
       " 0.5974564552307129,\n",
       " 0.024958431720733643,\n",
       " 0.33651572465896606,\n",
       " 0.8091041445732117,\n",
       " 0.9961338043212891,\n",
       " 0.7392391562461853,\n",
       " 0.8473224639892578,\n",
       " 0.7109858989715576,\n",
       " 0.033834993839263916,\n",
       " 0.9977721571922302,\n",
       " 0.9503419995307922,\n",
       " 0.5282595157623291,\n",
       " 0.20963478088378906,\n",
       " 0.5835437774658203,\n",
       " 0.7965834736824036,\n",
       " 0.6544371843338013,\n",
       " 0.7710309624671936,\n",
       " 0.7071840763092041,\n",
       " 0.4023818373680115,\n",
       " 0.4644045829772949,\n",
       " 0.2022051215171814,\n",
       " 0.4881553649902344,\n",
       " 0.23549187183380127,\n",
       " 0.5531372427940369,\n",
       " 0.050286948680877686,\n",
       " 0.025557100772857666,\n",
       " 0.12132930755615234,\n",
       " 0.7423194050788879,\n",
       " 0.019285917282104492,\n",
       " 0.4578758478164673,\n",
       " 0.5028685331344604,\n",
       " 0.026884794235229492,\n",
       " 0.6332634687423706,\n",
       " 0.6938201785087585,\n",
       " 0.10030436515808105,\n",
       " 0.4288173317909241,\n",
       " 0.431037962436676,\n",
       " 0.09494078159332275,\n",
       " 0.5701484084129333,\n",
       " 0.02778446674346924,\n",
       " 0.043150901794433594,\n",
       " 0.06798762083053589,\n",
       " 0.031321704387664795,\n",
       " 0.9067211151123047,\n",
       " 0.6600807905197144,\n",
       " 0.9882475733757019,\n",
       " 0.9849879741668701,\n",
       " 0.6169756650924683,\n",
       " 0.8787671327590942,\n",
       " 0.20424699783325195,\n",
       " 0.6324059963226318,\n",
       " 0.6991827487945557,\n",
       " 0.41016340255737305,\n",
       " 0.46790313720703125,\n",
       " 0.37412792444229126,\n",
       " 0.19834691286087036,\n",
       " 0.9171125888824463,\n",
       " 0.04226726293563843,\n",
       " 0.01124119758605957,\n",
       " 0.857148289680481,\n",
       " 0.9115712642669678,\n",
       " 0.23508864641189575,\n",
       " 0.024577438831329346,\n",
       " 0.16969192028045654]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('my_dict.json', 'r') as file:\n",
    "    new_dict = json.load(file)\n",
    "\n",
    "new_dict['19dc00d3-7745-49de-8a55-b240a30de790']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
