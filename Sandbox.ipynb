{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure\n",
    "\n",
    "Run the following cell to ensure the project folder is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Datasets', exist_ok=True)\n",
    "os.makedirs('Datasetss/images', exist_ok=True)\n",
    "os.makedirs('Datasetss/cleaned_images_224', exist_ok=True)\n",
    "os.makedirs('model_evaluation', exist_ok=True)\n",
    "os.makedirs('model_evaluation/final_model', exist_ok=True)\n",
    "os.makedirs('model_evaluation/weights', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Management\n",
    "\n",
    "### Importing the datasets\n",
    "\n",
    "The dataset is stored in the file `products.csv` file and has the following columns:\n",
    "\n",
    "- id\n",
    "- product_name\n",
    "- category\n",
    "- product_description\n",
    "- price\n",
    "- location\n",
    "\n",
    "We'll onyl need the id, category and price columns, but let's import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Datasets/Products.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Cleaning\n",
    "\n",
    "Let's print out some data regarding the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7156 entries, 0 to 7155\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Unnamed: 0           7156 non-null   int64 \n",
      " 1   id                   7156 non-null   object\n",
      " 2   product_name         7156 non-null   object\n",
      " 3   category             7156 non-null   object\n",
      " 4   product_description  7156 non-null   object\n",
      " 5   price                7156 non-null   object\n",
      " 6   location             7156 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 391.5+ KB\n",
      "None\n",
      "Unnamed: 0             7156\n",
      "id                     7156\n",
      "product_name           7156\n",
      "category               7156\n",
      "product_description    7156\n",
      "price                  7156\n",
      "location               7156\n",
      "dtype: int64\n",
      "Unnamed: 0             7156\n",
      "id                     7156\n",
      "product_name           7156\n",
      "category               7156\n",
      "product_description    7156\n",
      "price                  7156\n",
      "location               7156\n",
      "dtype: int64\n",
      "['£']\n",
      "['.00' '.99' '.78' '.01' '.97' '.25' '.50' '.20' '.90' '.80' '.60' '.23'\n",
      " '.05' '.75' '.56' '.40' '.44' '.95' '.66' '.35' '.85' '.30' '.45' '.16'\n",
      " '.69' '.49' '.55' '.09' '.11']\n"
     ]
    }
   ],
   "source": [
    "# Print out general information.\n",
    "print(dataset.info())\n",
    "\n",
    "# Count the number of entries in the dataset.\n",
    "print(dataset.count())\n",
    "\n",
    "# Count the number of non-null entires in the dataset.\n",
    "print(dataset.dropna().count())\n",
    "\n",
    "# Check all the price entries begin with a pound sign.\n",
    "print(dataset[\"price\"].map(lambda x: x[0]).unique())\n",
    "\n",
    "# Check all the price entries are two point floats.\n",
    "print(dataset[\"price\"].map(lambda x: x[-3:]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"product_description\"] = dataset[\"product_description\"].astype(\"string\")\n",
    "\n",
    "dataset[\"product_name\"] = dataset[\"product_name\"].astype(\"string\")\n",
    "\n",
    "dataset[\"combined_name_and_description\"] = dataset[\"product_name\"] + '. ' + dataset[\"product_description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the dataset is already clean. \n",
    "\n",
    "### Price column processing\n",
    "\n",
    "Let's convert the perice column to type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset\n",
    "\n",
    "# Remove all columns and pound signs from the price column.\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x : x.replace('£', '')) #Removes the pound signs\n",
    "dataset_cleaned['price'] = dataset_cleaned['price'].map(lambda x: x.replace(',', '')) #Removes commas\n",
    "\n",
    "# Transform it to numeric type.\n",
    "dataset_cleaned['price'] = pd.to_numeric(dataset_cleaned['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category processing\n",
    "\n",
    "We'd like to extract the root category from each entry. If we examine the category entries, we see that subcategories are differentiated by a '/'. So we can use the `string.split` method to extract the highest level category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['category'] = dataset_cleaned['category'].map(lambda x: x.split(' /' )[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the unique values in `dataset_cleaned['category']` to check this was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home & Garden', 'Baby & Kids Stuff', 'DIY Tools & Materials',\n",
       "       'Music, Films, Books & Games', 'Phones, Mobile Phones & Telecoms',\n",
       "       'Clothes, Footwear & Accessories', 'Other Goods',\n",
       "       'Health & Beauty', 'Sports, Leisure & Travel', 'Appliances',\n",
       "       'Computers & Software', 'Office Furniture & Equipment',\n",
       "       'Video Games & Consoles'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categories\n",
    "\n",
    "We'll create a dictionary to assign each category a numeric value, called `encoder`, and another dictionary `decoder` to do the inverse. Then we save them as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the unique categories.\n",
    "list_of_categories = list(dataset_cleaned['category'].unique())\n",
    "\n",
    "encoder = {x: list_of_categories.index(x) for x in list_of_categories}\n",
    "decoder = {list_of_categories.index(x):x for x in list_of_categories}\n",
    "\n",
    "#Save these to pickle files:\n",
    "with open(\"encoder_pickle\", 'wb') as encody:\n",
    "    pickle.dump(encoder, encody)\n",
    "\n",
    "with open(\"app/Pickle_Files/encoder_pickle\", 'wb') as encody:\n",
    "    pickle.dump(encoder, encody)\n",
    "\n",
    "with open(\"decoder_pickle\", 'wb') as decody:\n",
    "    pickle.dump(decoder, decody)\n",
    "\n",
    "with open(\"app/Pickle_Files/decoder_pickle\", 'wb') as decody:\n",
    "    pickle.dump(decoder, decody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add a new column called `'root_category'` which contains the numeric value associated with the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned['root_category'] = dataset_cleaned['category']\n",
    "dataset_cleaned['root_category_index'] = dataset_cleaned['category'].map(lambda x:encoder[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging with Images id\n",
    "\n",
    "The image associated with each product is specified in the file `images.csv`. Within it, there are two columns:\n",
    "\n",
    "- id - Contains the file name of the image associated with the item\n",
    "- product_id - The id, in the sense of `products.csv` of the product associated with the image.\n",
    "\n",
    "We'd like to create a new table with the following:\n",
    "\n",
    "- The product id of each product\n",
    "- The image file name associated with the image\n",
    "- The numeric value of the category associated with the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0                                int64\n",
       " id                                       object\n",
       " product_name                     string[python]\n",
       " category                                 object\n",
       " product_description              string[python]\n",
       " price                                   float64\n",
       " location                                 object\n",
       " combined_name_and_description    string[python]\n",
       " root_category                            object\n",
       " root_category_index                       int64\n",
       " merge_column                             object\n",
       " dtype: object,\n",
       " Unnamed: 0       int64\n",
       " id              object\n",
       " product_id      object\n",
       " merge_column    object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data from `Images.csv` to a pandas dataframe.\n",
    "images_dataset = pd.read_csv('Datasets/Images.csv')\n",
    "\n",
    "# Create new column in both `images_dataset` and `dataset_cleaned` to merge the dataframes on.\n",
    "dataset_cleaned[\"merge_column\"] = dataset_cleaned['id']\n",
    "images_dataset['merge_column'] = images_dataset['product_id']\n",
    "\n",
    "# Double check the dtypes of the dataframe\n",
    "dataset_cleaned.dtypes, images_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems to be good, so we can go ahead and merge the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = images_dataset.merge(dataset_cleaned, on='merge_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we only need some of the columns of `merged_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                       object\n",
       "root_category                            object\n",
       "root_category_index                       int64\n",
       "product_name                     string[python]\n",
       "product_description              string[python]\n",
       "combined_name_and_description    string[python]\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab only the columns we need from `merged_df`.\n",
    "merged_df = merged_df[[\"id_x\", \"product_id\", \"root_category\", \"root_category_index\", \"product_name\", \"product_description\", \"combined_name_and_description\"]]\n",
    "\n",
    "# Rename the column `id_x` to just `id`.\n",
    "merged_df = merged_df.rename(columns={'id_x':'id'})\n",
    "\n",
    "# Drop the product_id column, since we only need to associated images to categories.\n",
    "merged_df = merged_df.drop(columns=['product_id'])\n",
    "\n",
    "# Double check the dtypes.\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll put everything in `merged_df` onto a `.csv` file called `training_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Datasets/training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 9.444444444444438, 'Token Count')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df['tokenized_len'] = merged_df[\"combined_name_and_description\"].map(lambda x : len(bert_tokenizer(x)['input_ids']))\n",
    "\n",
    "print(merged_df['tokenized_len'].max())\n",
    "\n",
    "sns.displot(merged_df['tokenized_len'])\n",
    "plt.xlim([0,400])# merged_df['tokenized_len'].max()])\n",
    "plt.xlabel('Token Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cetain items have multiple images, if we ignore image data then `merged_df` has many duplicate entires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7050"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_text_df = merged_df[[\"root_category\", \"root_category_index\", \"product_name\", \"product_description\", \"combined_name_and_description\"]]\n",
    "\n",
    "pure_text_df = pure_text_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "\n",
    "len(pure_text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDescriptionDataset(Dataset):\n",
    "    def __init__(self, item_descriptions:pd.Series, item_categories:pd.Series, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.descriptions = item_descriptions\n",
    "        self.categories = item_categories\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoding = self.tokenizer(self.descriptions[index], return_tensors=\"pt\", pad_to_max_length=True, max_length = 200)\n",
    "\n",
    "        return {\"input_ids\":encoding[\"input_ids\"].squeeze().to(device), \"attention_mask\":encoding[\"attention_mask\"].squeeze().to(device)}, self.categories[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = ProductDescriptionDataset(\n",
    "    item_descriptions=pure_text_df[\"combined_name_and_description\"],\n",
    "    item_categories= pure_text_df[\"root_category_index\"],\n",
    "    tokenizer=bert_tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(text_dataset[900][0]['input_ids'].shape)\n",
    "\n",
    "text_train_dataset, text_validation_dataset = torch.utils.data.random_split(\n",
    "    text_dataset, [6000, len(text_dataset) - 6000]\n",
    ")\n",
    "\n",
    "text_train_loader = DataLoader(text_train_dataset, batch_size=32, shuffle=True)\n",
    "text_validation_loader = DataLoader(text_validation_dataset, batch_size=32, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class BertNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_categories = 13) -> None:\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            torch.nn.Linear(768, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, num_categories),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "        for _, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.bert(**X)\n",
    "        X = X['pooler_output']\n",
    "        X = self.layers(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def forward_no_grad(self, X):\n",
    "        \"\"\"\n",
    "        Define a forward method which does not use grad, to speed up computaion when not training the model.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            X = self.bert(**X)\n",
    "            X = X['pooler_output']\n",
    "            X = self.layers(X)\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_model = BertNeuralNetwork(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "\n",
    "### Resizing and cropping images\n",
    "\n",
    "The image dataset contains images of varing aspect ratios and sizes. We'd like to make them a uniform size which is small enough so that training takes a reasonable amount of time. Let's define some functions to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an image and an integer. Transforms the image to a square of size `final_size`.\n",
    "def resize_image(final_size:int, im:PIL.Image):\n",
    "\n",
    "    # Current image size\n",
    "    size = im.size\n",
    "\n",
    "    # Ratio to transform the image by\n",
    "    ratio = float(final_size) / max(size)\n",
    "    new_image_size = tuple([int(x*ratio) for x in size])\n",
    "    im = im.resize(new_image_size)\n",
    "    new_im = Image.new(\"RGB\", (final_size, final_size))\n",
    "    new_im.paste(im, ((final_size-new_image_size[0])//2, (final_size-new_image_size[1])//2))\n",
    "    return new_im\n",
    "\n",
    "# Resizes all the images in `path_to_extract` to the size `image_size` and places them in `path_to_save`\n",
    "def clean_images(path_to_extract:str = \"Datasets/images/\", path_to_save:str = \"Datasets/cleaned_images/\", image_size:int = 64):\n",
    "    dirs = os.listdir(path_to_extract)\n",
    "    final_size = image_size\n",
    "    for n, item in enumerate(dirs, 1):\n",
    "        im = Image.open(path_to_extract + item)\n",
    "        new_im = resize_image(final_size, im)\n",
    "        new_im.save(path_to_save + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the image clean function\n",
    "# clean_images(path_to_save=\"Datasets/cleaned_images_224/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an image dataset\n",
    "\n",
    "We'll create a Torch dataset to load the images. It will be a class inheriting from the `torch.nn.Dataset` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df_of_keys:pd.DataFrame, folder_of_images:str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the labels to be the column 'root_cotegory_index' of df_of_keys\n",
    "        self.labels = df_of_keys['root_category_index']\n",
    "\n",
    "        # Assings image_paths to the file name from the column 'id' of df_of_keys and maps it to it's path relative to the project root folder\n",
    "        self.image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "\n",
    "        # Turns the image to a Torch tensor.\n",
    "        self.image_transformer = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Opens the image at index with using PIL.Image\n",
    "        with Image.open(self.image_paths[index]) as img:\n",
    "\n",
    "            # Sets the feature to be a tensor obtained by applying PILToTensor to the relevant image\n",
    "            X = self.image_transformer(img)\n",
    "        \n",
    "        # y is the label from self.labels\n",
    "        y = self.labels[index]\n",
    "\n",
    "        #print(X.shape, y.shape)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to see how the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " (),\n",
       " 12604)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = ImageDataset(merged_df, \"Datasets/cleaned_images_224/\")\n",
    "\n",
    "# A quick sanity check of the dataset:\n",
    "my_dataset[1][0], my_dataset[1][1].shape, len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "Since we will only be tuning hyper-parameters and not choosing between different models, we won't need a test dataset. So let's split the dataset into train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split merged_df to a train_df and validation_df\n",
    "train_df, validation_df = train_test_split(merged_df, train_size=10000, test_size= len(merged_df) - 10000)\n",
    "\n",
    "# Reset the indicies\n",
    "train_df.reset_index(inplace=True)\n",
    "validation_df.reset_index(inplace=True)\n",
    "\n",
    "# Create instances of the ImageDataset class\n",
    "train_dataset = ImageDataset(train_df, \"Datasets/cleaned_images_224/\")\n",
    "validation_dataset = ImageDataset(validation_df, \"Datasets/cleaned_images_224/\")\n",
    "\n",
    "# Allow random flips and rotations when loading an image in the train dataset\n",
    "train_dataset.image_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.PILToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Pass both to dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's to a quick sanity check to see we are loading the correct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pillow_to_image = transforms.ToPILImage()\n",
    "\n",
    "pillow_to_image(train_dataset[712][0].squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "Let's do some setup regarding Pytorch and the pretrained model, resnet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Sets the device for PyTorch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Loads the resnet50 model\n",
    "resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Don't know what this does\n",
    "resnet50.eval().to(device)\n",
    "\n",
    "print('Model succesfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the model\n",
    "\n",
    "The following is a useful function to look at the layers within a model and thier grad attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_layers(model):\n",
    "    for name, module in model.named_parameters():\n",
    "        if module.requires_grad:\n",
    "            print('Grad!', name, module.requires_grad)\n",
    "        else:\n",
    "            print(name, module.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad! conv1.weight True\n",
      "Grad! bn1.weight True\n",
      "Grad! bn1.bias True\n",
      "Grad! layer1.0.conv1.weight True\n",
      "Grad! layer1.0.bn1.weight True\n",
      "Grad! layer1.0.bn1.bias True\n",
      "Grad! layer1.0.conv2.weight True\n",
      "Grad! layer1.0.bn2.weight True\n",
      "Grad! layer1.0.bn2.bias True\n",
      "Grad! layer1.0.conv3.weight True\n",
      "Grad! layer1.0.bn3.weight True\n",
      "Grad! layer1.0.bn3.bias True\n",
      "Grad! layer1.0.downsample.0.weight True\n",
      "Grad! layer1.0.downsample.1.weight True\n",
      "Grad! layer1.0.downsample.1.bias True\n",
      "Grad! layer1.1.conv1.weight True\n",
      "Grad! layer1.1.bn1.weight True\n",
      "Grad! layer1.1.bn1.bias True\n",
      "Grad! layer1.1.conv2.weight True\n",
      "Grad! layer1.1.bn2.weight True\n",
      "Grad! layer1.1.bn2.bias True\n",
      "Grad! layer1.1.conv3.weight True\n",
      "Grad! layer1.1.bn3.weight True\n",
      "Grad! layer1.1.bn3.bias True\n",
      "Grad! layer1.2.conv1.weight True\n",
      "Grad! layer1.2.bn1.weight True\n",
      "Grad! layer1.2.bn1.bias True\n",
      "Grad! layer1.2.conv2.weight True\n",
      "Grad! layer1.2.bn2.weight True\n",
      "Grad! layer1.2.bn2.bias True\n",
      "Grad! layer1.2.conv3.weight True\n",
      "Grad! layer1.2.bn3.weight True\n",
      "Grad! layer1.2.bn3.bias True\n",
      "Grad! layer2.0.conv1.weight True\n",
      "Grad! layer2.0.bn1.weight True\n",
      "Grad! layer2.0.bn1.bias True\n",
      "Grad! layer2.0.conv2.weight True\n",
      "Grad! layer2.0.bn2.weight True\n",
      "Grad! layer2.0.bn2.bias True\n",
      "Grad! layer2.0.conv3.weight True\n",
      "Grad! layer2.0.bn3.weight True\n",
      "Grad! layer2.0.bn3.bias True\n",
      "Grad! layer2.0.downsample.0.weight True\n",
      "Grad! layer2.0.downsample.1.weight True\n",
      "Grad! layer2.0.downsample.1.bias True\n",
      "Grad! layer2.1.conv1.weight True\n",
      "Grad! layer2.1.bn1.weight True\n",
      "Grad! layer2.1.bn1.bias True\n",
      "Grad! layer2.1.conv2.weight True\n",
      "Grad! layer2.1.bn2.weight True\n",
      "Grad! layer2.1.bn2.bias True\n",
      "Grad! layer2.1.conv3.weight True\n",
      "Grad! layer2.1.bn3.weight True\n",
      "Grad! layer2.1.bn3.bias True\n",
      "Grad! layer2.2.conv1.weight True\n",
      "Grad! layer2.2.bn1.weight True\n",
      "Grad! layer2.2.bn1.bias True\n",
      "Grad! layer2.2.conv2.weight True\n",
      "Grad! layer2.2.bn2.weight True\n",
      "Grad! layer2.2.bn2.bias True\n",
      "Grad! layer2.2.conv3.weight True\n",
      "Grad! layer2.2.bn3.weight True\n",
      "Grad! layer2.2.bn3.bias True\n",
      "Grad! layer2.3.conv1.weight True\n",
      "Grad! layer2.3.bn1.weight True\n",
      "Grad! layer2.3.bn1.bias True\n",
      "Grad! layer2.3.conv2.weight True\n",
      "Grad! layer2.3.bn2.weight True\n",
      "Grad! layer2.3.bn2.bias True\n",
      "Grad! layer2.3.conv3.weight True\n",
      "Grad! layer2.3.bn3.weight True\n",
      "Grad! layer2.3.bn3.bias True\n",
      "Grad! layer3.0.conv1.weight True\n",
      "Grad! layer3.0.bn1.weight True\n",
      "Grad! layer3.0.bn1.bias True\n",
      "Grad! layer3.0.conv2.weight True\n",
      "Grad! layer3.0.bn2.weight True\n",
      "Grad! layer3.0.bn2.bias True\n",
      "Grad! layer3.0.conv3.weight True\n",
      "Grad! layer3.0.bn3.weight True\n",
      "Grad! layer3.0.bn3.bias True\n",
      "Grad! layer3.0.downsample.0.weight True\n",
      "Grad! layer3.0.downsample.1.weight True\n",
      "Grad! layer3.0.downsample.1.bias True\n",
      "Grad! layer3.1.conv1.weight True\n",
      "Grad! layer3.1.bn1.weight True\n",
      "Grad! layer3.1.bn1.bias True\n",
      "Grad! layer3.1.conv2.weight True\n",
      "Grad! layer3.1.bn2.weight True\n",
      "Grad! layer3.1.bn2.bias True\n",
      "Grad! layer3.1.conv3.weight True\n",
      "Grad! layer3.1.bn3.weight True\n",
      "Grad! layer3.1.bn3.bias True\n",
      "Grad! layer3.2.conv1.weight True\n",
      "Grad! layer3.2.bn1.weight True\n",
      "Grad! layer3.2.bn1.bias True\n",
      "Grad! layer3.2.conv2.weight True\n",
      "Grad! layer3.2.bn2.weight True\n",
      "Grad! layer3.2.bn2.bias True\n",
      "Grad! layer3.2.conv3.weight True\n",
      "Grad! layer3.2.bn3.weight True\n",
      "Grad! layer3.2.bn3.bias True\n",
      "Grad! layer3.3.conv1.weight True\n",
      "Grad! layer3.3.bn1.weight True\n",
      "Grad! layer3.3.bn1.bias True\n",
      "Grad! layer3.3.conv2.weight True\n",
      "Grad! layer3.3.bn2.weight True\n",
      "Grad! layer3.3.bn2.bias True\n",
      "Grad! layer3.3.conv3.weight True\n",
      "Grad! layer3.3.bn3.weight True\n",
      "Grad! layer3.3.bn3.bias True\n",
      "Grad! layer3.4.conv1.weight True\n",
      "Grad! layer3.4.bn1.weight True\n",
      "Grad! layer3.4.bn1.bias True\n",
      "Grad! layer3.4.conv2.weight True\n",
      "Grad! layer3.4.bn2.weight True\n",
      "Grad! layer3.4.bn2.bias True\n",
      "Grad! layer3.4.conv3.weight True\n",
      "Grad! layer3.4.bn3.weight True\n",
      "Grad! layer3.4.bn3.bias True\n",
      "Grad! layer3.5.conv1.weight True\n",
      "Grad! layer3.5.bn1.weight True\n",
      "Grad! layer3.5.bn1.bias True\n",
      "Grad! layer3.5.conv2.weight True\n",
      "Grad! layer3.5.bn2.weight True\n",
      "Grad! layer3.5.bn2.bias True\n",
      "Grad! layer3.5.conv3.weight True\n",
      "Grad! layer3.5.bn3.weight True\n",
      "Grad! layer3.5.bn3.bias True\n",
      "Grad! layer4.0.conv1.weight True\n",
      "Grad! layer4.0.bn1.weight True\n",
      "Grad! layer4.0.bn1.bias True\n",
      "Grad! layer4.0.conv2.weight True\n",
      "Grad! layer4.0.bn2.weight True\n",
      "Grad! layer4.0.bn2.bias True\n",
      "Grad! layer4.0.conv3.weight True\n",
      "Grad! layer4.0.bn3.weight True\n",
      "Grad! layer4.0.bn3.bias True\n",
      "Grad! layer4.0.downsample.0.weight True\n",
      "Grad! layer4.0.downsample.1.weight True\n",
      "Grad! layer4.0.downsample.1.bias True\n",
      "Grad! layer4.1.conv1.weight True\n",
      "Grad! layer4.1.bn1.weight True\n",
      "Grad! layer4.1.bn1.bias True\n",
      "Grad! layer4.1.conv2.weight True\n",
      "Grad! layer4.1.bn2.weight True\n",
      "Grad! layer4.1.bn2.bias True\n",
      "Grad! layer4.1.conv3.weight True\n",
      "Grad! layer4.1.bn3.weight True\n",
      "Grad! layer4.1.bn3.bias True\n",
      "Grad! layer4.2.conv1.weight True\n",
      "Grad! layer4.2.bn1.weight True\n",
      "Grad! layer4.2.bn1.bias True\n",
      "Grad! layer4.2.conv2.weight True\n",
      "Grad! layer4.2.bn2.weight True\n",
      "Grad! layer4.2.bn2.bias True\n",
      "Grad! layer4.2.conv3.weight True\n",
      "Grad! layer4.2.bn3.weight True\n",
      "Grad! layer4.2.bn3.bias True\n",
      "Grad! fc.weight True\n",
      "Grad! fc.bias True\n"
     ]
    }
   ],
   "source": [
    "show_model_layers(resnet50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final layer of resnet50, `fc` takes in 2048 features and outputs 1000 features. The number of labels we need to classify, i.e. the number of distinct values in `merged_df['root_categories']` is 13.\n",
    "\n",
    "The approach we'll take is to add a few extra layers to the end of resnet50 eventually reducing the number of outputs to 13.\n",
    "\n",
    "### Defining the Model\n",
    "\n",
    "Let's define a class for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_labels:int) -> None:\n",
    "        \"\"\"\n",
    "        Takes in one argument, num_labels which will the dimension of the output of the final layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load resnet50 into the object.\n",
    "        self.resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze all the layers, by setting 'requires_grad = False'.\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the final layer of renset50.\n",
    "\n",
    "        for param in self.resnet50.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Adjoin some extra layers to the end of the model\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            self.resnet50,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, num_labels),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Standard forward method.\n",
    "        \"\"\"\n",
    "        X = X.float()\n",
    "        X = self.layers(X)\n",
    "        return X\n",
    "    \n",
    "    def forward_no_grad(self, X):\n",
    "        \"\"\"\n",
    "        Define a forward method which does not use grad, to speed up computaion when not training the model.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            X = X.float()\n",
    "            X = self.layers(X)\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the class is set up, we can create an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50.conv1.weight False\n",
      "resnet50.bn1.weight False\n",
      "resnet50.bn1.bias False\n",
      "resnet50.layer1.0.conv1.weight False\n",
      "resnet50.layer1.0.bn1.weight False\n",
      "resnet50.layer1.0.bn1.bias False\n",
      "resnet50.layer1.0.conv2.weight False\n",
      "resnet50.layer1.0.bn2.weight False\n",
      "resnet50.layer1.0.bn2.bias False\n",
      "resnet50.layer1.0.conv3.weight False\n",
      "resnet50.layer1.0.bn3.weight False\n",
      "resnet50.layer1.0.bn3.bias False\n",
      "resnet50.layer1.0.downsample.0.weight False\n",
      "resnet50.layer1.0.downsample.1.weight False\n",
      "resnet50.layer1.0.downsample.1.bias False\n",
      "resnet50.layer1.1.conv1.weight False\n",
      "resnet50.layer1.1.bn1.weight False\n",
      "resnet50.layer1.1.bn1.bias False\n",
      "resnet50.layer1.1.conv2.weight False\n",
      "resnet50.layer1.1.bn2.weight False\n",
      "resnet50.layer1.1.bn2.bias False\n",
      "resnet50.layer1.1.conv3.weight False\n",
      "resnet50.layer1.1.bn3.weight False\n",
      "resnet50.layer1.1.bn3.bias False\n",
      "resnet50.layer1.2.conv1.weight False\n",
      "resnet50.layer1.2.bn1.weight False\n",
      "resnet50.layer1.2.bn1.bias False\n",
      "resnet50.layer1.2.conv2.weight False\n",
      "resnet50.layer1.2.bn2.weight False\n",
      "resnet50.layer1.2.bn2.bias False\n",
      "resnet50.layer1.2.conv3.weight False\n",
      "resnet50.layer1.2.bn3.weight False\n",
      "resnet50.layer1.2.bn3.bias False\n",
      "resnet50.layer2.0.conv1.weight False\n",
      "resnet50.layer2.0.bn1.weight False\n",
      "resnet50.layer2.0.bn1.bias False\n",
      "resnet50.layer2.0.conv2.weight False\n",
      "resnet50.layer2.0.bn2.weight False\n",
      "resnet50.layer2.0.bn2.bias False\n",
      "resnet50.layer2.0.conv3.weight False\n",
      "resnet50.layer2.0.bn3.weight False\n",
      "resnet50.layer2.0.bn3.bias False\n",
      "resnet50.layer2.0.downsample.0.weight False\n",
      "resnet50.layer2.0.downsample.1.weight False\n",
      "resnet50.layer2.0.downsample.1.bias False\n",
      "resnet50.layer2.1.conv1.weight False\n",
      "resnet50.layer2.1.bn1.weight False\n",
      "resnet50.layer2.1.bn1.bias False\n",
      "resnet50.layer2.1.conv2.weight False\n",
      "resnet50.layer2.1.bn2.weight False\n",
      "resnet50.layer2.1.bn2.bias False\n",
      "resnet50.layer2.1.conv3.weight False\n",
      "resnet50.layer2.1.bn3.weight False\n",
      "resnet50.layer2.1.bn3.bias False\n",
      "resnet50.layer2.2.conv1.weight False\n",
      "resnet50.layer2.2.bn1.weight False\n",
      "resnet50.layer2.2.bn1.bias False\n",
      "resnet50.layer2.2.conv2.weight False\n",
      "resnet50.layer2.2.bn2.weight False\n",
      "resnet50.layer2.2.bn2.bias False\n",
      "resnet50.layer2.2.conv3.weight False\n",
      "resnet50.layer2.2.bn3.weight False\n",
      "resnet50.layer2.2.bn3.bias False\n",
      "resnet50.layer2.3.conv1.weight False\n",
      "resnet50.layer2.3.bn1.weight False\n",
      "resnet50.layer2.3.bn1.bias False\n",
      "resnet50.layer2.3.conv2.weight False\n",
      "resnet50.layer2.3.bn2.weight False\n",
      "resnet50.layer2.3.bn2.bias False\n",
      "resnet50.layer2.3.conv3.weight False\n",
      "resnet50.layer2.3.bn3.weight False\n",
      "resnet50.layer2.3.bn3.bias False\n",
      "resnet50.layer3.0.conv1.weight False\n",
      "resnet50.layer3.0.bn1.weight False\n",
      "resnet50.layer3.0.bn1.bias False\n",
      "resnet50.layer3.0.conv2.weight False\n",
      "resnet50.layer3.0.bn2.weight False\n",
      "resnet50.layer3.0.bn2.bias False\n",
      "resnet50.layer3.0.conv3.weight False\n",
      "resnet50.layer3.0.bn3.weight False\n",
      "resnet50.layer3.0.bn3.bias False\n",
      "resnet50.layer3.0.downsample.0.weight False\n",
      "resnet50.layer3.0.downsample.1.weight False\n",
      "resnet50.layer3.0.downsample.1.bias False\n",
      "resnet50.layer3.1.conv1.weight False\n",
      "resnet50.layer3.1.bn1.weight False\n",
      "resnet50.layer3.1.bn1.bias False\n",
      "resnet50.layer3.1.conv2.weight False\n",
      "resnet50.layer3.1.bn2.weight False\n",
      "resnet50.layer3.1.bn2.bias False\n",
      "resnet50.layer3.1.conv3.weight False\n",
      "resnet50.layer3.1.bn3.weight False\n",
      "resnet50.layer3.1.bn3.bias False\n",
      "resnet50.layer3.2.conv1.weight False\n",
      "resnet50.layer3.2.bn1.weight False\n",
      "resnet50.layer3.2.bn1.bias False\n",
      "resnet50.layer3.2.conv2.weight False\n",
      "resnet50.layer3.2.bn2.weight False\n",
      "resnet50.layer3.2.bn2.bias False\n",
      "resnet50.layer3.2.conv3.weight False\n",
      "resnet50.layer3.2.bn3.weight False\n",
      "resnet50.layer3.2.bn3.bias False\n",
      "resnet50.layer3.3.conv1.weight False\n",
      "resnet50.layer3.3.bn1.weight False\n",
      "resnet50.layer3.3.bn1.bias False\n",
      "resnet50.layer3.3.conv2.weight False\n",
      "resnet50.layer3.3.bn2.weight False\n",
      "resnet50.layer3.3.bn2.bias False\n",
      "resnet50.layer3.3.conv3.weight False\n",
      "resnet50.layer3.3.bn3.weight False\n",
      "resnet50.layer3.3.bn3.bias False\n",
      "resnet50.layer3.4.conv1.weight False\n",
      "resnet50.layer3.4.bn1.weight False\n",
      "resnet50.layer3.4.bn1.bias False\n",
      "resnet50.layer3.4.conv2.weight False\n",
      "resnet50.layer3.4.bn2.weight False\n",
      "resnet50.layer3.4.bn2.bias False\n",
      "resnet50.layer3.4.conv3.weight False\n",
      "resnet50.layer3.4.bn3.weight False\n",
      "resnet50.layer3.4.bn3.bias False\n",
      "resnet50.layer3.5.conv1.weight False\n",
      "resnet50.layer3.5.bn1.weight False\n",
      "resnet50.layer3.5.bn1.bias False\n",
      "resnet50.layer3.5.conv2.weight False\n",
      "resnet50.layer3.5.bn2.weight False\n",
      "resnet50.layer3.5.bn2.bias False\n",
      "resnet50.layer3.5.conv3.weight False\n",
      "resnet50.layer3.5.bn3.weight False\n",
      "resnet50.layer3.5.bn3.bias False\n",
      "resnet50.layer4.0.conv1.weight False\n",
      "resnet50.layer4.0.bn1.weight False\n",
      "resnet50.layer4.0.bn1.bias False\n",
      "resnet50.layer4.0.conv2.weight False\n",
      "resnet50.layer4.0.bn2.weight False\n",
      "resnet50.layer4.0.bn2.bias False\n",
      "resnet50.layer4.0.conv3.weight False\n",
      "resnet50.layer4.0.bn3.weight False\n",
      "resnet50.layer4.0.bn3.bias False\n",
      "resnet50.layer4.0.downsample.0.weight False\n",
      "resnet50.layer4.0.downsample.1.weight False\n",
      "resnet50.layer4.0.downsample.1.bias False\n",
      "resnet50.layer4.1.conv1.weight False\n",
      "resnet50.layer4.1.bn1.weight False\n",
      "resnet50.layer4.1.bn1.bias False\n",
      "resnet50.layer4.1.conv2.weight False\n",
      "resnet50.layer4.1.bn2.weight False\n",
      "resnet50.layer4.1.bn2.bias False\n",
      "resnet50.layer4.1.conv3.weight False\n",
      "resnet50.layer4.1.bn3.weight False\n",
      "resnet50.layer4.1.bn3.bias False\n",
      "resnet50.layer4.2.conv1.weight False\n",
      "resnet50.layer4.2.bn1.weight False\n",
      "resnet50.layer4.2.bn1.bias False\n",
      "resnet50.layer4.2.conv2.weight False\n",
      "resnet50.layer4.2.bn2.weight False\n",
      "resnet50.layer4.2.bn2.bias False\n",
      "resnet50.layer4.2.conv3.weight False\n",
      "resnet50.layer4.2.bn3.weight False\n",
      "resnet50.layer4.2.bn3.bias False\n",
      "Grad! resnet50.fc.weight True\n",
      "Grad! resnet50.fc.bias True\n",
      "Grad! layers.2.weight True\n",
      "Grad! layers.2.bias True\n",
      "Grad! layers.5.weight True\n",
      "Grad! layers.5.bias True\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the class with 13 outputs\n",
    "model = ImageClassifier(13)\n",
    "\n",
    "# Load the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Check the layers of newly defined model.\n",
    "show_model_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model behaves as expcted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0808, 0.0932, 0.0819, 0.0857, 0.0667, 0.1166, 0.0578, 0.0741, 0.0798,\n",
       "          0.0629, 0.0852, 0.0493, 0.0661]], device='cuda:0',\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " torch.Size([1, 13]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, label = train_dataset[0]\n",
    "predition = model(feature.cuda().unsqueeze(0))\n",
    "\n",
    "predition, predition.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output has dimension 13, so everything seems to be working correctly.\n",
    "\n",
    "### Training loop\n",
    "\n",
    "The model will be trained using stochastic gradient descent. The training loop is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1, model_name:str = \"My Model\"):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    # Only want the parameters in fc to be updated\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=7, eps=1e-20)\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "    \n",
    "    # Prints an validation score\n",
    "    print(f\"Initial validation accuracy score{accuracy_score_from_valiadation(model, validation_loader)}\")\n",
    "\n",
    "    #Create a dictionary to store the best model parameters\n",
    "    best_model_parameters = {'Epoch':-1, 'Accuracy':0, 'Parameters':model.state_dict()}\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "            \n",
    "            if torch.is_tensor(features):\n",
    "                features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = F.cross_entropy(model(features), labels)\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "        \n",
    "        # Print the validation loss\n",
    "        print('Calculating validation accuracy')\n",
    "        accuracy = accuracy_score_from_valiadation(model, validation_loader)\n",
    "        print(f\"Epoch {epoch}, validation accuracy score{accuracy}\")\n",
    "\n",
    "        # Check if the model has the best perfomrance and save the parameters to 'best_model.pt'\n",
    "        if accuracy > best_model_parameters['Accuracy']:\n",
    "            best_model_parameters['Epoch'] = epoch\n",
    "            best_model_parameters['Accuracy'] = loss.item()\n",
    "            best_model_parameters['Parameters'] = model.state_dict()\n",
    "            torch.save(model.state_dict(), f'model_evaluation/weights/{model_name}_best_model.pt')\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Saving model parameters!\")\n",
    "            # Create an instance of the datetime class\n",
    "            dt = datetime.now()\n",
    "            date_stamp = str(dt).replace(':', '_').replace('.', '_').replace(' ', '_')\n",
    "\n",
    "            # Save the model parameters to the folder 'model_evaluation/weights', along with the time and epoch they were generated\n",
    "            torch.save(model.state_dict(), f'model_evaluation/weights/{model_name}_model_{date_stamp}_epoch_{epoch}_accuracy_{accuracy}.pt') # Is there a better way to do this?\n",
    "    \n",
    "    print('Loading best model')\n",
    "    \n",
    "    #Update model parameters with the best model parameters:\n",
    "    model.load_state_dict(best_model_parameters['Parameters'])\n",
    "    print(f'The best model has validation accuracy {accuracy_score_from_valiadation(model, validation_loader)}')\n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy using the WHOLE of the validation dataset.\n",
    "    \"\"\"\n",
    "    predictions = torch.zeros(0).to(device)\n",
    "    labels = torch.zeros(0).to(device)\n",
    "\n",
    "    for batch_index, batch in enumerate(validation_loader):\n",
    "        features, target = batch\n",
    "        target.to(device)\n",
    "        if torch.is_tensor(features):\n",
    "            features = features.to(device)\n",
    "        model.to(device)\n",
    "        target = target.to(device)\n",
    "        predictions = torch.cat((predictions, model(features).max(dim = 1).indices))\n",
    "        labels = torch.cat((labels, target))\n",
    "    \n",
    "    accuracy_score = torch.sum(predictions == labels) / len(predictions)\n",
    "\n",
    "    return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score0.07851562649011612\n",
      "Epoch: 0, batch index: 0, learning rate: [1], loss:2.566716432571411\n",
      "Epoch: 0, batch index: 1, learning rate: [1], loss:2.555245876312256\n",
      "Epoch: 0, batch index: 2, learning rate: [1], loss:2.56062650680542\n",
      "Epoch: 0, batch index: 3, learning rate: [1], loss:2.533644914627075\n",
      "Epoch: 0, batch index: 4, learning rate: [1], loss:2.4867658615112305\n",
      "Epoch: 0, batch index: 5, learning rate: [1], loss:2.467845916748047\n",
      "Epoch: 0, batch index: 6, learning rate: [1], loss:2.5706679821014404\n",
      "Epoch: 0, batch index: 7, learning rate: [1], loss:2.470830202102661\n",
      "Epoch: 0, batch index: 8, learning rate: [1], loss:2.4685916900634766\n",
      "Epoch: 0, batch index: 9, learning rate: [1], loss:2.465259075164795\n",
      "Epoch: 0, batch index: 10, learning rate: [1], loss:2.5014355182647705\n",
      "Epoch: 0, batch index: 11, learning rate: [1], loss:2.463172674179077\n",
      "Epoch: 0, batch index: 12, learning rate: [1], loss:2.5170798301696777\n",
      "Epoch: 0, batch index: 13, learning rate: [1], loss:2.4926252365112305\n",
      "Epoch: 0, batch index: 14, learning rate: [1], loss:2.445845127105713\n",
      "Epoch: 0, batch index: 15, learning rate: [1], loss:2.556363105773926\n",
      "Epoch: 0, batch index: 16, learning rate: [1], loss:2.4390060901641846\n",
      "Epoch: 0, batch index: 17, learning rate: [1], loss:2.471019983291626\n",
      "Epoch: 0, batch index: 18, learning rate: [1], loss:2.4154000282287598\n",
      "Epoch: 0, batch index: 19, learning rate: [1], loss:2.4177751541137695\n",
      "Epoch: 0, batch index: 20, learning rate: [1], loss:2.4292497634887695\n",
      "Epoch: 0, batch index: 21, learning rate: [1], loss:2.433408737182617\n",
      "Epoch: 0, batch index: 22, learning rate: [1], loss:2.4211363792419434\n",
      "Epoch: 0, batch index: 23, learning rate: [1], loss:2.439390182495117\n",
      "Epoch: 0, batch index: 24, learning rate: [1], loss:2.3688204288482666\n",
      "Epoch: 0, batch index: 25, learning rate: [1], loss:2.4214937686920166\n",
      "Epoch: 0, batch index: 26, learning rate: [1], loss:2.4682068824768066\n",
      "Epoch: 0, batch index: 27, learning rate: [1], loss:2.4247286319732666\n",
      "Epoch: 0, batch index: 28, learning rate: [1], loss:2.389260768890381\n",
      "Epoch: 0, batch index: 29, learning rate: [1], loss:2.4264883995056152\n",
      "Epoch: 0, batch index: 30, learning rate: [1], loss:2.416438341140747\n",
      "Epoch: 0, batch index: 31, learning rate: [1], loss:2.475367784500122\n",
      "Epoch: 0, batch index: 32, learning rate: [1], loss:2.4883170127868652\n",
      "Epoch: 0, batch index: 33, learning rate: [1], loss:2.389066457748413\n",
      "Epoch: 0, batch index: 34, learning rate: [1], loss:2.4380946159362793\n",
      "Epoch: 0, batch index: 35, learning rate: [1], loss:2.4042725563049316\n",
      "Epoch: 0, batch index: 36, learning rate: [1], loss:2.4727401733398438\n",
      "Epoch: 0, batch index: 37, learning rate: [1], loss:2.474487781524658\n",
      "Epoch: 0, batch index: 38, learning rate: [1], loss:2.4961464405059814\n",
      "Epoch: 0, batch index: 39, learning rate: [1], loss:2.357668876647949\n",
      "Epoch: 0, batch index: 40, learning rate: [1], loss:2.452119827270508\n",
      "Epoch: 0, batch index: 41, learning rate: [1], loss:2.4348456859588623\n",
      "Epoch: 0, batch index: 42, learning rate: [1], loss:2.3893675804138184\n",
      "Epoch: 0, batch index: 43, learning rate: [1], loss:2.4738211631774902\n",
      "Epoch: 0, batch index: 44, learning rate: [1], loss:2.418915033340454\n",
      "Epoch: 0, batch index: 45, learning rate: [1], loss:2.4034159183502197\n",
      "Epoch: 0, batch index: 46, learning rate: [1], loss:2.362574338912964\n",
      "Epoch: 0, batch index: 47, learning rate: [1], loss:2.4477007389068604\n",
      "Epoch: 0, batch index: 48, learning rate: [1], loss:2.381803274154663\n",
      "Epoch: 0, batch index: 49, learning rate: [1], loss:2.3728833198547363\n",
      "Epoch: 0, batch index: 50, learning rate: [1], loss:2.3990423679351807\n",
      "Epoch: 0, batch index: 51, learning rate: [1], loss:2.4151551723480225\n",
      "Epoch: 0, batch index: 52, learning rate: [1], loss:2.3947954177856445\n",
      "Epoch: 0, batch index: 53, learning rate: [1], loss:2.378067970275879\n",
      "Epoch: 0, batch index: 54, learning rate: [1], loss:2.3834986686706543\n",
      "Epoch: 0, batch index: 55, learning rate: [1], loss:2.3120858669281006\n",
      "Epoch: 0, batch index: 56, learning rate: [1], loss:2.3617963790893555\n",
      "Epoch: 0, batch index: 57, learning rate: [1], loss:2.364145278930664\n",
      "Epoch: 0, batch index: 58, learning rate: [1], loss:2.4097325801849365\n",
      "Epoch: 0, batch index: 59, learning rate: [1], loss:2.332458019256592\n",
      "Epoch: 0, batch index: 60, learning rate: [1], loss:2.339129686355591\n",
      "Epoch: 0, batch index: 61, learning rate: [1], loss:2.4016706943511963\n",
      "Epoch: 0, batch index: 62, learning rate: [1], loss:2.3830325603485107\n",
      "Epoch: 0, batch index: 63, learning rate: [1], loss:2.341756820678711\n",
      "Epoch: 0, batch index: 64, learning rate: [1], loss:2.37980055809021\n",
      "Epoch: 0, batch index: 65, learning rate: [1], loss:2.352243423461914\n",
      "Epoch: 0, batch index: 66, learning rate: [1], loss:2.3735101222991943\n",
      "Epoch: 0, batch index: 67, learning rate: [1], loss:2.410688638687134\n",
      "Epoch: 0, batch index: 68, learning rate: [1], loss:2.347496509552002\n",
      "Epoch: 0, batch index: 69, learning rate: [1], loss:2.377195358276367\n",
      "Epoch: 0, batch index: 70, learning rate: [1], loss:2.33431339263916\n",
      "Epoch: 0, batch index: 71, learning rate: [1], loss:2.453782558441162\n",
      "Epoch: 0, batch index: 72, learning rate: [1], loss:2.4042561054229736\n",
      "Epoch: 0, batch index: 73, learning rate: [1], loss:2.429142713546753\n",
      "Epoch: 0, batch index: 74, learning rate: [1], loss:2.307941436767578\n",
      "Epoch: 0, batch index: 75, learning rate: [1], loss:2.387542724609375\n",
      "Epoch: 0, batch index: 76, learning rate: [1], loss:2.3076517581939697\n",
      "Epoch: 0, batch index: 77, learning rate: [1], loss:2.2977817058563232\n",
      "Epoch: 0, batch index: 78, learning rate: [1], loss:2.3439018726348877\n",
      "Calculating validation accuracy\n",
      "Epoch 0, validation accuracy score0.314453125\n",
      "Saving model parameters!\n",
      "Epoch: 1, batch index: 79, learning rate: [1], loss:2.3626179695129395\n",
      "Epoch: 1, batch index: 80, learning rate: [1], loss:2.3840742111206055\n",
      "Epoch: 1, batch index: 81, learning rate: [1], loss:2.306227922439575\n",
      "Epoch: 1, batch index: 82, learning rate: [1], loss:2.315048933029175\n",
      "Epoch: 1, batch index: 83, learning rate: [1], loss:2.352046489715576\n",
      "Epoch: 1, batch index: 84, learning rate: [1], loss:2.3955085277557373\n",
      "Epoch: 1, batch index: 85, learning rate: [1], loss:2.4199440479278564\n",
      "Epoch: 1, batch index: 86, learning rate: [1], loss:2.370251417160034\n",
      "Epoch: 1, batch index: 87, learning rate: [1], loss:2.3409035205841064\n",
      "Epoch: 1, batch index: 88, learning rate: [1], loss:2.3329789638519287\n",
      "Epoch: 1, batch index: 89, learning rate: [1], loss:2.3433163166046143\n",
      "Epoch: 1, batch index: 90, learning rate: [1], loss:2.407975196838379\n",
      "Epoch: 1, batch index: 91, learning rate: [1], loss:2.3583829402923584\n",
      "Epoch: 1, batch index: 92, learning rate: [1], loss:2.318495988845825\n",
      "Epoch: 1, batch index: 93, learning rate: [1], loss:2.403972864151001\n",
      "Epoch: 1, batch index: 94, learning rate: [1], loss:2.354947090148926\n",
      "Epoch: 1, batch index: 95, learning rate: [1], loss:2.3297512531280518\n",
      "Epoch: 1, batch index: 96, learning rate: [1], loss:2.3591206073760986\n",
      "Epoch: 1, batch index: 97, learning rate: [1], loss:2.386247158050537\n",
      "Epoch: 1, batch index: 98, learning rate: [1], loss:2.374835729598999\n",
      "Epoch: 1, batch index: 99, learning rate: [1], loss:2.328213930130005\n",
      "Epoch: 1, batch index: 100, learning rate: [1], loss:2.310215711593628\n",
      "Epoch: 1, batch index: 101, learning rate: [1], loss:2.3817801475524902\n",
      "Epoch: 1, batch index: 102, learning rate: [1], loss:2.3265268802642822\n",
      "Epoch: 1, batch index: 103, learning rate: [1], loss:2.3541293144226074\n",
      "Epoch: 1, batch index: 104, learning rate: [1], loss:2.289511203765869\n",
      "Epoch: 1, batch index: 105, learning rate: [1], loss:2.274637222290039\n",
      "Epoch: 1, batch index: 106, learning rate: [1], loss:2.2669849395751953\n",
      "Epoch: 1, batch index: 107, learning rate: [1], loss:2.3535900115966797\n",
      "Epoch: 1, batch index: 108, learning rate: [1], loss:2.2734265327453613\n",
      "Epoch: 1, batch index: 109, learning rate: [1], loss:2.383812189102173\n",
      "Epoch: 1, batch index: 110, learning rate: [1], loss:2.399632215499878\n",
      "Epoch: 1, batch index: 111, learning rate: [1], loss:2.2893166542053223\n",
      "Epoch: 1, batch index: 112, learning rate: [1], loss:2.3953640460968018\n",
      "Epoch: 1, batch index: 113, learning rate: [1], loss:2.3442182540893555\n",
      "Epoch: 1, batch index: 114, learning rate: [1], loss:2.374239206314087\n",
      "Epoch: 1, batch index: 115, learning rate: [1], loss:2.3186492919921875\n",
      "Epoch: 1, batch index: 116, learning rate: [1], loss:2.4028749465942383\n",
      "Epoch: 1, batch index: 117, learning rate: [1], loss:2.2110774517059326\n",
      "Epoch: 1, batch index: 118, learning rate: [1], loss:2.382949113845825\n",
      "Epoch: 1, batch index: 119, learning rate: [1], loss:2.398524045944214\n",
      "Epoch: 1, batch index: 120, learning rate: [1], loss:2.3424928188323975\n",
      "Epoch: 1, batch index: 121, learning rate: [1], loss:2.3770103454589844\n",
      "Epoch: 1, batch index: 122, learning rate: [1], loss:2.2221851348876953\n",
      "Epoch: 1, batch index: 123, learning rate: [1], loss:2.292807102203369\n",
      "Epoch: 1, batch index: 124, learning rate: [1], loss:2.2920970916748047\n",
      "Epoch: 1, batch index: 125, learning rate: [1], loss:2.338271141052246\n",
      "Epoch: 1, batch index: 126, learning rate: [1], loss:2.371192216873169\n",
      "Epoch: 1, batch index: 127, learning rate: [1], loss:2.31113338470459\n",
      "Epoch: 1, batch index: 128, learning rate: [1], loss:2.2972207069396973\n",
      "Epoch: 1, batch index: 129, learning rate: [1], loss:2.361337423324585\n",
      "Epoch: 1, batch index: 130, learning rate: [1], loss:2.318544387817383\n",
      "Epoch: 1, batch index: 131, learning rate: [1], loss:2.3549396991729736\n",
      "Epoch: 1, batch index: 132, learning rate: [1], loss:2.283306360244751\n",
      "Epoch: 1, batch index: 133, learning rate: [1], loss:2.2765843868255615\n",
      "Epoch: 1, batch index: 134, learning rate: [1], loss:2.301436185836792\n",
      "Epoch: 1, batch index: 135, learning rate: [1], loss:2.301100730895996\n",
      "Epoch: 1, batch index: 136, learning rate: [1], loss:2.3674569129943848\n",
      "Epoch: 1, batch index: 137, learning rate: [1], loss:2.3694143295288086\n",
      "Epoch: 1, batch index: 138, learning rate: [1], loss:2.2989249229431152\n",
      "Epoch: 1, batch index: 139, learning rate: [1], loss:2.3314268589019775\n",
      "Epoch: 1, batch index: 140, learning rate: [1], loss:2.3885385990142822\n",
      "Epoch: 1, batch index: 141, learning rate: [1], loss:2.3035695552825928\n",
      "Epoch: 1, batch index: 142, learning rate: [1], loss:2.294034957885742\n",
      "Epoch: 1, batch index: 143, learning rate: [1], loss:2.2999839782714844\n",
      "Epoch: 1, batch index: 144, learning rate: [1], loss:2.2220282554626465\n",
      "Epoch: 1, batch index: 145, learning rate: [1], loss:2.3337717056274414\n",
      "Epoch: 1, batch index: 146, learning rate: [1], loss:2.320828437805176\n",
      "Epoch: 1, batch index: 147, learning rate: [1], loss:2.3298707008361816\n",
      "Epoch: 1, batch index: 148, learning rate: [1], loss:2.330587387084961\n",
      "Epoch: 1, batch index: 149, learning rate: [1], loss:2.32814359664917\n",
      "Epoch: 1, batch index: 150, learning rate: [1], loss:2.3449859619140625\n",
      "Epoch: 1, batch index: 151, learning rate: [1], loss:2.2792723178863525\n",
      "Epoch: 1, batch index: 152, learning rate: [1], loss:2.319807291030884\n",
      "Epoch: 1, batch index: 153, learning rate: [1], loss:2.430462598800659\n",
      "Epoch: 1, batch index: 154, learning rate: [1], loss:2.2525699138641357\n",
      "Epoch: 1, batch index: 155, learning rate: [1], loss:2.3115756511688232\n",
      "Epoch: 1, batch index: 156, learning rate: [1], loss:2.419455051422119\n",
      "Epoch: 1, batch index: 157, learning rate: [1], loss:2.2155332565307617\n",
      "Calculating validation accuracy\n",
      "Epoch 1, validation accuracy score0.2914062440395355\n",
      "Epoch: 2, batch index: 158, learning rate: [1], loss:2.4426450729370117\n",
      "Epoch: 2, batch index: 159, learning rate: [1], loss:2.3754265308380127\n",
      "Epoch: 2, batch index: 160, learning rate: [1], loss:2.3143868446350098\n",
      "Epoch: 2, batch index: 161, learning rate: [1], loss:2.3632752895355225\n",
      "Epoch: 2, batch index: 162, learning rate: [1], loss:2.39072585105896\n",
      "Epoch: 2, batch index: 163, learning rate: [1], loss:2.350698471069336\n",
      "Epoch: 2, batch index: 164, learning rate: [1], loss:2.3784210681915283\n",
      "Epoch: 2, batch index: 165, learning rate: [1], loss:2.39011812210083\n",
      "Epoch: 2, batch index: 166, learning rate: [1], loss:2.3486883640289307\n",
      "Epoch: 2, batch index: 167, learning rate: [1], loss:2.2867023944854736\n",
      "Epoch: 2, batch index: 168, learning rate: [1], loss:2.2972166538238525\n",
      "Epoch: 2, batch index: 169, learning rate: [0.1], loss:2.3055849075317383\n",
      "Epoch: 2, batch index: 170, learning rate: [0.1], loss:2.3617019653320312\n",
      "Epoch: 2, batch index: 171, learning rate: [0.1], loss:2.30698823928833\n",
      "Epoch: 2, batch index: 172, learning rate: [0.1], loss:2.375284194946289\n",
      "Epoch: 2, batch index: 173, learning rate: [0.1], loss:2.375667095184326\n",
      "Epoch: 2, batch index: 174, learning rate: [0.1], loss:2.2704687118530273\n",
      "Epoch: 2, batch index: 175, learning rate: [0.1], loss:2.291841745376587\n",
      "Epoch: 2, batch index: 176, learning rate: [0.1], loss:2.3332486152648926\n",
      "Epoch: 2, batch index: 177, learning rate: [0.1], loss:2.2644505500793457\n",
      "Epoch: 2, batch index: 178, learning rate: [0.1], loss:2.299668073654175\n",
      "Epoch: 2, batch index: 179, learning rate: [0.1], loss:2.2615935802459717\n",
      "Epoch: 2, batch index: 180, learning rate: [0.1], loss:2.3401684761047363\n",
      "Epoch: 2, batch index: 181, learning rate: [0.1], loss:2.2476954460144043\n",
      "Epoch: 2, batch index: 182, learning rate: [0.1], loss:2.2763278484344482\n",
      "Epoch: 2, batch index: 183, learning rate: [0.1], loss:2.2532784938812256\n",
      "Epoch: 2, batch index: 184, learning rate: [0.1], loss:2.2607264518737793\n",
      "Epoch: 2, batch index: 185, learning rate: [0.1], loss:2.3156747817993164\n",
      "Epoch: 2, batch index: 186, learning rate: [0.1], loss:2.252532482147217\n",
      "Epoch: 2, batch index: 187, learning rate: [0.1], loss:2.271717071533203\n",
      "Epoch: 2, batch index: 188, learning rate: [0.1], loss:2.276489496231079\n",
      "Epoch: 2, batch index: 189, learning rate: [0.1], loss:2.345248222351074\n",
      "Epoch: 2, batch index: 190, learning rate: [0.1], loss:2.2812860012054443\n",
      "Epoch: 2, batch index: 191, learning rate: [0.1], loss:2.2282090187072754\n",
      "Epoch: 2, batch index: 192, learning rate: [0.1], loss:2.2789511680603027\n",
      "Epoch: 2, batch index: 193, learning rate: [0.1], loss:2.2615840435028076\n",
      "Epoch: 2, batch index: 194, learning rate: [0.1], loss:2.3860936164855957\n",
      "Epoch: 2, batch index: 195, learning rate: [0.1], loss:2.257550001144409\n",
      "Epoch: 2, batch index: 196, learning rate: [0.1], loss:2.294153928756714\n",
      "Epoch: 2, batch index: 197, learning rate: [0.1], loss:2.2669553756713867\n",
      "Epoch: 2, batch index: 198, learning rate: [0.1], loss:2.222510814666748\n",
      "Epoch: 2, batch index: 199, learning rate: [0.1], loss:2.3600456714630127\n",
      "Epoch: 2, batch index: 200, learning rate: [0.1], loss:2.285905361175537\n",
      "Epoch: 2, batch index: 201, learning rate: [0.1], loss:2.217069387435913\n",
      "Epoch: 2, batch index: 202, learning rate: [0.1], loss:2.3319873809814453\n",
      "Epoch: 2, batch index: 203, learning rate: [0.1], loss:2.304828643798828\n",
      "Epoch: 2, batch index: 204, learning rate: [0.1], loss:2.3302109241485596\n",
      "Epoch: 2, batch index: 205, learning rate: [0.1], loss:2.3703453540802\n",
      "Epoch: 2, batch index: 206, learning rate: [0.1], loss:2.3145458698272705\n",
      "Epoch: 2, batch index: 207, learning rate: [0.1], loss:2.2805612087249756\n",
      "Epoch: 2, batch index: 208, learning rate: [0.1], loss:2.2948367595672607\n",
      "Epoch: 2, batch index: 209, learning rate: [0.1], loss:2.30330491065979\n",
      "Epoch: 2, batch index: 210, learning rate: [0.1], loss:2.297642707824707\n",
      "Epoch: 2, batch index: 211, learning rate: [0.1], loss:2.2514376640319824\n",
      "Epoch: 2, batch index: 212, learning rate: [0.1], loss:2.240220308303833\n",
      "Epoch: 2, batch index: 213, learning rate: [0.1], loss:2.2871081829071045\n",
      "Epoch: 2, batch index: 214, learning rate: [0.1], loss:2.2672667503356934\n",
      "Epoch: 2, batch index: 215, learning rate: [0.1], loss:2.3309550285339355\n",
      "Epoch: 2, batch index: 216, learning rate: [0.1], loss:2.2908663749694824\n",
      "Epoch: 2, batch index: 217, learning rate: [0.1], loss:2.402336597442627\n",
      "Epoch: 2, batch index: 218, learning rate: [0.1], loss:2.3141915798187256\n",
      "Epoch: 2, batch index: 219, learning rate: [0.1], loss:2.3097620010375977\n",
      "Epoch: 2, batch index: 220, learning rate: [0.1], loss:2.238334894180298\n",
      "Epoch: 2, batch index: 221, learning rate: [0.1], loss:2.3420350551605225\n",
      "Epoch: 2, batch index: 222, learning rate: [0.1], loss:2.316060781478882\n",
      "Epoch: 2, batch index: 223, learning rate: [0.1], loss:2.2458560466766357\n",
      "Epoch: 2, batch index: 224, learning rate: [0.1], loss:2.2659895420074463\n",
      "Epoch: 2, batch index: 225, learning rate: [0.1], loss:2.2729735374450684\n",
      "Epoch: 2, batch index: 226, learning rate: [0.1], loss:2.2776715755462646\n",
      "Epoch: 2, batch index: 227, learning rate: [0.010000000000000002], loss:2.3154022693634033\n",
      "Epoch: 2, batch index: 228, learning rate: [0.010000000000000002], loss:2.3063738346099854\n",
      "Epoch: 2, batch index: 229, learning rate: [0.010000000000000002], loss:2.311436653137207\n",
      "Epoch: 2, batch index: 230, learning rate: [0.010000000000000002], loss:2.342838764190674\n",
      "Epoch: 2, batch index: 231, learning rate: [0.010000000000000002], loss:2.2548112869262695\n",
      "Epoch: 2, batch index: 232, learning rate: [0.010000000000000002], loss:2.3181591033935547\n",
      "Epoch: 2, batch index: 233, learning rate: [0.010000000000000002], loss:2.2428770065307617\n",
      "Epoch: 2, batch index: 234, learning rate: [0.010000000000000002], loss:2.3434581756591797\n",
      "Epoch: 2, batch index: 235, learning rate: [0.010000000000000002], loss:2.3327739238739014\n",
      "Epoch: 2, batch index: 236, learning rate: [0.010000000000000002], loss:2.1948347091674805\n",
      "Calculating validation accuracy\n",
      "Epoch 2, validation accuracy score0.41679689288139343\n",
      "Epoch: 3, batch index: 237, learning rate: [0.010000000000000002], loss:2.282928943634033\n",
      "Epoch: 3, batch index: 238, learning rate: [0.010000000000000002], loss:2.187319040298462\n",
      "Epoch: 3, batch index: 239, learning rate: [0.010000000000000002], loss:2.2074167728424072\n",
      "Epoch: 3, batch index: 240, learning rate: [0.010000000000000002], loss:2.2925775051116943\n",
      "Epoch: 3, batch index: 241, learning rate: [0.010000000000000002], loss:2.276775360107422\n",
      "Epoch: 3, batch index: 242, learning rate: [0.010000000000000002], loss:2.3525638580322266\n",
      "Epoch: 3, batch index: 243, learning rate: [0.010000000000000002], loss:2.2357635498046875\n",
      "Epoch: 3, batch index: 244, learning rate: [0.010000000000000002], loss:2.2788236141204834\n",
      "Epoch: 3, batch index: 245, learning rate: [0.010000000000000002], loss:2.254129409790039\n",
      "Epoch: 3, batch index: 246, learning rate: [0.010000000000000002], loss:2.3284175395965576\n",
      "Epoch: 3, batch index: 247, learning rate: [0.010000000000000002], loss:2.2975423336029053\n",
      "Epoch: 3, batch index: 248, learning rate: [0.010000000000000002], loss:2.29561185836792\n",
      "Epoch: 3, batch index: 249, learning rate: [0.010000000000000002], loss:2.3287434577941895\n",
      "Epoch: 3, batch index: 250, learning rate: [0.010000000000000002], loss:2.3256359100341797\n",
      "Epoch: 3, batch index: 251, learning rate: [0.010000000000000002], loss:2.2820138931274414\n",
      "Epoch: 3, batch index: 252, learning rate: [0.010000000000000002], loss:2.267918109893799\n",
      "Epoch: 3, batch index: 253, learning rate: [0.010000000000000002], loss:2.244809865951538\n",
      "Epoch: 3, batch index: 254, learning rate: [0.010000000000000002], loss:2.337186574935913\n",
      "Epoch: 3, batch index: 255, learning rate: [0.010000000000000002], loss:2.3111977577209473\n",
      "Epoch: 3, batch index: 256, learning rate: [0.010000000000000002], loss:2.2487287521362305\n",
      "Epoch: 3, batch index: 257, learning rate: [0.010000000000000002], loss:2.3470916748046875\n",
      "Epoch: 3, batch index: 258, learning rate: [0.010000000000000002], loss:2.3163280487060547\n",
      "Epoch: 3, batch index: 259, learning rate: [0.010000000000000002], loss:2.276543617248535\n",
      "Epoch: 3, batch index: 260, learning rate: [0.010000000000000002], loss:2.3188321590423584\n",
      "Epoch: 3, batch index: 261, learning rate: [0.010000000000000002], loss:2.257235288619995\n",
      "Epoch: 3, batch index: 262, learning rate: [0.010000000000000002], loss:2.2736728191375732\n",
      "Epoch: 3, batch index: 263, learning rate: [0.010000000000000002], loss:2.26047682762146\n",
      "Epoch: 3, batch index: 264, learning rate: [0.010000000000000002], loss:2.3266663551330566\n",
      "Epoch: 3, batch index: 265, learning rate: [0.010000000000000002], loss:2.340895414352417\n",
      "Epoch: 3, batch index: 266, learning rate: [0.010000000000000002], loss:2.285637855529785\n",
      "Epoch: 3, batch index: 267, learning rate: [0.010000000000000002], loss:2.26566743850708\n",
      "Epoch: 3, batch index: 268, learning rate: [0.010000000000000002], loss:2.2429747581481934\n",
      "Epoch: 3, batch index: 269, learning rate: [0.010000000000000002], loss:2.2331643104553223\n",
      "Epoch: 3, batch index: 270, learning rate: [0.010000000000000002], loss:2.299409866333008\n",
      "Epoch: 3, batch index: 271, learning rate: [0.010000000000000002], loss:2.3017473220825195\n",
      "Epoch: 3, batch index: 272, learning rate: [0.010000000000000002], loss:2.2998857498168945\n",
      "Epoch: 3, batch index: 273, learning rate: [0.010000000000000002], loss:2.274869441986084\n",
      "Epoch: 3, batch index: 274, learning rate: [0.010000000000000002], loss:2.300400495529175\n",
      "Epoch: 3, batch index: 275, learning rate: [0.010000000000000002], loss:2.3468024730682373\n",
      "Epoch: 3, batch index: 276, learning rate: [0.010000000000000002], loss:2.2267727851867676\n",
      "Epoch: 3, batch index: 277, learning rate: [0.010000000000000002], loss:2.3299412727355957\n",
      "Epoch: 3, batch index: 278, learning rate: [0.010000000000000002], loss:2.2955000400543213\n",
      "Epoch: 3, batch index: 279, learning rate: [0.010000000000000002], loss:2.273693561553955\n",
      "Epoch: 3, batch index: 280, learning rate: [0.010000000000000002], loss:2.26562762260437\n",
      "Epoch: 3, batch index: 281, learning rate: [0.010000000000000002], loss:2.3347678184509277\n",
      "Epoch: 3, batch index: 282, learning rate: [0.010000000000000002], loss:2.3375046253204346\n",
      "Epoch: 3, batch index: 283, learning rate: [0.010000000000000002], loss:2.2642574310302734\n",
      "Epoch: 3, batch index: 284, learning rate: [0.010000000000000002], loss:2.229499101638794\n",
      "Epoch: 3, batch index: 285, learning rate: [0.010000000000000002], loss:2.214318037033081\n",
      "Epoch: 3, batch index: 286, learning rate: [0.010000000000000002], loss:2.2322998046875\n",
      "Epoch: 3, batch index: 287, learning rate: [0.010000000000000002], loss:2.2398927211761475\n",
      "Epoch: 3, batch index: 288, learning rate: [0.010000000000000002], loss:2.350471258163452\n",
      "Epoch: 3, batch index: 289, learning rate: [0.010000000000000002], loss:2.2982845306396484\n",
      "Epoch: 3, batch index: 290, learning rate: [0.0010000000000000002], loss:2.2105319499969482\n",
      "Epoch: 3, batch index: 291, learning rate: [0.0010000000000000002], loss:2.180518865585327\n",
      "Epoch: 3, batch index: 292, learning rate: [0.0010000000000000002], loss:2.296060085296631\n",
      "Epoch: 3, batch index: 293, learning rate: [0.0010000000000000002], loss:2.2946362495422363\n",
      "Epoch: 3, batch index: 294, learning rate: [0.0010000000000000002], loss:2.252197265625\n",
      "Epoch: 3, batch index: 295, learning rate: [0.0010000000000000002], loss:2.3084053993225098\n",
      "Epoch: 3, batch index: 296, learning rate: [0.0010000000000000002], loss:2.270756483078003\n",
      "Epoch: 3, batch index: 297, learning rate: [0.0010000000000000002], loss:2.3209502696990967\n",
      "Epoch: 3, batch index: 298, learning rate: [0.0010000000000000002], loss:2.261209011077881\n",
      "Epoch: 3, batch index: 299, learning rate: [0.0010000000000000002], loss:2.301837205886841\n",
      "Epoch: 3, batch index: 300, learning rate: [0.0010000000000000002], loss:2.2987701892852783\n",
      "Epoch: 3, batch index: 301, learning rate: [0.0010000000000000002], loss:2.312811851501465\n",
      "Epoch: 3, batch index: 302, learning rate: [0.0010000000000000002], loss:2.242924451828003\n",
      "Epoch: 3, batch index: 303, learning rate: [0.0010000000000000002], loss:2.297778844833374\n",
      "Epoch: 3, batch index: 304, learning rate: [0.0010000000000000002], loss:2.2594480514526367\n",
      "Epoch: 3, batch index: 305, learning rate: [0.0010000000000000002], loss:2.2888343334198\n",
      "Epoch: 3, batch index: 306, learning rate: [0.0010000000000000002], loss:2.351980686187744\n",
      "Epoch: 3, batch index: 307, learning rate: [0.0010000000000000002], loss:2.2848262786865234\n",
      "Epoch: 3, batch index: 308, learning rate: [0.0010000000000000002], loss:2.195305109024048\n",
      "Epoch: 3, batch index: 309, learning rate: [0.0010000000000000002], loss:2.2946672439575195\n",
      "Epoch: 3, batch index: 310, learning rate: [0.0010000000000000002], loss:2.1978607177734375\n",
      "Epoch: 3, batch index: 311, learning rate: [0.0010000000000000002], loss:2.3057000637054443\n",
      "Epoch: 3, batch index: 312, learning rate: [0.0010000000000000002], loss:2.3247342109680176\n",
      "Epoch: 3, batch index: 313, learning rate: [0.0010000000000000002], loss:2.2538349628448486\n",
      "Epoch: 3, batch index: 314, learning rate: [0.0010000000000000002], loss:2.216313123703003\n",
      "Epoch: 3, batch index: 315, learning rate: [0.0010000000000000002], loss:2.4430501461029053\n",
      "Calculating validation accuracy\n",
      "Epoch 3, validation accuracy score0.41328126192092896\n",
      "Loading best model\n",
      "The best model has validation accuracy 0.41484376788139343\n"
     ]
    }
   ],
   "source": [
    "# Train the model, uncomment to train\n",
    "train(model, train_loader, validation_loader, epochs=4, learning_rate=1, model_name = 'image_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibs/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ibs/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "Initial validation accuracy score0.0771484375\n",
      "Epoch: 0, batch index: 0, learning rate: [1], loss:2.566009283065796\n",
      "Epoch: 0, batch index: 1, learning rate: [1], loss:2.5624454021453857\n",
      "Epoch: 0, batch index: 2, learning rate: [1], loss:2.5428481101989746\n",
      "Epoch: 0, batch index: 3, learning rate: [1], loss:2.605640411376953\n",
      "Epoch: 0, batch index: 4, learning rate: [1], loss:2.5039119720458984\n",
      "Epoch: 0, batch index: 5, learning rate: [1], loss:2.6784985065460205\n",
      "Epoch: 0, batch index: 6, learning rate: [1], loss:2.535290002822876\n",
      "Epoch: 0, batch index: 7, learning rate: [1], loss:2.5893754959106445\n",
      "Epoch: 0, batch index: 8, learning rate: [1], loss:2.576002359390259\n",
      "Epoch: 0, batch index: 9, learning rate: [1], loss:2.5600507259368896\n",
      "Epoch: 0, batch index: 10, learning rate: [1], loss:2.582350730895996\n",
      "Epoch: 0, batch index: 11, learning rate: [1], loss:2.5469675064086914\n",
      "Epoch: 0, batch index: 12, learning rate: [1], loss:2.601773262023926\n",
      "Epoch: 0, batch index: 13, learning rate: [1], loss:2.5696794986724854\n",
      "Epoch: 0, batch index: 14, learning rate: [1], loss:2.56699800491333\n",
      "Epoch: 0, batch index: 15, learning rate: [1], loss:2.5650129318237305\n",
      "Epoch: 0, batch index: 16, learning rate: [1], loss:2.5745620727539062\n",
      "Epoch: 0, batch index: 17, learning rate: [1], loss:2.5549681186676025\n",
      "Epoch: 0, batch index: 18, learning rate: [1], loss:2.566537618637085\n",
      "Epoch: 0, batch index: 19, learning rate: [1], loss:2.555954694747925\n",
      "Epoch: 0, batch index: 20, learning rate: [1], loss:2.561746835708618\n",
      "Epoch: 0, batch index: 21, learning rate: [1], loss:2.5199413299560547\n",
      "Epoch: 0, batch index: 22, learning rate: [1], loss:2.562345504760742\n",
      "Epoch: 0, batch index: 23, learning rate: [1], loss:2.56093168258667\n",
      "Epoch: 0, batch index: 24, learning rate: [1], loss:2.532283067703247\n",
      "Epoch: 0, batch index: 25, learning rate: [1], loss:2.6149206161499023\n",
      "Epoch: 0, batch index: 26, learning rate: [1], loss:2.572329044342041\n",
      "Epoch: 0, batch index: 27, learning rate: [1], loss:2.5126395225524902\n",
      "Epoch: 0, batch index: 28, learning rate: [1], loss:2.593141794204712\n",
      "Epoch: 0, batch index: 29, learning rate: [1], loss:2.649338722229004\n",
      "Epoch: 0, batch index: 30, learning rate: [1], loss:2.5776078701019287\n",
      "Epoch: 0, batch index: 31, learning rate: [1], loss:2.532763957977295\n",
      "Epoch: 0, batch index: 32, learning rate: [1], loss:2.5869641304016113\n",
      "Epoch: 0, batch index: 33, learning rate: [1], loss:2.5227625370025635\n",
      "Epoch: 0, batch index: 34, learning rate: [1], loss:2.5525002479553223\n",
      "Epoch: 0, batch index: 35, learning rate: [1], loss:2.6029841899871826\n",
      "Epoch: 0, batch index: 36, learning rate: [1], loss:2.553269386291504\n",
      "Epoch: 0, batch index: 37, learning rate: [1], loss:2.5303590297698975\n",
      "Epoch: 0, batch index: 38, learning rate: [1], loss:2.572033643722534\n",
      "Epoch: 0, batch index: 39, learning rate: [1], loss:2.5217607021331787\n",
      "Epoch: 0, batch index: 40, learning rate: [1], loss:2.5142662525177\n",
      "Epoch: 0, batch index: 41, learning rate: [1], loss:2.599651575088501\n",
      "Epoch: 0, batch index: 42, learning rate: [1], loss:2.5529162883758545\n",
      "Epoch: 0, batch index: 43, learning rate: [1], loss:2.5355582237243652\n",
      "Epoch: 0, batch index: 44, learning rate: [1], loss:2.538630962371826\n",
      "Epoch: 0, batch index: 45, learning rate: [1], loss:2.5580227375030518\n",
      "Epoch: 0, batch index: 46, learning rate: [1], loss:2.493253469467163\n",
      "Epoch: 0, batch index: 47, learning rate: [1], loss:2.626682758331299\n",
      "Epoch: 0, batch index: 48, learning rate: [1], loss:2.5394320487976074\n",
      "Epoch: 0, batch index: 49, learning rate: [1], loss:2.5166633129119873\n",
      "Epoch: 0, batch index: 50, learning rate: [1], loss:2.5635228157043457\n",
      "Epoch: 0, batch index: 51, learning rate: [1], loss:2.4904215335845947\n",
      "Epoch: 0, batch index: 52, learning rate: [1], loss:2.5738019943237305\n",
      "Epoch: 0, batch index: 53, learning rate: [1], loss:2.5597078800201416\n",
      "Epoch: 0, batch index: 54, learning rate: [1], loss:2.594895362854004\n",
      "Epoch: 0, batch index: 55, learning rate: [1], loss:2.520770788192749\n",
      "Epoch: 0, batch index: 56, learning rate: [1], loss:2.6126163005828857\n",
      "Epoch: 0, batch index: 57, learning rate: [1], loss:2.567321538925171\n",
      "Epoch: 0, batch index: 58, learning rate: [1], loss:2.5830931663513184\n",
      "Epoch: 0, batch index: 59, learning rate: [1], loss:2.526864528656006\n",
      "Epoch: 0, batch index: 60, learning rate: [1], loss:2.5081331729888916\n",
      "Epoch: 0, batch index: 61, learning rate: [1], loss:2.579270839691162\n",
      "Epoch: 0, batch index: 62, learning rate: [1], loss:2.536895275115967\n",
      "Epoch: 0, batch index: 63, learning rate: [1], loss:2.5538110733032227\n",
      "Epoch: 0, batch index: 64, learning rate: [1], loss:2.5682947635650635\n",
      "Epoch: 0, batch index: 65, learning rate: [1], loss:2.479519844055176\n",
      "Epoch: 0, batch index: 66, learning rate: [1], loss:2.452965021133423\n",
      "Epoch: 0, batch index: 67, learning rate: [1], loss:2.5281994342803955\n",
      "Epoch: 0, batch index: 68, learning rate: [1], loss:2.542435646057129\n",
      "Epoch: 0, batch index: 69, learning rate: [1], loss:2.5439298152923584\n",
      "Epoch: 0, batch index: 70, learning rate: [1], loss:2.5601768493652344\n",
      "Epoch: 0, batch index: 71, learning rate: [1], loss:2.533848285675049\n",
      "Epoch: 0, batch index: 72, learning rate: [1], loss:2.4997060298919678\n",
      "Epoch: 0, batch index: 73, learning rate: [1], loss:2.6093993186950684\n",
      "Epoch: 0, batch index: 74, learning rate: [1], loss:2.462495803833008\n",
      "Epoch: 0, batch index: 75, learning rate: [1], loss:2.5051002502441406\n",
      "Epoch: 0, batch index: 76, learning rate: [1], loss:2.6696648597717285\n",
      "Epoch: 0, batch index: 77, learning rate: [1], loss:2.6011836528778076\n",
      "Epoch: 0, batch index: 78, learning rate: [1], loss:2.5903522968292236\n",
      "Epoch: 0, batch index: 79, learning rate: [1], loss:2.565208911895752\n",
      "Epoch: 0, batch index: 80, learning rate: [1], loss:2.5662808418273926\n",
      "Epoch: 0, batch index: 81, learning rate: [1], loss:2.537126064300537\n",
      "Epoch: 0, batch index: 82, learning rate: [1], loss:2.5251057147979736\n",
      "Epoch: 0, batch index: 83, learning rate: [1], loss:2.517954111099243\n",
      "Epoch: 0, batch index: 84, learning rate: [1], loss:2.575361490249634\n",
      "Epoch: 0, batch index: 85, learning rate: [1], loss:2.600200653076172\n",
      "Epoch: 0, batch index: 86, learning rate: [1], loss:2.5677742958068848\n",
      "Epoch: 0, batch index: 87, learning rate: [1], loss:2.579416036605835\n",
      "Epoch: 0, batch index: 88, learning rate: [1], loss:2.545586585998535\n",
      "Epoch: 0, batch index: 89, learning rate: [1], loss:2.5587940216064453\n",
      "Epoch: 0, batch index: 90, learning rate: [1], loss:2.562167167663574\n",
      "Epoch: 0, batch index: 91, learning rate: [1], loss:2.5542666912078857\n",
      "Epoch: 0, batch index: 92, learning rate: [1], loss:2.5347330570220947\n",
      "Epoch: 0, batch index: 93, learning rate: [1], loss:2.509587526321411\n",
      "Epoch: 0, batch index: 94, learning rate: [1], loss:2.5941877365112305\n",
      "Epoch: 0, batch index: 95, learning rate: [1], loss:2.5795838832855225\n",
      "Epoch: 0, batch index: 96, learning rate: [1], loss:2.5761733055114746\n",
      "Epoch: 0, batch index: 97, learning rate: [1], loss:2.529263734817505\n",
      "Epoch: 0, batch index: 98, learning rate: [1], loss:2.5707204341888428\n",
      "Epoch: 0, batch index: 99, learning rate: [1], loss:2.4617931842803955\n",
      "Epoch: 0, batch index: 100, learning rate: [1], loss:2.526383876800537\n",
      "Epoch: 0, batch index: 101, learning rate: [1], loss:2.4971282482147217\n",
      "Epoch: 0, batch index: 102, learning rate: [1], loss:2.599752902984619\n",
      "Epoch: 0, batch index: 103, learning rate: [1], loss:2.5954298973083496\n",
      "Epoch: 0, batch index: 104, learning rate: [1], loss:2.5321760177612305\n",
      "Epoch: 0, batch index: 105, learning rate: [1], loss:2.5918948650360107\n",
      "Epoch: 0, batch index: 106, learning rate: [1], loss:2.5151185989379883\n",
      "Epoch: 0, batch index: 107, learning rate: [1], loss:2.593743085861206\n",
      "Epoch: 0, batch index: 108, learning rate: [1], loss:2.501185655593872\n",
      "Epoch: 0, batch index: 109, learning rate: [1], loss:2.5603725910186768\n",
      "Epoch: 0, batch index: 110, learning rate: [1], loss:2.4376323223114014\n",
      "Epoch: 0, batch index: 111, learning rate: [1], loss:2.561892032623291\n",
      "Epoch: 0, batch index: 112, learning rate: [1], loss:2.5884146690368652\n",
      "Epoch: 0, batch index: 113, learning rate: [1], loss:2.618542194366455\n",
      "Epoch: 0, batch index: 114, learning rate: [1], loss:2.5200035572052\n",
      "Epoch: 0, batch index: 115, learning rate: [1], loss:2.578430652618408\n",
      "Epoch: 0, batch index: 116, learning rate: [1], loss:2.498363494873047\n",
      "Epoch: 0, batch index: 117, learning rate: [1], loss:2.645111322402954\n",
      "Epoch: 0, batch index: 118, learning rate: [1], loss:2.581310749053955\n",
      "Epoch: 0, batch index: 119, learning rate: [1], loss:2.6016592979431152\n",
      "Epoch: 0, batch index: 120, learning rate: [1], loss:2.501461982727051\n",
      "Epoch: 0, batch index: 121, learning rate: [1], loss:2.5702898502349854\n",
      "Epoch: 0, batch index: 122, learning rate: [1], loss:2.600429058074951\n",
      "Epoch: 0, batch index: 123, learning rate: [1], loss:2.5737521648406982\n",
      "Epoch: 0, batch index: 124, learning rate: [1], loss:2.576901912689209\n",
      "Epoch: 0, batch index: 125, learning rate: [1], loss:2.543173313140869\n",
      "Epoch: 0, batch index: 126, learning rate: [1], loss:2.5537056922912598\n",
      "Epoch: 0, batch index: 127, learning rate: [1], loss:2.5325615406036377\n",
      "Epoch: 0, batch index: 128, learning rate: [1], loss:2.5385613441467285\n",
      "Epoch: 0, batch index: 129, learning rate: [1], loss:2.6088762283325195\n",
      "Epoch: 0, batch index: 130, learning rate: [1], loss:2.5448262691497803\n",
      "Epoch: 0, batch index: 131, learning rate: [1], loss:2.5820345878601074\n",
      "Epoch: 0, batch index: 132, learning rate: [1], loss:2.578199863433838\n",
      "Epoch: 0, batch index: 133, learning rate: [1], loss:2.5626566410064697\n",
      "Epoch: 0, batch index: 134, learning rate: [1], loss:2.554703950881958\n",
      "Epoch: 0, batch index: 135, learning rate: [1], loss:2.552438735961914\n",
      "Epoch: 0, batch index: 136, learning rate: [1], loss:2.551251173019409\n",
      "Epoch: 0, batch index: 137, learning rate: [1], loss:2.5425262451171875\n",
      "Epoch: 0, batch index: 138, learning rate: [1], loss:2.5413930416107178\n",
      "Epoch: 0, batch index: 139, learning rate: [1], loss:2.547914743423462\n",
      "Epoch: 0, batch index: 140, learning rate: [1], loss:2.543215274810791\n",
      "Epoch: 0, batch index: 141, learning rate: [1], loss:2.5730504989624023\n",
      "Epoch: 0, batch index: 142, learning rate: [1], loss:2.5250508785247803\n",
      "Epoch: 0, batch index: 143, learning rate: [1], loss:2.564229965209961\n",
      "Epoch: 0, batch index: 144, learning rate: [1], loss:2.5550496578216553\n",
      "Epoch: 0, batch index: 145, learning rate: [1], loss:2.5808944702148438\n",
      "Epoch: 0, batch index: 146, learning rate: [1], loss:2.5405142307281494\n",
      "Epoch: 0, batch index: 147, learning rate: [1], loss:2.5187408924102783\n",
      "Epoch: 0, batch index: 148, learning rate: [1], loss:2.5405735969543457\n",
      "Epoch: 0, batch index: 149, learning rate: [1], loss:2.563214063644409\n",
      "Epoch: 0, batch index: 150, learning rate: [1], loss:2.52986741065979\n",
      "Epoch: 0, batch index: 151, learning rate: [1], loss:2.5288798809051514\n",
      "Epoch: 0, batch index: 152, learning rate: [1], loss:2.5549302101135254\n",
      "Epoch: 0, batch index: 153, learning rate: [1], loss:2.5673978328704834\n",
      "Epoch: 0, batch index: 154, learning rate: [1], loss:2.5241892337799072\n",
      "Epoch: 0, batch index: 155, learning rate: [1], loss:2.569702625274658\n",
      "Epoch: 0, batch index: 156, learning rate: [1], loss:2.577824115753174\n",
      "Epoch: 0, batch index: 157, learning rate: [1], loss:2.5016870498657227\n",
      "Epoch: 0, batch index: 158, learning rate: [1], loss:2.539698839187622\n",
      "Epoch: 0, batch index: 159, learning rate: [1], loss:2.5252084732055664\n",
      "Epoch: 0, batch index: 160, learning rate: [1], loss:2.601367235183716\n",
      "Epoch: 0, batch index: 161, learning rate: [1], loss:2.573296070098877\n",
      "Epoch: 0, batch index: 162, learning rate: [0.1], loss:2.569042682647705\n",
      "Epoch: 0, batch index: 163, learning rate: [0.1], loss:2.47151780128479\n",
      "Epoch: 0, batch index: 164, learning rate: [0.1], loss:2.4992802143096924\n",
      "Epoch: 0, batch index: 165, learning rate: [0.1], loss:2.5294430255889893\n",
      "Epoch: 0, batch index: 166, learning rate: [0.1], loss:2.572949171066284\n",
      "Epoch: 0, batch index: 167, learning rate: [0.1], loss:2.493802547454834\n",
      "Epoch: 0, batch index: 168, learning rate: [0.1], loss:2.5371689796447754\n",
      "Epoch: 0, batch index: 169, learning rate: [0.1], loss:2.564223527908325\n",
      "Epoch: 0, batch index: 170, learning rate: [0.1], loss:2.550271987915039\n",
      "Epoch: 0, batch index: 171, learning rate: [0.1], loss:2.5505855083465576\n",
      "Epoch: 0, batch index: 172, learning rate: [0.1], loss:2.607473373413086\n",
      "Epoch: 0, batch index: 173, learning rate: [0.1], loss:2.56374192237854\n",
      "Epoch: 0, batch index: 174, learning rate: [0.1], loss:2.5677149295806885\n",
      "Epoch: 0, batch index: 175, learning rate: [0.1], loss:2.560706615447998\n",
      "Epoch: 0, batch index: 176, learning rate: [0.1], loss:2.544822931289673\n",
      "Epoch: 0, batch index: 177, learning rate: [0.1], loss:2.5556764602661133\n",
      "Epoch: 0, batch index: 178, learning rate: [0.1], loss:2.5374326705932617\n",
      "Epoch: 0, batch index: 179, learning rate: [0.1], loss:2.52756929397583\n",
      "Epoch: 0, batch index: 180, learning rate: [0.1], loss:2.5422203540802\n",
      "Epoch: 0, batch index: 181, learning rate: [0.1], loss:2.5432207584381104\n",
      "Epoch: 0, batch index: 182, learning rate: [0.1], loss:2.5849313735961914\n",
      "Epoch: 0, batch index: 183, learning rate: [0.1], loss:2.5235037803649902\n",
      "Epoch: 0, batch index: 184, learning rate: [0.1], loss:2.561471700668335\n",
      "Epoch: 0, batch index: 185, learning rate: [0.1], loss:2.52339506149292\n",
      "Epoch: 0, batch index: 186, learning rate: [0.1], loss:2.52315616607666\n",
      "Epoch: 0, batch index: 187, learning rate: [0.1], loss:2.5403692722320557\n",
      "Calculating validation accuracy\n",
      "cuda:0\n",
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "Epoch 0, validation accuracy score0.1064453125\n",
      "Saving model parameters!\n",
      "Epoch: 1, batch index: 188, learning rate: [0.1], loss:2.5825576782226562\n",
      "Epoch: 1, batch index: 189, learning rate: [0.1], loss:2.514378786087036\n",
      "Epoch: 1, batch index: 190, learning rate: [0.1], loss:2.52647066116333\n",
      "Epoch: 1, batch index: 191, learning rate: [0.1], loss:2.50508451461792\n",
      "Epoch: 1, batch index: 192, learning rate: [0.1], loss:2.530449867248535\n",
      "Epoch: 1, batch index: 193, learning rate: [0.1], loss:2.544142961502075\n",
      "Epoch: 1, batch index: 194, learning rate: [0.1], loss:2.5072574615478516\n",
      "Epoch: 1, batch index: 195, learning rate: [0.1], loss:2.4980883598327637\n",
      "Epoch: 1, batch index: 196, learning rate: [0.1], loss:2.538674831390381\n",
      "Epoch: 1, batch index: 197, learning rate: [0.1], loss:2.6150481700897217\n",
      "Epoch: 1, batch index: 198, learning rate: [0.1], loss:2.5645203590393066\n",
      "Epoch: 1, batch index: 199, learning rate: [0.1], loss:2.581751585006714\n",
      "Epoch: 1, batch index: 200, learning rate: [0.1], loss:2.528445243835449\n",
      "Epoch: 1, batch index: 201, learning rate: [0.1], loss:2.5092856884002686\n",
      "Epoch: 1, batch index: 202, learning rate: [0.1], loss:2.5300514698028564\n",
      "Epoch: 1, batch index: 203, learning rate: [0.1], loss:2.5724709033966064\n",
      "Epoch: 1, batch index: 204, learning rate: [0.1], loss:2.526602268218994\n",
      "Epoch: 1, batch index: 205, learning rate: [0.1], loss:2.617253065109253\n",
      "Epoch: 1, batch index: 206, learning rate: [0.1], loss:2.5507466793060303\n",
      "Epoch: 1, batch index: 207, learning rate: [0.1], loss:2.572085380554199\n",
      "Epoch: 1, batch index: 208, learning rate: [0.1], loss:2.5410823822021484\n",
      "Epoch: 1, batch index: 209, learning rate: [0.1], loss:2.50869083404541\n",
      "Epoch: 1, batch index: 210, learning rate: [0.1], loss:2.5844249725341797\n",
      "Epoch: 1, batch index: 211, learning rate: [0.1], loss:2.5567286014556885\n",
      "Epoch: 1, batch index: 212, learning rate: [0.1], loss:2.5357303619384766\n",
      "Epoch: 1, batch index: 213, learning rate: [0.1], loss:2.447075128555298\n",
      "Epoch: 1, batch index: 214, learning rate: [0.1], loss:2.52250075340271\n",
      "Epoch: 1, batch index: 215, learning rate: [0.1], loss:2.530057430267334\n",
      "Epoch: 1, batch index: 216, learning rate: [0.1], loss:2.579381227493286\n",
      "Epoch: 1, batch index: 217, learning rate: [0.1], loss:2.600346565246582\n",
      "Epoch: 1, batch index: 218, learning rate: [0.1], loss:2.5422403812408447\n",
      "Epoch: 1, batch index: 219, learning rate: [0.1], loss:2.5633108615875244\n",
      "Epoch: 1, batch index: 220, learning rate: [0.010000000000000002], loss:2.524609327316284\n",
      "Epoch: 1, batch index: 221, learning rate: [0.010000000000000002], loss:2.5755481719970703\n",
      "Epoch: 1, batch index: 222, learning rate: [0.010000000000000002], loss:2.486656665802002\n",
      "Epoch: 1, batch index: 223, learning rate: [0.010000000000000002], loss:2.5346944332122803\n",
      "Epoch: 1, batch index: 224, learning rate: [0.010000000000000002], loss:2.5608463287353516\n",
      "Epoch: 1, batch index: 225, learning rate: [0.010000000000000002], loss:2.5700464248657227\n",
      "Epoch: 1, batch index: 226, learning rate: [0.010000000000000002], loss:2.5355687141418457\n",
      "Epoch: 1, batch index: 227, learning rate: [0.010000000000000002], loss:2.516982316970825\n",
      "Epoch: 1, batch index: 228, learning rate: [0.010000000000000002], loss:2.501359701156616\n",
      "Epoch: 1, batch index: 229, learning rate: [0.010000000000000002], loss:2.5604238510131836\n",
      "Epoch: 1, batch index: 230, learning rate: [0.010000000000000002], loss:2.5376312732696533\n",
      "Epoch: 1, batch index: 231, learning rate: [0.010000000000000002], loss:2.492544412612915\n",
      "Epoch: 1, batch index: 232, learning rate: [0.010000000000000002], loss:2.5216622352600098\n",
      "Epoch: 1, batch index: 233, learning rate: [0.010000000000000002], loss:2.544125556945801\n",
      "Epoch: 1, batch index: 234, learning rate: [0.010000000000000002], loss:2.5354504585266113\n",
      "Epoch: 1, batch index: 235, learning rate: [0.010000000000000002], loss:2.5685207843780518\n",
      "Epoch: 1, batch index: 236, learning rate: [0.010000000000000002], loss:2.5160086154937744\n",
      "Epoch: 1, batch index: 237, learning rate: [0.010000000000000002], loss:2.5018374919891357\n",
      "Epoch: 1, batch index: 238, learning rate: [0.010000000000000002], loss:2.519758939743042\n",
      "Epoch: 1, batch index: 239, learning rate: [0.010000000000000002], loss:2.5898711681365967\n",
      "Epoch: 1, batch index: 240, learning rate: [0.010000000000000002], loss:2.5601329803466797\n",
      "Epoch: 1, batch index: 241, learning rate: [0.010000000000000002], loss:2.582900047302246\n",
      "Epoch: 1, batch index: 242, learning rate: [0.010000000000000002], loss:2.5076632499694824\n",
      "Epoch: 1, batch index: 243, learning rate: [0.010000000000000002], loss:2.5348734855651855\n",
      "Epoch: 1, batch index: 244, learning rate: [0.010000000000000002], loss:2.5565385818481445\n",
      "Epoch: 1, batch index: 245, learning rate: [0.010000000000000002], loss:2.5378799438476562\n",
      "Epoch: 1, batch index: 246, learning rate: [0.010000000000000002], loss:2.582073926925659\n",
      "Epoch: 1, batch index: 247, learning rate: [0.010000000000000002], loss:2.5637807846069336\n",
      "Epoch: 1, batch index: 248, learning rate: [0.010000000000000002], loss:2.4821972846984863\n",
      "Epoch: 1, batch index: 249, learning rate: [0.010000000000000002], loss:2.537871837615967\n",
      "Epoch: 1, batch index: 250, learning rate: [0.010000000000000002], loss:2.553560733795166\n",
      "Epoch: 1, batch index: 251, learning rate: [0.010000000000000002], loss:2.5164783000946045\n",
      "Epoch: 1, batch index: 252, learning rate: [0.010000000000000002], loss:2.5178868770599365\n",
      "Epoch: 1, batch index: 253, learning rate: [0.010000000000000002], loss:2.510460615158081\n",
      "Epoch: 1, batch index: 254, learning rate: [0.010000000000000002], loss:2.5723447799682617\n",
      "Epoch: 1, batch index: 255, learning rate: [0.010000000000000002], loss:2.5879299640655518\n",
      "Epoch: 1, batch index: 256, learning rate: [0.010000000000000002], loss:2.532921552658081\n",
      "Epoch: 1, batch index: 257, learning rate: [0.010000000000000002], loss:2.573302984237671\n",
      "Epoch: 1, batch index: 258, learning rate: [0.010000000000000002], loss:2.557525396347046\n",
      "Epoch: 1, batch index: 259, learning rate: [0.010000000000000002], loss:2.5596275329589844\n",
      "Epoch: 1, batch index: 260, learning rate: [0.010000000000000002], loss:2.539834499359131\n",
      "Epoch: 1, batch index: 261, learning rate: [0.010000000000000002], loss:2.5606725215911865\n",
      "Epoch: 1, batch index: 262, learning rate: [0.010000000000000002], loss:2.555511951446533\n",
      "Epoch: 1, batch index: 263, learning rate: [0.010000000000000002], loss:2.5431969165802\n",
      "Epoch: 1, batch index: 264, learning rate: [0.010000000000000002], loss:2.562347412109375\n",
      "Epoch: 1, batch index: 265, learning rate: [0.010000000000000002], loss:2.5617177486419678\n",
      "Epoch: 1, batch index: 266, learning rate: [0.010000000000000002], loss:2.516896963119507\n",
      "Epoch: 1, batch index: 267, learning rate: [0.010000000000000002], loss:2.5209734439849854\n",
      "Epoch: 1, batch index: 268, learning rate: [0.010000000000000002], loss:2.5168092250823975\n",
      "Epoch: 1, batch index: 269, learning rate: [0.010000000000000002], loss:2.5543010234832764\n",
      "Epoch: 1, batch index: 270, learning rate: [0.010000000000000002], loss:2.5037789344787598\n",
      "Epoch: 1, batch index: 271, learning rate: [0.010000000000000002], loss:2.4515068531036377\n",
      "Epoch: 1, batch index: 272, learning rate: [0.010000000000000002], loss:2.569614887237549\n",
      "Epoch: 1, batch index: 273, learning rate: [0.010000000000000002], loss:2.6010398864746094\n",
      "Epoch: 1, batch index: 274, learning rate: [0.010000000000000002], loss:2.5289111137390137\n",
      "Epoch: 1, batch index: 275, learning rate: [0.010000000000000002], loss:2.511683702468872\n",
      "Epoch: 1, batch index: 276, learning rate: [0.010000000000000002], loss:2.507652997970581\n",
      "Epoch: 1, batch index: 277, learning rate: [0.010000000000000002], loss:2.548757791519165\n",
      "Epoch: 1, batch index: 278, learning rate: [0.0010000000000000002], loss:2.539137363433838\n",
      "Epoch: 1, batch index: 279, learning rate: [0.0010000000000000002], loss:2.5135128498077393\n",
      "Epoch: 1, batch index: 280, learning rate: [0.0010000000000000002], loss:2.537351369857788\n",
      "Epoch: 1, batch index: 281, learning rate: [0.0010000000000000002], loss:2.546063184738159\n",
      "Epoch: 1, batch index: 282, learning rate: [0.0010000000000000002], loss:2.490139961242676\n",
      "Epoch: 1, batch index: 283, learning rate: [0.0010000000000000002], loss:2.5981428623199463\n",
      "Epoch: 1, batch index: 284, learning rate: [0.0010000000000000002], loss:2.5707194805145264\n",
      "Epoch: 1, batch index: 285, learning rate: [0.0010000000000000002], loss:2.5450992584228516\n",
      "Epoch: 1, batch index: 286, learning rate: [0.0010000000000000002], loss:2.5293214321136475\n",
      "Epoch: 1, batch index: 287, learning rate: [0.0010000000000000002], loss:2.5373294353485107\n",
      "Epoch: 1, batch index: 288, learning rate: [0.0010000000000000002], loss:2.5345001220703125\n",
      "Epoch: 1, batch index: 289, learning rate: [0.0010000000000000002], loss:2.5517640113830566\n",
      "Epoch: 1, batch index: 290, learning rate: [0.0010000000000000002], loss:2.5510194301605225\n",
      "Epoch: 1, batch index: 291, learning rate: [0.0010000000000000002], loss:2.5739905834198\n",
      "Epoch: 1, batch index: 292, learning rate: [0.0010000000000000002], loss:2.5073153972625732\n",
      "Epoch: 1, batch index: 293, learning rate: [0.0010000000000000002], loss:2.558011770248413\n",
      "Epoch: 1, batch index: 294, learning rate: [0.0010000000000000002], loss:2.5420515537261963\n",
      "Epoch: 1, batch index: 295, learning rate: [0.0010000000000000002], loss:2.543365478515625\n",
      "Epoch: 1, batch index: 296, learning rate: [0.0010000000000000002], loss:2.518179178237915\n",
      "Epoch: 1, batch index: 297, learning rate: [0.0010000000000000002], loss:2.521862268447876\n",
      "Epoch: 1, batch index: 298, learning rate: [0.0010000000000000002], loss:2.533271074295044\n",
      "Epoch: 1, batch index: 299, learning rate: [0.0010000000000000002], loss:2.5387704372406006\n",
      "Epoch: 1, batch index: 300, learning rate: [0.0010000000000000002], loss:2.4549872875213623\n",
      "Epoch: 1, batch index: 301, learning rate: [0.0010000000000000002], loss:2.543219804763794\n",
      "Epoch: 1, batch index: 302, learning rate: [0.0010000000000000002], loss:2.539160966873169\n",
      "Epoch: 1, batch index: 303, learning rate: [0.0010000000000000002], loss:2.514019727706909\n",
      "Epoch: 1, batch index: 304, learning rate: [0.0010000000000000002], loss:2.509263753890991\n",
      "Epoch: 1, batch index: 305, learning rate: [0.0010000000000000002], loss:2.5042920112609863\n",
      "Epoch: 1, batch index: 306, learning rate: [0.0010000000000000002], loss:2.5466506481170654\n",
      "Epoch: 1, batch index: 307, learning rate: [0.0010000000000000002], loss:2.4291512966156006\n",
      "Epoch: 1, batch index: 308, learning rate: [0.0010000000000000002], loss:2.4694719314575195\n",
      "Epoch: 1, batch index: 309, learning rate: [0.0010000000000000002], loss:2.5027406215667725\n",
      "Epoch: 1, batch index: 310, learning rate: [0.0010000000000000002], loss:2.5225417613983154\n",
      "Epoch: 1, batch index: 311, learning rate: [0.0010000000000000002], loss:2.5121026039123535\n",
      "Epoch: 1, batch index: 312, learning rate: [0.0010000000000000002], loss:2.50620698928833\n",
      "Epoch: 1, batch index: 313, learning rate: [0.0010000000000000002], loss:2.536757469177246\n",
      "Epoch: 1, batch index: 314, learning rate: [0.0010000000000000002], loss:2.5788466930389404\n",
      "Epoch: 1, batch index: 315, learning rate: [0.0010000000000000002], loss:2.533137321472168\n",
      "Epoch: 1, batch index: 316, learning rate: [0.0010000000000000002], loss:2.504142999649048\n",
      "Epoch: 1, batch index: 317, learning rate: [0.0010000000000000002], loss:2.5585439205169678\n",
      "Epoch: 1, batch index: 318, learning rate: [0.0010000000000000002], loss:2.573369264602661\n",
      "Epoch: 1, batch index: 319, learning rate: [0.0010000000000000002], loss:2.540332078933716\n",
      "Epoch: 1, batch index: 320, learning rate: [0.0010000000000000002], loss:2.51176118850708\n",
      "Epoch: 1, batch index: 321, learning rate: [0.0010000000000000002], loss:2.5145530700683594\n",
      "Epoch: 1, batch index: 322, learning rate: [0.0010000000000000002], loss:2.5372214317321777\n",
      "Epoch: 1, batch index: 323, learning rate: [0.0010000000000000002], loss:2.534261465072632\n",
      "Epoch: 1, batch index: 324, learning rate: [0.0010000000000000002], loss:2.506024122238159\n",
      "Epoch: 1, batch index: 325, learning rate: [0.0010000000000000002], loss:2.569455146789551\n",
      "Epoch: 1, batch index: 326, learning rate: [0.0010000000000000002], loss:2.5458126068115234\n",
      "Epoch: 1, batch index: 327, learning rate: [0.0010000000000000002], loss:2.531994104385376\n",
      "Epoch: 1, batch index: 328, learning rate: [0.0010000000000000002], loss:2.466289520263672\n",
      "Epoch: 1, batch index: 329, learning rate: [0.0010000000000000002], loss:2.5533509254455566\n",
      "Epoch: 1, batch index: 330, learning rate: [0.0010000000000000002], loss:2.446054458618164\n",
      "Epoch: 1, batch index: 331, learning rate: [0.0010000000000000002], loss:2.5065221786499023\n",
      "Epoch: 1, batch index: 332, learning rate: [0.0010000000000000002], loss:2.5622079372406006\n",
      "Epoch: 1, batch index: 333, learning rate: [0.0010000000000000002], loss:2.564912796020508\n",
      "Epoch: 1, batch index: 334, learning rate: [0.0010000000000000002], loss:2.5624070167541504\n",
      "Epoch: 1, batch index: 335, learning rate: [0.0010000000000000002], loss:2.4949746131896973\n",
      "Epoch: 1, batch index: 336, learning rate: [0.0010000000000000002], loss:2.5570571422576904\n",
      "Epoch: 1, batch index: 337, learning rate: [0.0010000000000000002], loss:2.497539758682251\n",
      "Epoch: 1, batch index: 338, learning rate: [0.0010000000000000002], loss:2.5224971771240234\n",
      "Epoch: 1, batch index: 339, learning rate: [0.0010000000000000002], loss:2.5360348224639893\n",
      "Epoch: 1, batch index: 340, learning rate: [0.0010000000000000002], loss:2.5281307697296143\n",
      "Epoch: 1, batch index: 341, learning rate: [0.0010000000000000002], loss:2.5911474227905273\n",
      "Epoch: 1, batch index: 342, learning rate: [0.0010000000000000002], loss:2.5622270107269287\n",
      "Epoch: 1, batch index: 343, learning rate: [0.0010000000000000002], loss:2.506082534790039\n",
      "Epoch: 1, batch index: 344, learning rate: [0.0010000000000000002], loss:2.5771381855010986\n",
      "Epoch: 1, batch index: 345, learning rate: [0.0010000000000000002], loss:2.5887999534606934\n",
      "Epoch: 1, batch index: 346, learning rate: [0.0010000000000000002], loss:2.5063304901123047\n",
      "Epoch: 1, batch index: 347, learning rate: [0.0010000000000000002], loss:2.5469491481781006\n",
      "Epoch: 1, batch index: 348, learning rate: [0.0010000000000000002], loss:2.5178983211517334\n",
      "Epoch: 1, batch index: 349, learning rate: [0.0010000000000000002], loss:2.461477279663086\n",
      "Epoch: 1, batch index: 350, learning rate: [0.0010000000000000002], loss:2.5687310695648193\n",
      "Epoch: 1, batch index: 351, learning rate: [0.0010000000000000002], loss:2.593722343444824\n",
      "Epoch: 1, batch index: 352, learning rate: [0.0010000000000000002], loss:2.581458806991577\n",
      "Epoch: 1, batch index: 353, learning rate: [0.0010000000000000002], loss:2.537997245788574\n",
      "Epoch: 1, batch index: 354, learning rate: [0.0010000000000000002], loss:2.538362979888916\n",
      "Epoch: 1, batch index: 355, learning rate: [0.0010000000000000002], loss:2.5047144889831543\n",
      "Epoch: 1, batch index: 356, learning rate: [0.0010000000000000002], loss:2.5104689598083496\n",
      "Epoch: 1, batch index: 357, learning rate: [0.0010000000000000002], loss:2.5368740558624268\n",
      "Epoch: 1, batch index: 358, learning rate: [0.0010000000000000002], loss:2.5489418506622314\n",
      "Epoch: 1, batch index: 359, learning rate: [0.00010000000000000003], loss:2.5123403072357178\n",
      "Epoch: 1, batch index: 360, learning rate: [0.00010000000000000003], loss:2.5218796730041504\n",
      "Epoch: 1, batch index: 361, learning rate: [0.00010000000000000003], loss:2.508023738861084\n",
      "Epoch: 1, batch index: 362, learning rate: [0.00010000000000000003], loss:2.5168533325195312\n",
      "Epoch: 1, batch index: 363, learning rate: [0.00010000000000000003], loss:2.537203073501587\n",
      "Epoch: 1, batch index: 364, learning rate: [0.00010000000000000003], loss:2.5131306648254395\n",
      "Epoch: 1, batch index: 365, learning rate: [0.00010000000000000003], loss:2.522545099258423\n",
      "Epoch: 1, batch index: 366, learning rate: [0.00010000000000000003], loss:2.5794451236724854\n",
      "Epoch: 1, batch index: 367, learning rate: [0.00010000000000000003], loss:2.5571129322052\n",
      "Epoch: 1, batch index: 368, learning rate: [0.00010000000000000003], loss:2.597106695175171\n",
      "Epoch: 1, batch index: 369, learning rate: [0.00010000000000000003], loss:2.4866204261779785\n",
      "Epoch: 1, batch index: 370, learning rate: [0.00010000000000000003], loss:2.5179646015167236\n",
      "Epoch: 1, batch index: 371, learning rate: [0.00010000000000000003], loss:2.5789639949798584\n",
      "Epoch: 1, batch index: 372, learning rate: [0.00010000000000000003], loss:2.564824104309082\n",
      "Epoch: 1, batch index: 373, learning rate: [0.00010000000000000003], loss:2.5440640449523926\n",
      "Epoch: 1, batch index: 374, learning rate: [0.00010000000000000003], loss:2.5946571826934814\n",
      "Epoch: 1, batch index: 375, learning rate: [0.00010000000000000003], loss:2.530734062194824\n",
      "Calculating validation accuracy\n",
      "cuda:0\n",
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "Epoch 1, validation accuracy score0.1083984375\n",
      "Epoch: 2, batch index: 376, learning rate: [0.00010000000000000003], loss:2.5484073162078857\n",
      "Epoch: 2, batch index: 377, learning rate: [0.00010000000000000003], loss:2.528761386871338\n",
      "Epoch: 2, batch index: 378, learning rate: [0.00010000000000000003], loss:2.496459484100342\n",
      "Epoch: 2, batch index: 379, learning rate: [0.00010000000000000003], loss:2.5149974822998047\n",
      "Epoch: 2, batch index: 380, learning rate: [0.00010000000000000003], loss:2.5138278007507324\n",
      "Epoch: 2, batch index: 381, learning rate: [0.00010000000000000003], loss:2.5071253776550293\n",
      "Epoch: 2, batch index: 382, learning rate: [0.00010000000000000003], loss:2.4999136924743652\n",
      "Epoch: 2, batch index: 383, learning rate: [0.00010000000000000003], loss:2.534249782562256\n",
      "Epoch: 2, batch index: 384, learning rate: [0.00010000000000000003], loss:2.5795552730560303\n",
      "Epoch: 2, batch index: 385, learning rate: [0.00010000000000000003], loss:2.5714683532714844\n",
      "Epoch: 2, batch index: 386, learning rate: [0.00010000000000000003], loss:2.4980216026306152\n",
      "Epoch: 2, batch index: 387, learning rate: [0.00010000000000000003], loss:2.5487401485443115\n",
      "Epoch: 2, batch index: 388, learning rate: [0.00010000000000000003], loss:2.5089025497436523\n",
      "Epoch: 2, batch index: 389, learning rate: [0.00010000000000000003], loss:2.5018842220306396\n",
      "Epoch: 2, batch index: 390, learning rate: [0.00010000000000000003], loss:2.5338687896728516\n",
      "Epoch: 2, batch index: 391, learning rate: [0.00010000000000000003], loss:2.45729398727417\n",
      "Epoch: 2, batch index: 392, learning rate: [0.00010000000000000003], loss:2.5398175716400146\n",
      "Epoch: 2, batch index: 393, learning rate: [0.00010000000000000003], loss:2.576422929763794\n",
      "Epoch: 2, batch index: 394, learning rate: [0.00010000000000000003], loss:2.5717427730560303\n",
      "Epoch: 2, batch index: 395, learning rate: [0.00010000000000000003], loss:2.469149351119995\n",
      "Epoch: 2, batch index: 396, learning rate: [0.00010000000000000003], loss:2.5158073902130127\n",
      "Epoch: 2, batch index: 397, learning rate: [0.00010000000000000003], loss:2.553825855255127\n",
      "Epoch: 2, batch index: 398, learning rate: [0.00010000000000000003], loss:2.507554292678833\n",
      "Epoch: 2, batch index: 399, learning rate: [0.00010000000000000003], loss:2.5408225059509277\n",
      "Epoch: 2, batch index: 400, learning rate: [0.00010000000000000003], loss:2.5475211143493652\n",
      "Epoch: 2, batch index: 401, learning rate: [0.00010000000000000003], loss:2.53924822807312\n",
      "Epoch: 2, batch index: 402, learning rate: [0.00010000000000000003], loss:2.5211055278778076\n",
      "Epoch: 2, batch index: 403, learning rate: [0.00010000000000000003], loss:2.549445867538452\n",
      "Epoch: 2, batch index: 404, learning rate: [0.00010000000000000003], loss:2.529681444168091\n",
      "Epoch: 2, batch index: 405, learning rate: [0.00010000000000000003], loss:2.55493426322937\n",
      "Epoch: 2, batch index: 406, learning rate: [0.00010000000000000003], loss:2.553647041320801\n",
      "Epoch: 2, batch index: 407, learning rate: [0.00010000000000000003], loss:2.503114700317383\n",
      "Epoch: 2, batch index: 408, learning rate: [0.00010000000000000003], loss:2.525941848754883\n",
      "Epoch: 2, batch index: 409, learning rate: [0.00010000000000000003], loss:2.4897994995117188\n",
      "Epoch: 2, batch index: 410, learning rate: [0.00010000000000000003], loss:2.5413918495178223\n",
      "Epoch: 2, batch index: 411, learning rate: [0.00010000000000000003], loss:2.538011074066162\n",
      "Epoch: 2, batch index: 412, learning rate: [0.00010000000000000003], loss:2.5711867809295654\n",
      "Epoch: 2, batch index: 413, learning rate: [0.00010000000000000003], loss:2.5039162635803223\n",
      "Epoch: 2, batch index: 414, learning rate: [0.00010000000000000003], loss:2.5005316734313965\n",
      "Epoch: 2, batch index: 415, learning rate: [0.00010000000000000003], loss:2.5594398975372314\n",
      "Epoch: 2, batch index: 416, learning rate: [0.00010000000000000003], loss:2.4864308834075928\n",
      "Epoch: 2, batch index: 417, learning rate: [1.0000000000000004e-05], loss:2.5163822174072266\n",
      "Epoch: 2, batch index: 418, learning rate: [1.0000000000000004e-05], loss:2.5168583393096924\n",
      "Epoch: 2, batch index: 419, learning rate: [1.0000000000000004e-05], loss:2.5459866523742676\n",
      "Epoch: 2, batch index: 420, learning rate: [1.0000000000000004e-05], loss:2.5872771739959717\n",
      "Epoch: 2, batch index: 421, learning rate: [1.0000000000000004e-05], loss:2.4684665203094482\n",
      "Epoch: 2, batch index: 422, learning rate: [1.0000000000000004e-05], loss:2.5264852046966553\n",
      "Epoch: 2, batch index: 423, learning rate: [1.0000000000000004e-05], loss:2.5488152503967285\n",
      "Epoch: 2, batch index: 424, learning rate: [1.0000000000000004e-05], loss:2.453758478164673\n",
      "Epoch: 2, batch index: 425, learning rate: [1.0000000000000004e-05], loss:2.5325889587402344\n",
      "Epoch: 2, batch index: 426, learning rate: [1.0000000000000004e-05], loss:2.5533974170684814\n",
      "Epoch: 2, batch index: 427, learning rate: [1.0000000000000004e-05], loss:2.5328874588012695\n",
      "Epoch: 2, batch index: 428, learning rate: [1.0000000000000004e-05], loss:2.5498833656311035\n",
      "Epoch: 2, batch index: 429, learning rate: [1.0000000000000004e-05], loss:2.56500244140625\n",
      "Epoch: 2, batch index: 430, learning rate: [1.0000000000000004e-05], loss:2.5760741233825684\n",
      "Epoch: 2, batch index: 431, learning rate: [1.0000000000000004e-05], loss:2.5005059242248535\n",
      "Epoch: 2, batch index: 432, learning rate: [1.0000000000000004e-05], loss:2.560450792312622\n",
      "Epoch: 2, batch index: 433, learning rate: [1.0000000000000004e-05], loss:2.520059823989868\n",
      "Epoch: 2, batch index: 434, learning rate: [1.0000000000000004e-05], loss:2.554859161376953\n",
      "Epoch: 2, batch index: 435, learning rate: [1.0000000000000004e-05], loss:2.5577404499053955\n",
      "Epoch: 2, batch index: 436, learning rate: [1.0000000000000004e-05], loss:2.5607471466064453\n",
      "Epoch: 2, batch index: 437, learning rate: [1.0000000000000004e-05], loss:2.5271921157836914\n",
      "Epoch: 2, batch index: 438, learning rate: [1.0000000000000004e-05], loss:2.545792579650879\n",
      "Epoch: 2, batch index: 439, learning rate: [1.0000000000000004e-05], loss:2.5172297954559326\n",
      "Epoch: 2, batch index: 440, learning rate: [1.0000000000000004e-05], loss:2.5640265941619873\n",
      "Epoch: 2, batch index: 441, learning rate: [1.0000000000000004e-05], loss:2.519864082336426\n",
      "Epoch: 2, batch index: 442, learning rate: [1.0000000000000004e-05], loss:2.5900180339813232\n",
      "Epoch: 2, batch index: 443, learning rate: [1.0000000000000004e-05], loss:2.5493690967559814\n",
      "Epoch: 2, batch index: 444, learning rate: [1.0000000000000004e-05], loss:2.5015714168548584\n",
      "Epoch: 2, batch index: 445, learning rate: [1.0000000000000004e-05], loss:2.5879313945770264\n",
      "Epoch: 2, batch index: 446, learning rate: [1.0000000000000004e-05], loss:2.5254366397857666\n",
      "Epoch: 2, batch index: 447, learning rate: [1.0000000000000004e-05], loss:2.5442655086517334\n",
      "Epoch: 2, batch index: 448, learning rate: [1.0000000000000004e-05], loss:2.5175952911376953\n",
      "Epoch: 2, batch index: 449, learning rate: [1.0000000000000004e-05], loss:2.543130874633789\n",
      "Epoch: 2, batch index: 450, learning rate: [1.0000000000000004e-05], loss:2.5657455921173096\n",
      "Epoch: 2, batch index: 451, learning rate: [1.0000000000000004e-05], loss:2.548701286315918\n",
      "Epoch: 2, batch index: 452, learning rate: [1.0000000000000004e-05], loss:2.493973970413208\n",
      "Epoch: 2, batch index: 453, learning rate: [1.0000000000000004e-05], loss:2.482656240463257\n",
      "Epoch: 2, batch index: 454, learning rate: [1.0000000000000004e-05], loss:2.522998809814453\n",
      "Epoch: 2, batch index: 455, learning rate: [1.0000000000000004e-05], loss:2.5468413829803467\n",
      "Epoch: 2, batch index: 456, learning rate: [1.0000000000000004e-05], loss:2.531958818435669\n",
      "Epoch: 2, batch index: 457, learning rate: [1.0000000000000004e-05], loss:2.570984125137329\n",
      "Epoch: 2, batch index: 458, learning rate: [1.0000000000000004e-05], loss:2.562263250350952\n",
      "Epoch: 2, batch index: 459, learning rate: [1.0000000000000004e-05], loss:2.5543055534362793\n",
      "Epoch: 2, batch index: 460, learning rate: [1.0000000000000004e-05], loss:2.53474497795105\n",
      "Epoch: 2, batch index: 461, learning rate: [1.0000000000000004e-05], loss:2.5133509635925293\n",
      "Epoch: 2, batch index: 462, learning rate: [1.0000000000000004e-05], loss:2.5436577796936035\n",
      "Epoch: 2, batch index: 463, learning rate: [1.0000000000000004e-05], loss:2.5343616008758545\n",
      "Epoch: 2, batch index: 464, learning rate: [1.0000000000000004e-05], loss:2.5079283714294434\n",
      "Epoch: 2, batch index: 465, learning rate: [1.0000000000000004e-05], loss:2.5785927772521973\n",
      "Epoch: 2, batch index: 466, learning rate: [1.0000000000000004e-05], loss:2.568976879119873\n",
      "Epoch: 2, batch index: 467, learning rate: [1.0000000000000004e-05], loss:2.5408575534820557\n",
      "Epoch: 2, batch index: 468, learning rate: [1.0000000000000004e-05], loss:2.5845422744750977\n",
      "Epoch: 2, batch index: 469, learning rate: [1.0000000000000004e-05], loss:2.5781116485595703\n",
      "Epoch: 2, batch index: 470, learning rate: [1.0000000000000004e-05], loss:2.516313314437866\n",
      "Epoch: 2, batch index: 471, learning rate: [1.0000000000000004e-05], loss:2.5168545246124268\n",
      "Epoch: 2, batch index: 472, learning rate: [1.0000000000000004e-05], loss:2.5439670085906982\n",
      "Epoch: 2, batch index: 473, learning rate: [1.0000000000000004e-05], loss:2.564208745956421\n",
      "Epoch: 2, batch index: 474, learning rate: [1.0000000000000004e-05], loss:2.5788612365722656\n",
      "Epoch: 2, batch index: 475, learning rate: [1.0000000000000004e-06], loss:2.5136029720306396\n",
      "Epoch: 2, batch index: 476, learning rate: [1.0000000000000004e-06], loss:2.4921000003814697\n",
      "Epoch: 2, batch index: 477, learning rate: [1.0000000000000004e-06], loss:2.4963066577911377\n",
      "Epoch: 2, batch index: 478, learning rate: [1.0000000000000004e-06], loss:2.5772881507873535\n",
      "Epoch: 2, batch index: 479, learning rate: [1.0000000000000004e-06], loss:2.5532355308532715\n",
      "Epoch: 2, batch index: 480, learning rate: [1.0000000000000004e-06], loss:2.5332367420196533\n",
      "Epoch: 2, batch index: 481, learning rate: [1.0000000000000004e-06], loss:2.5140788555145264\n",
      "Epoch: 2, batch index: 482, learning rate: [1.0000000000000004e-06], loss:2.5152573585510254\n",
      "Epoch: 2, batch index: 483, learning rate: [1.0000000000000004e-06], loss:2.554760456085205\n",
      "Epoch: 2, batch index: 484, learning rate: [1.0000000000000004e-06], loss:2.5324761867523193\n",
      "Epoch: 2, batch index: 485, learning rate: [1.0000000000000004e-06], loss:2.530625820159912\n",
      "Epoch: 2, batch index: 486, learning rate: [1.0000000000000004e-06], loss:2.539607286453247\n",
      "Epoch: 2, batch index: 487, learning rate: [1.0000000000000004e-06], loss:2.520648241043091\n",
      "Epoch: 2, batch index: 488, learning rate: [1.0000000000000004e-06], loss:2.5772178173065186\n",
      "Epoch: 2, batch index: 489, learning rate: [1.0000000000000004e-06], loss:2.541569232940674\n",
      "Epoch: 2, batch index: 490, learning rate: [1.0000000000000004e-06], loss:2.5746452808380127\n",
      "Epoch: 2, batch index: 491, learning rate: [1.0000000000000004e-06], loss:2.521350383758545\n",
      "Epoch: 2, batch index: 492, learning rate: [1.0000000000000004e-06], loss:2.5445196628570557\n",
      "Epoch: 2, batch index: 493, learning rate: [1.0000000000000004e-06], loss:2.6145594120025635\n",
      "Epoch: 2, batch index: 494, learning rate: [1.0000000000000004e-06], loss:2.5353457927703857\n",
      "Epoch: 2, batch index: 495, learning rate: [1.0000000000000004e-06], loss:2.548241138458252\n",
      "Epoch: 2, batch index: 496, learning rate: [1.0000000000000004e-06], loss:2.540405511856079\n",
      "Epoch: 2, batch index: 497, learning rate: [1.0000000000000004e-06], loss:2.5479817390441895\n",
      "Epoch: 2, batch index: 498, learning rate: [1.0000000000000004e-06], loss:2.5329606533050537\n",
      "Epoch: 2, batch index: 499, learning rate: [1.0000000000000004e-06], loss:2.559051752090454\n",
      "Epoch: 2, batch index: 500, learning rate: [1.0000000000000004e-06], loss:2.53788685798645\n",
      "Epoch: 2, batch index: 501, learning rate: [1.0000000000000004e-06], loss:2.5083136558532715\n",
      "Epoch: 2, batch index: 502, learning rate: [1.0000000000000004e-06], loss:2.5593485832214355\n",
      "Epoch: 2, batch index: 503, learning rate: [1.0000000000000004e-06], loss:2.583479881286621\n",
      "Epoch: 2, batch index: 504, learning rate: [1.0000000000000004e-06], loss:2.5399913787841797\n",
      "Epoch: 2, batch index: 505, learning rate: [1.0000000000000004e-06], loss:2.5052714347839355\n",
      "Epoch: 2, batch index: 506, learning rate: [1.0000000000000004e-06], loss:2.480872392654419\n",
      "Epoch: 2, batch index: 507, learning rate: [1.0000000000000004e-06], loss:2.5635530948638916\n",
      "Epoch: 2, batch index: 508, learning rate: [1.0000000000000004e-06], loss:2.5580315589904785\n",
      "Epoch: 2, batch index: 509, learning rate: [1.0000000000000004e-06], loss:2.4959568977355957\n",
      "Epoch: 2, batch index: 510, learning rate: [1.0000000000000004e-06], loss:2.5039703845977783\n",
      "Epoch: 2, batch index: 511, learning rate: [1.0000000000000004e-06], loss:2.539928913116455\n",
      "Epoch: 2, batch index: 512, learning rate: [1.0000000000000004e-06], loss:2.492316484451294\n",
      "Epoch: 2, batch index: 513, learning rate: [1.0000000000000004e-06], loss:2.4848554134368896\n",
      "Epoch: 2, batch index: 514, learning rate: [1.0000000000000004e-06], loss:2.535273313522339\n",
      "Epoch: 2, batch index: 515, learning rate: [1.0000000000000004e-06], loss:2.5699222087860107\n",
      "Epoch: 2, batch index: 516, learning rate: [1.0000000000000004e-06], loss:2.5636212825775146\n",
      "Epoch: 2, batch index: 517, learning rate: [1.0000000000000004e-06], loss:2.521451473236084\n",
      "Epoch: 2, batch index: 518, learning rate: [1.0000000000000004e-06], loss:2.513014078140259\n",
      "Epoch: 2, batch index: 519, learning rate: [1.0000000000000004e-06], loss:2.557389259338379\n",
      "Epoch: 2, batch index: 520, learning rate: [1.0000000000000004e-06], loss:2.5218546390533447\n",
      "Epoch: 2, batch index: 521, learning rate: [1.0000000000000004e-06], loss:2.550318956375122\n",
      "Epoch: 2, batch index: 522, learning rate: [1.0000000000000004e-06], loss:2.5491714477539062\n",
      "Epoch: 2, batch index: 523, learning rate: [1.0000000000000004e-06], loss:2.537109375\n",
      "Epoch: 2, batch index: 524, learning rate: [1.0000000000000004e-06], loss:2.5215554237365723\n",
      "Epoch: 2, batch index: 525, learning rate: [1.0000000000000004e-06], loss:2.5559775829315186\n",
      "Epoch: 2, batch index: 526, learning rate: [1.0000000000000004e-06], loss:2.546639919281006\n",
      "Epoch: 2, batch index: 527, learning rate: [1.0000000000000004e-06], loss:2.5041699409484863\n",
      "Epoch: 2, batch index: 528, learning rate: [1.0000000000000004e-06], loss:2.5347821712493896\n",
      "Epoch: 2, batch index: 529, learning rate: [1.0000000000000004e-06], loss:2.5669984817504883\n",
      "Epoch: 2, batch index: 530, learning rate: [1.0000000000000004e-06], loss:2.534674882888794\n",
      "Epoch: 2, batch index: 531, learning rate: [1.0000000000000004e-06], loss:2.547072410583496\n",
      "Epoch: 2, batch index: 532, learning rate: [1.0000000000000004e-06], loss:2.530785322189331\n",
      "Epoch: 2, batch index: 533, learning rate: [1.0000000000000005e-07], loss:2.528409242630005\n",
      "Epoch: 2, batch index: 534, learning rate: [1.0000000000000005e-07], loss:2.508859157562256\n",
      "Epoch: 2, batch index: 535, learning rate: [1.0000000000000005e-07], loss:2.4638147354125977\n",
      "Epoch: 2, batch index: 536, learning rate: [1.0000000000000005e-07], loss:2.5222251415252686\n",
      "Epoch: 2, batch index: 537, learning rate: [1.0000000000000005e-07], loss:2.5773096084594727\n",
      "Epoch: 2, batch index: 538, learning rate: [1.0000000000000005e-07], loss:2.5813045501708984\n",
      "Epoch: 2, batch index: 539, learning rate: [1.0000000000000005e-07], loss:2.5764336585998535\n",
      "Epoch: 2, batch index: 540, learning rate: [1.0000000000000005e-07], loss:2.54436993598938\n",
      "Epoch: 2, batch index: 541, learning rate: [1.0000000000000005e-07], loss:2.5359344482421875\n",
      "Epoch: 2, batch index: 542, learning rate: [1.0000000000000005e-07], loss:2.5640523433685303\n",
      "Epoch: 2, batch index: 543, learning rate: [1.0000000000000005e-07], loss:2.577702045440674\n",
      "Epoch: 2, batch index: 544, learning rate: [1.0000000000000005e-07], loss:2.5384440422058105\n",
      "Epoch: 2, batch index: 545, learning rate: [1.0000000000000005e-07], loss:2.550455331802368\n",
      "Epoch: 2, batch index: 546, learning rate: [1.0000000000000005e-07], loss:2.524793863296509\n",
      "Epoch: 2, batch index: 547, learning rate: [1.0000000000000005e-07], loss:2.555098295211792\n",
      "Epoch: 2, batch index: 548, learning rate: [1.0000000000000005e-07], loss:2.514972686767578\n",
      "Epoch: 2, batch index: 549, learning rate: [1.0000000000000005e-07], loss:2.5681121349334717\n",
      "Epoch: 2, batch index: 550, learning rate: [1.0000000000000005e-07], loss:2.500938892364502\n",
      "Epoch: 2, batch index: 551, learning rate: [1.0000000000000005e-07], loss:2.544588565826416\n",
      "Epoch: 2, batch index: 552, learning rate: [1.0000000000000005e-07], loss:2.4954094886779785\n",
      "Epoch: 2, batch index: 553, learning rate: [1.0000000000000005e-07], loss:2.5786168575286865\n",
      "Epoch: 2, batch index: 554, learning rate: [1.0000000000000005e-07], loss:2.5233967304229736\n",
      "Epoch: 2, batch index: 555, learning rate: [1.0000000000000005e-07], loss:2.5552804470062256\n",
      "Epoch: 2, batch index: 556, learning rate: [1.0000000000000005e-07], loss:2.578228235244751\n",
      "Epoch: 2, batch index: 557, learning rate: [1.0000000000000005e-07], loss:2.547346353530884\n",
      "Epoch: 2, batch index: 558, learning rate: [1.0000000000000005e-07], loss:2.53228759765625\n",
      "Epoch: 2, batch index: 559, learning rate: [1.0000000000000005e-07], loss:2.5204668045043945\n",
      "Epoch: 2, batch index: 560, learning rate: [1.0000000000000005e-07], loss:2.5448431968688965\n",
      "Epoch: 2, batch index: 561, learning rate: [1.0000000000000005e-07], loss:2.575129270553589\n",
      "Epoch: 2, batch index: 562, learning rate: [1.0000000000000005e-07], loss:2.478297710418701\n",
      "Epoch: 2, batch index: 563, learning rate: [1.0000000000000005e-07], loss:2.4697866439819336\n",
      "Calculating validation accuracy\n",
      "cuda:0\n",
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "Epoch 2, validation accuracy score0.107421875\n",
      "Epoch: 3, batch index: 564, learning rate: [1.0000000000000005e-07], loss:2.5243759155273438\n",
      "Epoch: 3, batch index: 565, learning rate: [1.0000000000000005e-07], loss:2.5572705268859863\n",
      "Epoch: 3, batch index: 566, learning rate: [1.0000000000000005e-07], loss:2.5646605491638184\n",
      "Epoch: 3, batch index: 567, learning rate: [1.0000000000000005e-07], loss:2.5564475059509277\n",
      "Epoch: 3, batch index: 568, learning rate: [1.0000000000000005e-07], loss:2.5492868423461914\n",
      "Epoch: 3, batch index: 569, learning rate: [1.0000000000000005e-07], loss:2.5313868522644043\n",
      "Epoch: 3, batch index: 570, learning rate: [1.0000000000000005e-07], loss:2.5693414211273193\n",
      "Epoch: 3, batch index: 571, learning rate: [1.0000000000000005e-07], loss:2.551734447479248\n",
      "Epoch: 3, batch index: 572, learning rate: [1.0000000000000005e-07], loss:2.546621322631836\n",
      "Epoch: 3, batch index: 573, learning rate: [1.0000000000000005e-07], loss:2.564633369445801\n",
      "Epoch: 3, batch index: 574, learning rate: [1.0000000000000005e-07], loss:2.57517147064209\n",
      "Epoch: 3, batch index: 575, learning rate: [1.0000000000000005e-07], loss:2.5663678646087646\n",
      "Epoch: 3, batch index: 576, learning rate: [1.0000000000000005e-07], loss:2.5674362182617188\n",
      "Epoch: 3, batch index: 577, learning rate: [1.0000000000000005e-07], loss:2.544034481048584\n",
      "Epoch: 3, batch index: 578, learning rate: [1.0000000000000005e-07], loss:2.573756456375122\n",
      "Epoch: 3, batch index: 579, learning rate: [1.0000000000000005e-07], loss:2.5000405311584473\n",
      "Epoch: 3, batch index: 580, learning rate: [1.0000000000000005e-07], loss:2.548999786376953\n",
      "Epoch: 3, batch index: 581, learning rate: [1.0000000000000005e-07], loss:2.4842896461486816\n",
      "Epoch: 3, batch index: 582, learning rate: [1.0000000000000005e-07], loss:2.586355209350586\n",
      "Epoch: 3, batch index: 583, learning rate: [1.0000000000000005e-07], loss:2.555915594100952\n",
      "Epoch: 3, batch index: 584, learning rate: [1.0000000000000005e-07], loss:2.57705020904541\n",
      "Epoch: 3, batch index: 585, learning rate: [1.0000000000000005e-07], loss:2.5297040939331055\n",
      "Epoch: 3, batch index: 586, learning rate: [1.0000000000000005e-07], loss:2.5449743270874023\n",
      "Epoch: 3, batch index: 587, learning rate: [1.0000000000000005e-07], loss:2.4925007820129395\n",
      "Epoch: 3, batch index: 588, learning rate: [1.0000000000000005e-07], loss:2.56207537651062\n",
      "Epoch: 3, batch index: 589, learning rate: [1.0000000000000005e-07], loss:2.5120017528533936\n",
      "Epoch: 3, batch index: 590, learning rate: [1.0000000000000005e-07], loss:2.4676766395568848\n",
      "Epoch: 3, batch index: 591, learning rate: [1.0000000000000005e-08], loss:2.5438332557678223\n",
      "Epoch: 3, batch index: 592, learning rate: [1.0000000000000005e-08], loss:2.4918673038482666\n",
      "Epoch: 3, batch index: 593, learning rate: [1.0000000000000005e-08], loss:2.5072782039642334\n",
      "Epoch: 3, batch index: 594, learning rate: [1.0000000000000005e-08], loss:2.585449457168579\n",
      "Epoch: 3, batch index: 595, learning rate: [1.0000000000000005e-08], loss:2.53981876373291\n",
      "Epoch: 3, batch index: 596, learning rate: [1.0000000000000005e-08], loss:2.5532548427581787\n",
      "Epoch: 3, batch index: 597, learning rate: [1.0000000000000005e-08], loss:2.577554941177368\n",
      "Epoch: 3, batch index: 598, learning rate: [1.0000000000000005e-08], loss:2.559652805328369\n",
      "Epoch: 3, batch index: 599, learning rate: [1.0000000000000005e-08], loss:2.514026403427124\n",
      "Epoch: 3, batch index: 600, learning rate: [1.0000000000000005e-08], loss:2.502774238586426\n",
      "Epoch: 3, batch index: 601, learning rate: [1.0000000000000005e-08], loss:2.567079544067383\n",
      "Epoch: 3, batch index: 602, learning rate: [1.0000000000000005e-08], loss:2.5309791564941406\n",
      "Epoch: 3, batch index: 603, learning rate: [1.0000000000000005e-08], loss:2.538978099822998\n",
      "Epoch: 3, batch index: 604, learning rate: [1.0000000000000005e-08], loss:2.4843597412109375\n",
      "Epoch: 3, batch index: 605, learning rate: [1.0000000000000005e-08], loss:2.5172135829925537\n",
      "Epoch: 3, batch index: 606, learning rate: [1.0000000000000005e-08], loss:2.498277425765991\n",
      "Epoch: 3, batch index: 607, learning rate: [1.0000000000000005e-08], loss:2.5743558406829834\n",
      "Epoch: 3, batch index: 608, learning rate: [1.0000000000000005e-08], loss:2.5519888401031494\n",
      "Epoch: 3, batch index: 609, learning rate: [1.0000000000000005e-08], loss:2.5374953746795654\n",
      "Epoch: 3, batch index: 610, learning rate: [1.0000000000000005e-08], loss:2.5158519744873047\n",
      "Epoch: 3, batch index: 611, learning rate: [1.0000000000000005e-08], loss:2.520765542984009\n",
      "Epoch: 3, batch index: 612, learning rate: [1.0000000000000005e-08], loss:2.5735583305358887\n",
      "Epoch: 3, batch index: 613, learning rate: [1.0000000000000005e-08], loss:2.5753281116485596\n",
      "Epoch: 3, batch index: 614, learning rate: [1.0000000000000005e-08], loss:2.5435311794281006\n",
      "Epoch: 3, batch index: 615, learning rate: [1.0000000000000005e-08], loss:2.5200467109680176\n",
      "Epoch: 3, batch index: 616, learning rate: [1.0000000000000005e-08], loss:2.4978396892547607\n",
      "Epoch: 3, batch index: 617, learning rate: [1.0000000000000005e-08], loss:2.4922101497650146\n",
      "Epoch: 3, batch index: 618, learning rate: [1.0000000000000005e-08], loss:2.608074188232422\n",
      "Epoch: 3, batch index: 619, learning rate: [1.0000000000000005e-08], loss:2.594111442565918\n",
      "Epoch: 3, batch index: 620, learning rate: [1.0000000000000005e-08], loss:2.4958713054656982\n",
      "Epoch: 3, batch index: 621, learning rate: [1.0000000000000005e-08], loss:2.5405848026275635\n",
      "Epoch: 3, batch index: 622, learning rate: [1.0000000000000005e-08], loss:2.4737608432769775\n",
      "Epoch: 3, batch index: 623, learning rate: [1.0000000000000005e-08], loss:2.52435040473938\n",
      "Epoch: 3, batch index: 624, learning rate: [1.0000000000000005e-08], loss:2.457880973815918\n",
      "Epoch: 3, batch index: 625, learning rate: [1.0000000000000005e-08], loss:2.5431602001190186\n",
      "Epoch: 3, batch index: 626, learning rate: [1.0000000000000005e-08], loss:2.5486176013946533\n",
      "Epoch: 3, batch index: 627, learning rate: [1.0000000000000005e-08], loss:2.5423636436462402\n",
      "Epoch: 3, batch index: 628, learning rate: [1.0000000000000005e-08], loss:2.5319175720214844\n",
      "Epoch: 3, batch index: 629, learning rate: [1.0000000000000005e-08], loss:2.55745530128479\n",
      "Epoch: 3, batch index: 630, learning rate: [1.0000000000000005e-08], loss:2.5316877365112305\n",
      "Epoch: 3, batch index: 631, learning rate: [1.0000000000000005e-08], loss:2.5109052658081055\n",
      "Epoch: 3, batch index: 632, learning rate: [1.0000000000000005e-08], loss:2.5363516807556152\n",
      "Epoch: 3, batch index: 633, learning rate: [1.0000000000000005e-08], loss:2.499281883239746\n",
      "Epoch: 3, batch index: 634, learning rate: [1.0000000000000005e-08], loss:2.542055606842041\n",
      "Epoch: 3, batch index: 635, learning rate: [1.0000000000000005e-08], loss:2.561825752258301\n",
      "Epoch: 3, batch index: 636, learning rate: [1.0000000000000005e-08], loss:2.5354838371276855\n",
      "Epoch: 3, batch index: 637, learning rate: [1.0000000000000005e-08], loss:2.55876088142395\n",
      "Epoch: 3, batch index: 638, learning rate: [1.0000000000000005e-08], loss:2.445249557495117\n",
      "Epoch: 3, batch index: 639, learning rate: [1.0000000000000005e-08], loss:2.556278944015503\n",
      "Epoch: 3, batch index: 640, learning rate: [1.0000000000000005e-08], loss:2.541998863220215\n",
      "Epoch: 3, batch index: 641, learning rate: [1.0000000000000005e-08], loss:2.5184438228607178\n",
      "Epoch: 3, batch index: 642, learning rate: [1.0000000000000005e-08], loss:2.567324161529541\n",
      "Epoch: 3, batch index: 643, learning rate: [1.0000000000000005e-08], loss:2.51788067817688\n",
      "Epoch: 3, batch index: 644, learning rate: [1.0000000000000005e-08], loss:2.543788194656372\n",
      "Epoch: 3, batch index: 645, learning rate: [1.0000000000000005e-08], loss:2.5042099952697754\n",
      "Epoch: 3, batch index: 646, learning rate: [1.0000000000000005e-08], loss:2.535695791244507\n",
      "Epoch: 3, batch index: 647, learning rate: [1.0000000000000005e-08], loss:2.5358083248138428\n",
      "Epoch: 3, batch index: 648, learning rate: [1.0000000000000005e-08], loss:2.518712043762207\n",
      "Epoch: 3, batch index: 649, learning rate: [1.0000000000000005e-09], loss:2.5519845485687256\n",
      "Epoch: 3, batch index: 650, learning rate: [1.0000000000000005e-09], loss:2.598599672317505\n",
      "Epoch: 3, batch index: 651, learning rate: [1.0000000000000005e-09], loss:2.4339356422424316\n",
      "Epoch: 3, batch index: 652, learning rate: [1.0000000000000005e-09], loss:2.5190300941467285\n",
      "Epoch: 3, batch index: 653, learning rate: [1.0000000000000005e-09], loss:2.5555503368377686\n",
      "Epoch: 3, batch index: 654, learning rate: [1.0000000000000005e-09], loss:2.5077903270721436\n",
      "Epoch: 3, batch index: 655, learning rate: [1.0000000000000005e-09], loss:2.54120135307312\n",
      "Epoch: 3, batch index: 656, learning rate: [1.0000000000000005e-09], loss:2.5683717727661133\n",
      "Epoch: 3, batch index: 657, learning rate: [1.0000000000000005e-09], loss:2.5607285499572754\n",
      "Epoch: 3, batch index: 658, learning rate: [1.0000000000000005e-09], loss:2.52730393409729\n",
      "Epoch: 3, batch index: 659, learning rate: [1.0000000000000005e-09], loss:2.5380795001983643\n",
      "Epoch: 3, batch index: 660, learning rate: [1.0000000000000005e-09], loss:2.5421550273895264\n",
      "Epoch: 3, batch index: 661, learning rate: [1.0000000000000005e-09], loss:2.527658700942993\n",
      "Epoch: 3, batch index: 662, learning rate: [1.0000000000000005e-09], loss:2.5390257835388184\n",
      "Epoch: 3, batch index: 663, learning rate: [1.0000000000000005e-09], loss:2.573035717010498\n",
      "Epoch: 3, batch index: 664, learning rate: [1.0000000000000005e-09], loss:2.5617847442626953\n",
      "Epoch: 3, batch index: 665, learning rate: [1.0000000000000005e-09], loss:2.5442309379577637\n",
      "Epoch: 3, batch index: 666, learning rate: [1.0000000000000005e-09], loss:2.558102607727051\n",
      "Epoch: 3, batch index: 667, learning rate: [1.0000000000000005e-09], loss:2.523960828781128\n",
      "Epoch: 3, batch index: 668, learning rate: [1.0000000000000005e-09], loss:2.500119209289551\n",
      "Epoch: 3, batch index: 669, learning rate: [1.0000000000000005e-09], loss:2.4982547760009766\n",
      "Epoch: 3, batch index: 670, learning rate: [1.0000000000000005e-09], loss:2.5241594314575195\n",
      "Epoch: 3, batch index: 671, learning rate: [1.0000000000000005e-09], loss:2.574094295501709\n",
      "Epoch: 3, batch index: 672, learning rate: [1.0000000000000005e-09], loss:2.4628491401672363\n",
      "Epoch: 3, batch index: 673, learning rate: [1.0000000000000005e-09], loss:2.5196480751037598\n",
      "Epoch: 3, batch index: 674, learning rate: [1.0000000000000005e-09], loss:2.541238784790039\n",
      "Epoch: 3, batch index: 675, learning rate: [1.0000000000000005e-09], loss:2.5527658462524414\n",
      "Epoch: 3, batch index: 676, learning rate: [1.0000000000000005e-09], loss:2.537233829498291\n",
      "Epoch: 3, batch index: 677, learning rate: [1.0000000000000005e-09], loss:2.522655963897705\n",
      "Epoch: 3, batch index: 678, learning rate: [1.0000000000000005e-09], loss:2.5258755683898926\n",
      "Epoch: 3, batch index: 679, learning rate: [1.0000000000000005e-09], loss:2.5037899017333984\n",
      "Epoch: 3, batch index: 680, learning rate: [1.0000000000000005e-09], loss:2.575914144515991\n",
      "Epoch: 3, batch index: 681, learning rate: [1.0000000000000005e-09], loss:2.4689576625823975\n",
      "Epoch: 3, batch index: 682, learning rate: [1.0000000000000005e-09], loss:2.5336289405822754\n",
      "Epoch: 3, batch index: 683, learning rate: [1.0000000000000005e-09], loss:2.5538883209228516\n",
      "Epoch: 3, batch index: 684, learning rate: [1.0000000000000005e-09], loss:2.5697708129882812\n",
      "Epoch: 3, batch index: 685, learning rate: [1.0000000000000005e-09], loss:2.539700746536255\n",
      "Epoch: 3, batch index: 686, learning rate: [1.0000000000000005e-09], loss:2.5402233600616455\n",
      "Epoch: 3, batch index: 687, learning rate: [1.0000000000000005e-09], loss:2.46047306060791\n",
      "Epoch: 3, batch index: 688, learning rate: [1.0000000000000005e-09], loss:2.5615179538726807\n",
      "Epoch: 3, batch index: 689, learning rate: [1.0000000000000005e-09], loss:2.592938184738159\n",
      "Epoch: 3, batch index: 690, learning rate: [1.0000000000000005e-09], loss:2.517467498779297\n",
      "Epoch: 3, batch index: 691, learning rate: [1.0000000000000005e-09], loss:2.4992003440856934\n",
      "Epoch: 3, batch index: 692, learning rate: [1.0000000000000005e-09], loss:2.518415927886963\n",
      "Epoch: 3, batch index: 693, learning rate: [1.0000000000000005e-09], loss:2.5640552043914795\n",
      "Epoch: 3, batch index: 694, learning rate: [1.0000000000000005e-09], loss:2.535489320755005\n",
      "Epoch: 3, batch index: 695, learning rate: [1.0000000000000005e-09], loss:2.5334482192993164\n",
      "Epoch: 3, batch index: 696, learning rate: [1.0000000000000005e-09], loss:2.5731780529022217\n",
      "Epoch: 3, batch index: 697, learning rate: [1.0000000000000005e-09], loss:2.585348606109619\n",
      "Epoch: 3, batch index: 698, learning rate: [1.0000000000000005e-09], loss:2.492835760116577\n",
      "Epoch: 3, batch index: 699, learning rate: [1.0000000000000005e-09], loss:2.5370028018951416\n",
      "Epoch: 3, batch index: 700, learning rate: [1.0000000000000005e-09], loss:2.5493667125701904\n",
      "Epoch: 3, batch index: 701, learning rate: [1.0000000000000005e-09], loss:2.546273708343506\n",
      "Epoch: 3, batch index: 702, learning rate: [1.0000000000000005e-09], loss:2.5652875900268555\n",
      "Epoch: 3, batch index: 703, learning rate: [1.0000000000000005e-09], loss:2.476318597793579\n",
      "Epoch: 3, batch index: 704, learning rate: [1.0000000000000005e-09], loss:2.554675340652466\n",
      "Epoch: 3, batch index: 705, learning rate: [1.0000000000000005e-09], loss:2.4862112998962402\n",
      "Epoch: 3, batch index: 706, learning rate: [1.0000000000000005e-09], loss:2.587120771408081\n",
      "Epoch: 3, batch index: 707, learning rate: [1.0000000000000006e-10], loss:2.5157012939453125\n",
      "Epoch: 3, batch index: 708, learning rate: [1.0000000000000006e-10], loss:2.4990458488464355\n",
      "Epoch: 3, batch index: 709, learning rate: [1.0000000000000006e-10], loss:2.5760560035705566\n",
      "Epoch: 3, batch index: 710, learning rate: [1.0000000000000006e-10], loss:2.551328182220459\n",
      "Epoch: 3, batch index: 711, learning rate: [1.0000000000000006e-10], loss:2.499385356903076\n",
      "Epoch: 3, batch index: 712, learning rate: [1.0000000000000006e-10], loss:2.47298526763916\n",
      "Epoch: 3, batch index: 713, learning rate: [1.0000000000000006e-10], loss:2.5370378494262695\n",
      "Epoch: 3, batch index: 714, learning rate: [1.0000000000000006e-10], loss:2.521357536315918\n",
      "Epoch: 3, batch index: 715, learning rate: [1.0000000000000006e-10], loss:2.5036654472351074\n",
      "Epoch: 3, batch index: 716, learning rate: [1.0000000000000006e-10], loss:2.5214126110076904\n",
      "Epoch: 3, batch index: 717, learning rate: [1.0000000000000006e-10], loss:2.5523149967193604\n",
      "Epoch: 3, batch index: 718, learning rate: [1.0000000000000006e-10], loss:2.533904790878296\n",
      "Epoch: 3, batch index: 719, learning rate: [1.0000000000000006e-10], loss:2.571873664855957\n",
      "Epoch: 3, batch index: 720, learning rate: [1.0000000000000006e-10], loss:2.5427770614624023\n",
      "Epoch: 3, batch index: 721, learning rate: [1.0000000000000006e-10], loss:2.5911803245544434\n",
      "Epoch: 3, batch index: 722, learning rate: [1.0000000000000006e-10], loss:2.5483269691467285\n",
      "Epoch: 3, batch index: 723, learning rate: [1.0000000000000006e-10], loss:2.548251152038574\n",
      "Epoch: 3, batch index: 724, learning rate: [1.0000000000000006e-10], loss:2.5882599353790283\n",
      "Epoch: 3, batch index: 725, learning rate: [1.0000000000000006e-10], loss:2.540964126586914\n",
      "Epoch: 3, batch index: 726, learning rate: [1.0000000000000006e-10], loss:2.489420175552368\n",
      "Epoch: 3, batch index: 727, learning rate: [1.0000000000000006e-10], loss:2.5315663814544678\n",
      "Epoch: 3, batch index: 728, learning rate: [1.0000000000000006e-10], loss:2.5539753437042236\n",
      "Epoch: 3, batch index: 729, learning rate: [1.0000000000000006e-10], loss:2.5699098110198975\n",
      "Epoch: 3, batch index: 730, learning rate: [1.0000000000000006e-10], loss:2.5577943325042725\n",
      "Epoch: 3, batch index: 731, learning rate: [1.0000000000000006e-10], loss:2.5313053131103516\n",
      "Epoch: 3, batch index: 732, learning rate: [1.0000000000000006e-10], loss:2.555643081665039\n",
      "Epoch: 3, batch index: 733, learning rate: [1.0000000000000006e-10], loss:2.523712635040283\n",
      "Epoch: 3, batch index: 734, learning rate: [1.0000000000000006e-10], loss:2.571542739868164\n",
      "Epoch: 3, batch index: 735, learning rate: [1.0000000000000006e-10], loss:2.563265085220337\n",
      "Epoch: 3, batch index: 736, learning rate: [1.0000000000000006e-10], loss:2.513538122177124\n",
      "Epoch: 3, batch index: 737, learning rate: [1.0000000000000006e-10], loss:2.5161001682281494\n",
      "Epoch: 3, batch index: 738, learning rate: [1.0000000000000006e-10], loss:2.496898651123047\n",
      "Epoch: 3, batch index: 739, learning rate: [1.0000000000000006e-10], loss:2.5783984661102295\n",
      "Epoch: 3, batch index: 740, learning rate: [1.0000000000000006e-10], loss:2.533357858657837\n",
      "Epoch: 3, batch index: 741, learning rate: [1.0000000000000006e-10], loss:2.583216428756714\n",
      "Epoch: 3, batch index: 742, learning rate: [1.0000000000000006e-10], loss:2.559619188308716\n",
      "Epoch: 3, batch index: 743, learning rate: [1.0000000000000006e-10], loss:2.5315377712249756\n",
      "Epoch: 3, batch index: 744, learning rate: [1.0000000000000006e-10], loss:2.510704278945923\n",
      "Epoch: 3, batch index: 745, learning rate: [1.0000000000000006e-10], loss:2.545858144760132\n",
      "Epoch: 3, batch index: 746, learning rate: [1.0000000000000006e-10], loss:2.46964955329895\n",
      "Epoch: 3, batch index: 747, learning rate: [1.0000000000000006e-10], loss:2.5401690006256104\n",
      "Epoch: 3, batch index: 748, learning rate: [1.0000000000000006e-10], loss:2.546609878540039\n",
      "Epoch: 3, batch index: 749, learning rate: [1.0000000000000006e-10], loss:2.5564141273498535\n",
      "Epoch: 3, batch index: 750, learning rate: [1.0000000000000006e-10], loss:2.5833139419555664\n",
      "Epoch: 3, batch index: 751, learning rate: [1.0000000000000006e-10], loss:2.5466039180755615\n",
      "Calculating validation accuracy\n",
      "cuda:0\n",
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "Epoch 3, validation accuracy score0.1123046875\n",
      "Loading best model\n",
      "cuda:0\n",
      "torch.Size([1024]) torch.Size([1024]) 1024\n",
      "The best model has validation accuracy 0.11328125\n"
     ]
    }
   ],
   "source": [
    "train(text_model, text_train_loader, text_validation_loader, epochs=4, learning_rate=1, model_name = 'text_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a slightly trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the image processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7468, 0.0109, 0.0162, 0.0273, 0.0051, 0.0113, 0.0278, 0.0113, 0.0534,\n",
       "         0.0125, 0.0081, 0.0531, 0.0162]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from image_processor import process_image_as_pil\n",
    "\n",
    "with Image.open(\"Datasets/images/ffff23f1-59fc-47bd-b0cd-186933803287.jpg\") as image:\n",
    "    image_tensor = process_image_as_pil(image).to(device)\n",
    "\n",
    "model(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction model\n",
    "\n",
    "The feature extraction model should take in an image and return a vector with 1000 entries loosely corresponding to the predicted image category and image characteristics. We can achieve this by taking the category classification model and removing the extra layers added on top of resnet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50.conv1.weight False\n",
      "resnet50.bn1.weight False\n",
      "resnet50.bn1.bias False\n",
      "resnet50.layer1.0.conv1.weight False\n",
      "resnet50.layer1.0.bn1.weight False\n",
      "resnet50.layer1.0.bn1.bias False\n",
      "resnet50.layer1.0.conv2.weight False\n",
      "resnet50.layer1.0.bn2.weight False\n",
      "resnet50.layer1.0.bn2.bias False\n",
      "resnet50.layer1.0.conv3.weight False\n",
      "resnet50.layer1.0.bn3.weight False\n",
      "resnet50.layer1.0.bn3.bias False\n",
      "resnet50.layer1.0.downsample.0.weight False\n",
      "resnet50.layer1.0.downsample.1.weight False\n",
      "resnet50.layer1.0.downsample.1.bias False\n",
      "resnet50.layer1.1.conv1.weight False\n",
      "resnet50.layer1.1.bn1.weight False\n",
      "resnet50.layer1.1.bn1.bias False\n",
      "resnet50.layer1.1.conv2.weight False\n",
      "resnet50.layer1.1.bn2.weight False\n",
      "resnet50.layer1.1.bn2.bias False\n",
      "resnet50.layer1.1.conv3.weight False\n",
      "resnet50.layer1.1.bn3.weight False\n",
      "resnet50.layer1.1.bn3.bias False\n",
      "resnet50.layer1.2.conv1.weight False\n",
      "resnet50.layer1.2.bn1.weight False\n",
      "resnet50.layer1.2.bn1.bias False\n",
      "resnet50.layer1.2.conv2.weight False\n",
      "resnet50.layer1.2.bn2.weight False\n",
      "resnet50.layer1.2.bn2.bias False\n",
      "resnet50.layer1.2.conv3.weight False\n",
      "resnet50.layer1.2.bn3.weight False\n",
      "resnet50.layer1.2.bn3.bias False\n",
      "resnet50.layer2.0.conv1.weight False\n",
      "resnet50.layer2.0.bn1.weight False\n",
      "resnet50.layer2.0.bn1.bias False\n",
      "resnet50.layer2.0.conv2.weight False\n",
      "resnet50.layer2.0.bn2.weight False\n",
      "resnet50.layer2.0.bn2.bias False\n",
      "resnet50.layer2.0.conv3.weight False\n",
      "resnet50.layer2.0.bn3.weight False\n",
      "resnet50.layer2.0.bn3.bias False\n",
      "resnet50.layer2.0.downsample.0.weight False\n",
      "resnet50.layer2.0.downsample.1.weight False\n",
      "resnet50.layer2.0.downsample.1.bias False\n",
      "resnet50.layer2.1.conv1.weight False\n",
      "resnet50.layer2.1.bn1.weight False\n",
      "resnet50.layer2.1.bn1.bias False\n",
      "resnet50.layer2.1.conv2.weight False\n",
      "resnet50.layer2.1.bn2.weight False\n",
      "resnet50.layer2.1.bn2.bias False\n",
      "resnet50.layer2.1.conv3.weight False\n",
      "resnet50.layer2.1.bn3.weight False\n",
      "resnet50.layer2.1.bn3.bias False\n",
      "resnet50.layer2.2.conv1.weight False\n",
      "resnet50.layer2.2.bn1.weight False\n",
      "resnet50.layer2.2.bn1.bias False\n",
      "resnet50.layer2.2.conv2.weight False\n",
      "resnet50.layer2.2.bn2.weight False\n",
      "resnet50.layer2.2.bn2.bias False\n",
      "resnet50.layer2.2.conv3.weight False\n",
      "resnet50.layer2.2.bn3.weight False\n",
      "resnet50.layer2.2.bn3.bias False\n",
      "resnet50.layer2.3.conv1.weight False\n",
      "resnet50.layer2.3.bn1.weight False\n",
      "resnet50.layer2.3.bn1.bias False\n",
      "resnet50.layer2.3.conv2.weight False\n",
      "resnet50.layer2.3.bn2.weight False\n",
      "resnet50.layer2.3.bn2.bias False\n",
      "resnet50.layer2.3.conv3.weight False\n",
      "resnet50.layer2.3.bn3.weight False\n",
      "resnet50.layer2.3.bn3.bias False\n",
      "resnet50.layer3.0.conv1.weight False\n",
      "resnet50.layer3.0.bn1.weight False\n",
      "resnet50.layer3.0.bn1.bias False\n",
      "resnet50.layer3.0.conv2.weight False\n",
      "resnet50.layer3.0.bn2.weight False\n",
      "resnet50.layer3.0.bn2.bias False\n",
      "resnet50.layer3.0.conv3.weight False\n",
      "resnet50.layer3.0.bn3.weight False\n",
      "resnet50.layer3.0.bn3.bias False\n",
      "resnet50.layer3.0.downsample.0.weight False\n",
      "resnet50.layer3.0.downsample.1.weight False\n",
      "resnet50.layer3.0.downsample.1.bias False\n",
      "resnet50.layer3.1.conv1.weight False\n",
      "resnet50.layer3.1.bn1.weight False\n",
      "resnet50.layer3.1.bn1.bias False\n",
      "resnet50.layer3.1.conv2.weight False\n",
      "resnet50.layer3.1.bn2.weight False\n",
      "resnet50.layer3.1.bn2.bias False\n",
      "resnet50.layer3.1.conv3.weight False\n",
      "resnet50.layer3.1.bn3.weight False\n",
      "resnet50.layer3.1.bn3.bias False\n",
      "resnet50.layer3.2.conv1.weight False\n",
      "resnet50.layer3.2.bn1.weight False\n",
      "resnet50.layer3.2.bn1.bias False\n",
      "resnet50.layer3.2.conv2.weight False\n",
      "resnet50.layer3.2.bn2.weight False\n",
      "resnet50.layer3.2.bn2.bias False\n",
      "resnet50.layer3.2.conv3.weight False\n",
      "resnet50.layer3.2.bn3.weight False\n",
      "resnet50.layer3.2.bn3.bias False\n",
      "resnet50.layer3.3.conv1.weight False\n",
      "resnet50.layer3.3.bn1.weight False\n",
      "resnet50.layer3.3.bn1.bias False\n",
      "resnet50.layer3.3.conv2.weight False\n",
      "resnet50.layer3.3.bn2.weight False\n",
      "resnet50.layer3.3.bn2.bias False\n",
      "resnet50.layer3.3.conv3.weight False\n",
      "resnet50.layer3.3.bn3.weight False\n",
      "resnet50.layer3.3.bn3.bias False\n",
      "resnet50.layer3.4.conv1.weight False\n",
      "resnet50.layer3.4.bn1.weight False\n",
      "resnet50.layer3.4.bn1.bias False\n",
      "resnet50.layer3.4.conv2.weight False\n",
      "resnet50.layer3.4.bn2.weight False\n",
      "resnet50.layer3.4.bn2.bias False\n",
      "resnet50.layer3.4.conv3.weight False\n",
      "resnet50.layer3.4.bn3.weight False\n",
      "resnet50.layer3.4.bn3.bias False\n",
      "resnet50.layer3.5.conv1.weight False\n",
      "resnet50.layer3.5.bn1.weight False\n",
      "resnet50.layer3.5.bn1.bias False\n",
      "resnet50.layer3.5.conv2.weight False\n",
      "resnet50.layer3.5.bn2.weight False\n",
      "resnet50.layer3.5.bn2.bias False\n",
      "resnet50.layer3.5.conv3.weight False\n",
      "resnet50.layer3.5.bn3.weight False\n",
      "resnet50.layer3.5.bn3.bias False\n",
      "resnet50.layer4.0.conv1.weight False\n",
      "resnet50.layer4.0.bn1.weight False\n",
      "resnet50.layer4.0.bn1.bias False\n",
      "resnet50.layer4.0.conv2.weight False\n",
      "resnet50.layer4.0.bn2.weight False\n",
      "resnet50.layer4.0.bn2.bias False\n",
      "resnet50.layer4.0.conv3.weight False\n",
      "resnet50.layer4.0.bn3.weight False\n",
      "resnet50.layer4.0.bn3.bias False\n",
      "resnet50.layer4.0.downsample.0.weight False\n",
      "resnet50.layer4.0.downsample.1.weight False\n",
      "resnet50.layer4.0.downsample.1.bias False\n",
      "resnet50.layer4.1.conv1.weight False\n",
      "resnet50.layer4.1.bn1.weight False\n",
      "resnet50.layer4.1.bn1.bias False\n",
      "resnet50.layer4.1.conv2.weight False\n",
      "resnet50.layer4.1.bn2.weight False\n",
      "resnet50.layer4.1.bn2.bias False\n",
      "resnet50.layer4.1.conv3.weight False\n",
      "resnet50.layer4.1.bn3.weight False\n",
      "resnet50.layer4.1.bn3.bias False\n",
      "resnet50.layer4.2.conv1.weight False\n",
      "resnet50.layer4.2.bn1.weight False\n",
      "resnet50.layer4.2.bn1.bias False\n",
      "resnet50.layer4.2.conv2.weight False\n",
      "resnet50.layer4.2.bn2.weight False\n",
      "resnet50.layer4.2.bn2.bias False\n",
      "resnet50.layer4.2.conv3.weight False\n",
      "resnet50.layer4.2.bn3.weight False\n",
      "resnet50.layer4.2.bn3.bias False\n",
      "Grad! resnet50.fc.weight True\n",
      "Grad! resnet50.fc.bias True\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ImageClassifier classs with the same architecture as the previously trained model\n",
    "feature_extraction_model = ImageClassifier(13)\n",
    "\n",
    "feature_extraction_model.to(device)\n",
    "\n",
    "# Load the same parameters from that model to this one\n",
    "feature_extraction_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Cut of the extra layers while ensuring the architecture naming scheme stays the same\n",
    "feature_extraction_model.layers = torch.nn.Sequential(\n",
    "    *list(feature_extraction_model.layers.children())[:-6]\n",
    ")\n",
    "\n",
    "# Print the layers as a sanity check\n",
    "show_model_layers(feature_extraction_model)\n",
    "\n",
    "# Save the feature extraction model paramters\n",
    "torch.save(feature_extraction_model.state_dict(), 'model_evaluation/final_model/image_model.pt')\n",
    "torch.save(feature_extraction_model.state_dict(), 'app/Model_Parameters/image_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n",
      "bert.embeddings.position_embeddings.weight False\n",
      "bert.embeddings.token_type_embeddings.weight False\n",
      "bert.embeddings.LayerNorm.weight False\n",
      "bert.embeddings.LayerNorm.bias False\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight False\n",
      "bert.encoder.layer.11.attention.self.query.bias False\n",
      "bert.encoder.layer.11.attention.self.key.weight False\n",
      "bert.encoder.layer.11.attention.self.key.bias False\n",
      "bert.encoder.layer.11.attention.self.value.weight False\n",
      "bert.encoder.layer.11.attention.self.value.bias False\n",
      "bert.encoder.layer.11.attention.output.dense.weight False\n",
      "bert.encoder.layer.11.attention.output.dense.bias False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.intermediate.dense.weight False\n",
      "bert.encoder.layer.11.intermediate.dense.bias False\n",
      "bert.encoder.layer.11.output.dense.weight False\n",
      "bert.encoder.layer.11.output.dense.bias False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias False\n",
      "bert.pooler.dense.weight False\n",
      "bert.pooler.dense.bias False\n",
      "Grad! layers.1.weight True\n",
      "Grad! layers.1.bias True\n"
     ]
    }
   ],
   "source": [
    "text_feature_model = BertNeuralNetwork(13)\n",
    "text_feature_model.to(device)\n",
    "\n",
    "# Load the same parameters from that model to this one\n",
    "#feature_extraction_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Cut of the extra layers while ensuring the architecture naming scheme stays the same\n",
    "text_feature_model.layers = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.Linear(768, 1000),\n",
    ")\n",
    "\n",
    "# Print the layers as a sanity check\n",
    "show_model_layers(text_feature_model)\n",
    "\n",
    "text_feature_model.to(device)\n",
    "\n",
    "# Save the feature extraction model paramters\n",
    "torch.save(text_feature_model.state_dict(), 'model_evaluation/final_model/text_model.pt')\n",
    "torch.save(text_feature_model.state_dict(), 'app/Model_Parameters/text_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the FAISS index search\n",
    "\n",
    "We need to set up a matrix containing the image embeddings for all the images in out dataset. First let's cerate a dictionary containg pairs of the image id and feature extraction model embeddings, and push it onto a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(folder_of_images:str, df_of_keys):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of all the image embeddings of from the feature extraction model from images in folder_of_images. Stored as key value pairs with keys as 'id' from 'df_of_keys' and values of image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the iamge transformer.\n",
    "    transformer =  transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.PILToTensor(),\n",
    "    ])\n",
    "    # Initialise an empty disctionary\n",
    "    dict_of_features = {}\n",
    "\n",
    "    # List containing all the image paths\n",
    "    list_of_image_paths = df_of_keys['id'].map(lambda x : folder_of_images + x + '.jpg')\n",
    "\n",
    "    # Iterate through all images to add thier key value pair of id and image embeddings to 'dict_of_features'.\n",
    "    for index in range(0, len(merged_df)):\n",
    "        image_path = list_of_image_paths[index]\n",
    "        with Image.open(image_path) as img:\n",
    "            features = transformer(img).unsqueeze(0)\n",
    "        features = features.to(device)\n",
    "        image_embedding = feature_extraction_model(features)\n",
    "\n",
    "        image_embedding = image_embedding.squeeze(0)\n",
    "\n",
    "        raw_text = df_of_keys[\"combined_name_and_description\"][index]\n",
    "        tokenized_text = bert_tokenizer(raw_text, return_tensors=\"pt\", pad_to_max_length=True, max_length = 200)\n",
    "        input_dictionary = {\n",
    "            'input_ids':tokenized_text[\"input_ids\"].view(1,-1).to(device),\n",
    "            'attention_mask':tokenized_text[\"attention_mask\"].view(1,-1).to(device)\n",
    "        }\n",
    "        \n",
    "        text_prediction = text_feature_model(input_dictionary).squeeze()\n",
    "\n",
    "        embedding = torch.cat((image_embedding, text_prediction))\n",
    "        embedding = embedding.tolist()\n",
    "        dict_of_features[df_of_keys['id'][index]] = embedding\n",
    "    \n",
    "    return dict_of_features\n",
    "\n",
    "    # Save these to project folder.\n",
    "    with open('image_embeddings_new.json', '+w') as file:\n",
    "        json.dump(dict_of_features, file)\n",
    "        pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "\n",
    "# Load the key value pairs of the image file names and image embeddings.\n",
    "# Uncomment to run\n",
    "#new_dict = feature_extraction('Datasets/cleaned_images_224/', merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load that json file and stack all the image embeddings into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12604, 2000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsqueeze each image embedding.\n",
    "for key, value in new_dict.items():\n",
    "    new_dict[key] = [value]\n",
    "\n",
    "# Create a dataframe from the loaded json\n",
    "vectors_df = pd.DataFrame.from_dict(new_dict, orient='index', columns=['image embedding'])\n",
    "\n",
    "# Stack all the image embeddings into a large numpy array.\n",
    "vectors = np.vstack(vectors_df['image embedding']).astype(np.float32)\n",
    "\n",
    "# Double check the shape.\n",
    "vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add `vectors` to the faiss index search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(2000)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the seach works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 100 8187 1577]] (1, 3)\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(np.expand_dims(vectors[100], 0), 3)\n",
    "print(I, I.shape)\n",
    "#print(D, D.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's save the index seach to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"app/Pickle_Files/faiss_pickle\", 'wb') as pickly:\n",
    "    pickle.dump(index, pickly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.     , 126.5222 , 131.95357, 134.16539, 138.15826, 138.66617,\n",
       "         139.42729]], dtype=float32),\n",
       " array([[  100,  8187,  1577,  1194,  1864,  2934, 11506]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(np.expand_dims(vectors[100], 0), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[100].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
